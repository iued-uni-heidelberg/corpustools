{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPDdNsgeYoGU5I5jLqJJuKJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/corpustools/blob/main/armenian_lemmatization_stanza_v02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of Armenian lemmatization"
      ],
      "metadata": {
        "id": "skix7t6sFaZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## downloading evaluation set\n",
        "~ 420 words"
      ],
      "metadata": {
        "id": "fFBRX6lTFfcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/ce6096da570f47b99500/?dl=1"
      ],
      "metadata": {
        "id": "m549clSLFHs_",
        "outputId": "bf7fd1de-6c5a-4e41-b1cb-5f6ed3eed22f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-10 14:28:48--  https://heibox.uni-heidelberg.de/f/ce6096da570f47b99500/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/811ae83d-c26c-4a74-958e-1922598967f9/evaluation-set-v01.txt [following]\n",
            "--2022-11-10 14:28:50--  https://heibox.uni-heidelberg.de/seafhttp/files/811ae83d-c26c-4a74-958e-1922598967f9/evaluation-set-v01.txt\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5882 (5.7K) [text/plain]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>]   5.74K  23.2KB/s    in 0.2s    \n",
            "\n",
            "2022-11-10 14:28:51 (23.2 KB/s) - ‘index.html?dl=1’ saved [5882/5882]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv index.html?dl=1 evaluation-set-v01.txt"
      ],
      "metadata": {
        "id": "eI_j8KDdFRCe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing stanza"
      ],
      "metadata": {
        "id": "i__aUXulFkw2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdzVArLUF3cb"
      },
      "outputs": [],
      "source": [
        "!pip install spacy-stanza"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### testing English stanza"
      ],
      "metadata": {
        "id": "w6Kfwj63FzYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "import spacy_stanza\n",
        "\n",
        "# Download the stanza model if necessary\n",
        "stanza.download(\"en\")\n",
        "\n",
        "# Initialize the pipeline\n",
        "nlp = spacy_stanza.load_pipeline(\"en\")\n",
        "\n"
      ],
      "metadata": {
        "id": "-VN9g4N4GAR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Barack Obama was born in Hawaii. He was elected president in 2008.\")\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.dep_, token.ent_type_)\n",
        "print(doc.ents)"
      ],
      "metadata": {
        "id": "yH43K0rg9ewM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### downloading and testing Armenian stanza"
      ],
      "metadata": {
        "id": "HGyKxEJNGG8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stanza.download(\"hy\")\n"
      ],
      "metadata": {
        "id": "xq53mDsUGumV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_hy = spacy_stanza.load_pipeline(\"hy\")"
      ],
      "metadata": {
        "id": "KZKOs0aVG8Kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp_hy(\"ՄԱՐԴՈՒ ԻՐԱՎՈՒՆՔՆԵՐԻ ՀԱՄԸՆԴՀԱՆՈՒՐ ՀՌՉԱԿԱԳԻՐ. ՆԵՐԱԾԱԿԱՆ. Քանզի մարդկային ընտանիքի բոլոր անդամներին ներհատուկ արժանապատվությունըև հավասար ու անօտարելի իրավունքները աշխարհի ազատության, արդարության ու խաղաղության հիմքն են.\")"
      ],
      "metadata": {
        "id": "m022wp2JHvOO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.dep_, token.ent_type_)\n"
      ],
      "metadata": {
        "id": "LPhSOX15ICmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/evaluation-set-v01.txt', 'r', encoding='utf-8') as infile, open('/content/evaluation-set-ANALYSIS-full-v01.txt', 'w') as outfile:\n",
        "    # read sample.txt an and write its content into sample2.txt\n",
        "    outfile.write(\"{token.text}\\t{token.lemma_}\\t{token.pos_}\\t{token.dep_}\\t{parentLem}\\t{LAncestors}\\n\")\n",
        "    for line in infile:\n",
        "        line = line.strip()\n",
        "        doc = nlp_hy(line)\n",
        "        # outfile.write(line + '\\n')\n",
        "        for token in doc:\n",
        "            LAncestors = list(token.ancestors)\n",
        "            print(str(LAncestors))\n",
        "            try:\n",
        "                SLAncestors = str(list(token.ancestors))\n",
        "                parent = LAncestors[0]\n",
        "                parentLem = parent.lemma_\n",
        "            except:\n",
        "                parentLem = \"NONE\"\n",
        "            outfile.write(f\"{token.text}\\t{token.lemma_}\\t{token.pos_}\\t{token.dep_}\\t{parentLem}\\t{SLAncestors}\\n\")\n",
        " "
      ],
      "metadata": {
        "id": "HJdW66EmJI2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parseFile(iFileName, oFileName, nlp_model = nlp):\n",
        "    with open(iFileName, 'r', encoding='utf-8') as infile, open(oFileName, 'w') as outfile:\n",
        "        # read sample.txt an and write its content into sample2.txt\n",
        "        outfile.write(\"{token.text}\\t{token.pos_}\\t{token.lemma_}\\n\")\n",
        "        c = 0\n",
        "        for line in infile:\n",
        "            c+=1\n",
        "            if c%10 == 0: print(str(c))\n",
        "            line = line.strip()\n",
        "            doc = nlp_model(line)\n",
        "            # outfile.write(line + '\\n')\n",
        "            for token in doc:\n",
        "                LAncestors = list(token.ancestors)\n",
        "                # print(str(LAncestors))\n",
        "                try:\n",
        "                    SLAncestors = str(list(token.ancestors))\n",
        "                    parent = LAncestors[0]\n",
        "                    parentLem = parent.lemma_\n",
        "                except:\n",
        "                    parentLem = \"NONE\"\n",
        "                outfile.write(f\"{token.text}\\t{token.pos_}\\t{token.lemma_}\\n\")\n",
        " \n",
        "    return\n"
      ],
      "metadata": {
        "id": "QT0tpHwjY4O5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parseFile('/content/evaluation-set-v01.txt', '/content/evaluation-set-lemmatization-v01.txt', nlp_hy)"
      ],
      "metadata": {
        "id": "rgdb1fl3a6F4",
        "outputId": "aab1af24-d0a7-40de-a217-ef8d9a4602ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    }
  ]
}
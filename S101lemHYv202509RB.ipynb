{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/corpustools/blob/main/S101lemHYv202509RB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing pipeline for DAAD project\n",
        "\n",
        "## Stage 1: Lemmatization (Armenian)\n",
        "Example: Universal declaration of human rights"
      ],
      "metadata": {
        "id": "j2SZAuuE1441"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading UDHR\n",
        "!wget https://unicode.org/udhr/assemblies/udhr_txt.zip"
      ],
      "metadata": {
        "id": "QUnIvH5a136j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir udhr\n",
        "mv udhr_txt.zip udhr\n",
        "cd udhr/\n",
        "unzip udhr_txt.zip"
      ],
      "metadata": {
        "id": "o4UYZhui2HJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# delete lines which are not translations in some files (hy)\n",
        "# delete between lines $a and $b inclusive\n",
        "a=9\n",
        "b=21\n",
        "awk -v m=$a -v n=$b 'm <= NR && NR <= n {next} {print}' < udhr/udhr_hye.txt >udhr/udhr_hye_v03.txt\n",
        "# put paragraph tags\n",
        "# awk '{print \"<p>\\n\"$0 ; print \"</p>\"}' udhr/udhr_hye2.txt >udhr/udhr_hye_v03.txt"
      ],
      "metadata": {
        "id": "gWOntGq42JvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing python libraries\n",
        "import os, re, sys"
      ],
      "metadata": {
        "id": "I3-okSYJ2NI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# installing Armenian morphological analyser\n",
        "!git clone https://github.com/timarkh/uniparser-grammar-eastern-armenian\n",
        "# Python classes\n",
        "!pip3 install uniparser-eastern-armenian\n",
        "# disambiguation\n",
        "!sudo apt-get install cg3"
      ],
      "metadata": {
        "id": "jF1Sa8dm3WFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from uniparser_eastern_armenian import EasternArmenianAnalyzer\n",
        "a = EasternArmenianAnalyzer()\n",
        "analyses = a.analyze_words('Ձևաբանություն')\n",
        "for ana in analyses:\n",
        "    print(ana.wf, ana.lemma, ana.gramm, ana.gloss, ana.stem, ana.subwords, ana.wfGlossed, ana.otherData)"
      ],
      "metadata": {
        "id": "dRFuA6sN3Z62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trying out:\n",
        "# nonexisting word\n",
        "analyses2 = a.analyze_words('Ձևաբայու')\n",
        "for ana2 in analyses2:\n",
        "    if ana2.lemma:\n",
        "      print(ana2.wf, ana2.lemma, ana2.gramm, ana2.gloss, ana2.stem, ana2.subwords, ana2.wfGlossed, ana2.otherData)\n",
        "    else:\n",
        "      print(ana2.wf, ana2.wf, \"N\", \"x\", ana2.stem, ana2.subwords, ana2.wfGlossed, ana2.otherData)"
      ],
      "metadata": {
        "id": "jMu1kHN03mxE",
        "outputId": "b617a8cd-4f17-43a4-ffc6-4977805868b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ձևաբայու Ձևաբայու N x  []  []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyses = a.analyze_words([['և'], ['Ես', 'սիրում', 'եմ', 'քեզ', ':']],\n",
        "                           format='xml')\n",
        "for ana in analyses:\n",
        "    print(str(ana))"
      ],
      "metadata": {
        "id": "5b0wetvb3zvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyses = a.analyze_words(['Ձևաբանություն', [['և'], ['Ես', 'սիրում', 'եմ', 'քեզ', ':']]],\n",
        "                           format='json')\n",
        "for ana in analyses:\n",
        "    print(str(ana))"
      ],
      "metadata": {
        "id": "R735YSlN3_rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# analysis with disambiguation\n",
        "analyses = a.analyze_words(['Ես', 'սիրում', 'եմ', 'քեզ'], disambiguate=True)\n",
        "for ana in analyses:\n",
        "    if len(ana) > 1: tab = \"  \"\n",
        "    else: tab = \"\"\n",
        "    for wfo in ana:\n",
        "        print(tab, wfo.wf, wfo.lemma, wfo.gramm, wfo.gloss)"
      ],
      "metadata": {
        "id": "401nl-i84GMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Str = \"Սառը, վճիտ ապրիլյան օր էր, ու ժամացույցը խփում էր տասներեքը։ Չար քամուց թաքնվելու համար կզակը սեղմելով կրծքին՝ Ուինսթոն Սմիթն արագ ներս խցկվեց «Հաղթանակ» բնակելի տան ապակե շքադռնից՝ իր ետևից ներս թողնելով հատիկավոր փոշու մի ամբողջ փոթորիկ։\"\n",
        "\n",
        "StrDe = ' „Es war ein kalter, trostloser Apriltag, und die Uhr schlug dreizehn. Das Kinn an die Brust gedrückt, um sich vor dem bitteren Wind zu schützen, eilte Winston Smith durch die gläserne Veranda des Wohnhauses Victory und hinterließ einen körnigen Sturm Staub.\" '\n",
        "\n",
        "StrEn = ' \"It was a cold, dreary April day, and the clock struck thirteen. Tucking his chin to his chest to shield himself from the bitter wind, Winston Smith hurried through the glass porch of the Victory apartment building, leaving behind him a storm of granular dust.\" '"
      ],
      "metadata": {
        "id": "xByNh1fO4XKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Str = \"Սառը, վճիտ ապրիլյան օր էր, ու ժամացույցը խփում էր տասներեքը։ Չար քամուց թաքնվելու համար կզակը սեղմելով կրծքին՝ Ուինսթոն Սմիթն արագ ներս խցկվեց «Հաղթանակ» բնակելի տան ապակե շքադռնից՝ իր ետևից ներս թողնելով հատիկավոր փոշու մի ամբողջ փոթորիկ։\""
      ],
      "metadata": {
        "id": "hySmqfbM4KBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Lst = re.split('([ ,\\.:;\\!\\(\\)\\\"\\[\\]՞՝«»\\-\\—՝։\\։]+)', Str)\n",
        "LstTok = []\n",
        "for el in Lst:\n",
        "    el = el.strip()\n",
        "    if el != '': LstTok.append(el)\n"
      ],
      "metadata": {
        "id": "sFoEbBZa4cc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(LstTok)\n",
        "# does disambiguation work? not yet..."
      ],
      "metadata": {
        "id": "JK8FDH6A4nD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec # The word2vec model class\n",
        "import gensim.downloader as api # Allows us to download some free training data"
      ],
      "metadata": {
        "id": "2izWzuMquhvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vahram's model for English\n",
        "!wget https://heibox.uni-heidelberg.de/f/c2ba64e4ad844f3a99d4/?dl=1\n",
        "!cp index.html?dl=1 WIKI_EN.model"
      ],
      "metadata": {
        "id": "hnW716PEuraV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_WIKI_EN = Word2Vec.load(\"/content/WIKI_EN.model\")\n",
        "word_vectors_WIKI_EN = model_WIKI_EN.wv"
      ],
      "metadata": {
        "id": "Ce8pWo4eu9VO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distance = model_WIKI_EN.similarity('obama', 'barak')\n",
        "distance2 = model_WIKI_EN.similarity('obama', 'zone')\n"
      ],
      "metadata": {
        "id": "W3d3XxegvGAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('distance = %.4f' % distance)\n",
        "print('distance2 = %.4f' % distance2)"
      ],
      "metadata": {
        "id": "GHlfrbD_vjkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizeHy(Str2tokenise, rePattern = '([ ,\\.:;\\!\\(\\)\\\"\\[\\]՞՝«»\\-\\—՝։\\։]+)'):\n",
        "    LTokens = []\n",
        "    Lst = re.split(rePattern, Str2tokenise)\n",
        "    # LstTok = []\n",
        "    for el in Lst:\n",
        "        el = el.strip()\n",
        "        if el != '': LTokens.append(el)\n",
        "    return LTokens"
      ],
      "metadata": {
        "id": "_FMKB1I7v2ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merging together different lines of code\n",
        "def disambiguate(LTok2disambiguate, Window = 4, show_variants = False):\n",
        "    SDisambig = ''\n",
        "\n",
        "    analysesN = a.analyze_words(LTok2disambiguate, disambiguate=False)\n",
        "\n",
        "    # preparing data structures\n",
        "    LLContext = [] # empty list of contexts, indices are the same as with the Text list\n",
        "    LLText = [] # test to disambiguate, ambiguous interpretations are double entries\n",
        "    for ana in analysesN:\n",
        "        # creating context window from glosses\n",
        "        # if len(ana) > 1: tab = \"~\"\n",
        "        # else: tab = \"!\"\n",
        "        LwfoContext = []\n",
        "        LwfoText = []\n",
        "        for wfo in ana: # preserve lemmas / word forms which are not found in dictionary\n",
        "            if wfo.gramm == '': wfo.gramm = 'N'\n",
        "            if wfo.lemma == '': wfo.lemma = wfo.wf\n",
        "            if wfo.gloss == '': wfo.gloss = '[unknown]'\n",
        "\n",
        "            SWfo = f'{wfo.wf}\\t{wfo.gramm}\\t{wfo.lemma}\\t{wfo.gloss}'\n",
        "            # print(SWfo)\n",
        "            # FNoD.write(SWfo + '\\n')\n",
        "\n",
        "            # find the first part of the gloss, which may be in the word vectors model\n",
        "            REPart = re.match('([A-Za-z]+)', wfo.gloss)\n",
        "            if REPart:\n",
        "                SGlossMin = REPart.group(1)\n",
        "                SGlossMin = SGlossMin.lower()\n",
        "                # print(SGlossMin)\n",
        "            else:\n",
        "                SGlossMin = '[NONE]'\n",
        "            LwfoContext.append(SGlossMin)\n",
        "            LwfoText.append(SWfo)\n",
        "        LLContext.append(LwfoContext)\n",
        "        LLText.append(LwfoText)\n",
        "\n",
        "    # for el in LLContext: print(el)\n",
        "    # for el in LLText: print(el)\n",
        "\n",
        "    # print(len(LLContext))\n",
        "    # print(len(LLText))\n",
        "\n",
        "    print('line done!...\\n')\n",
        "\n",
        "\n",
        "    for i in range(len(LLText)):\n",
        "        if len(LLText[i]) > 1:\n",
        "            # print(LLText[i])\n",
        "            # print(LLContext[i])\n",
        "            # collect context window +- 3 words\n",
        "            iwStart = i-Window\n",
        "            if iwStart <0: iwStart=0\n",
        "            iwEnd = i+Window\n",
        "            if iwEnd > len(LLText): iwEnd = len(LLText)\n",
        "            # iwLen = iwEnd - iwStart\n",
        "            winContext = LLContext[iwStart:iwEnd]\n",
        "            # print(winContext)\n",
        "            LScores = []\n",
        "            LScCand = []\n",
        "\n",
        "            for candidate in LLContext[i]:\n",
        "                ScoreCand = 0\n",
        "                for LCtx in winContext:\n",
        "                    for Ctx in LCtx:\n",
        "                        try: distance = model_WIKI_EN.similarity(candidate, Ctx)\n",
        "                        except: distance = 0\n",
        "                        ScoreCand += distance\n",
        "                LScores.append((candidate,ScoreCand))\n",
        "                LScCand.append(ScoreCand)\n",
        "            LScores.sort(key=lambda a: a[1], reverse=True)\n",
        "            # print(LScores)\n",
        "\n",
        "            max_value = max(LScCand)\n",
        "            #  Return the max value of the list\n",
        "            max_index = LScCand.index(max_value)\n",
        "            StoWrite = LLText[i][max_index] + '\\n'\n",
        "            SDisambig += StoWrite\n",
        "\n",
        "            if show_variants == True:\n",
        "                for el in LLText[i]:\n",
        "                    StoWrite = '\\t~\\t' + el + '\\n'\n",
        "                    SDisambig += StoWrite\n",
        "                for el in LScores:\n",
        "                    StoWrite = '\\t~sc:\\t' + str(el) + '\\n'\n",
        "                    SDisambig += StoWrite\n",
        "        else:\n",
        "            StoWrite = LLText[i][0] + '\\n'\n",
        "            SDisambig += StoWrite\n",
        "\n",
        "    return SDisambig"
      ],
      "metadata": {
        "id": "TpxYmxdTwGyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir udhrTT"
      ],
      "metadata": {
        "id": "8Oe0qK2z03AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "FInText = open('/content/udhr/udhr_hye_v03.txt','r')\n",
        "FOutText = open('/content/udhrTT/udhr_hye_vert.txt','w')"
      ],
      "metadata": {
        "id": "YMgnQ_crwuQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for SLine in FInText:\n",
        "    SLine = SLine.strip()\n",
        "    LTok = tokenizeHy(SLine)\n",
        "    SDisambig = disambiguate(LTok)\n",
        "    # FOutText.write('<p>\\n')\n",
        "    # FOutText.write('\\n')\n",
        "    FOutText.write(SDisambig)\n",
        "    # FOutText.write('</p>\\n')\n",
        "    FOutText.write('\\n')\n",
        "\n",
        "FOutText.flush()"
      ],
      "metadata": {
        "id": "61sqTRpmxOFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!awk -F '\\t' '(NF==4){printf \"%s \", $3}(NF!=4){printf \"\\n\"}' < /content/udhrTT/udhr_hye_vert.txt >/content/udhrTT/udhr_hye_lem.txt"
      ],
      "metadata": {
        "id": "ScstreEf1RDn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
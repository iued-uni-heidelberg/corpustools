{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "tV6irX9rkiE1",
        "6p6DaZz45mBQ",
        "DWpjU8WNq_bf"
      ],
      "authorship_tag": "ABX9TyOoU6ao34faSCma7RWwEjit",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/corpustools/blob/main/S105ocrCorrectionV06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# correction of ocr-generated text\n",
        "- using correction dictionaries\n",
        "- using rules\n",
        "\n",
        "### Notes on the development\n",
        "- Algorithm:\n",
        "1. We extract rewrite rules from the annotations\n",
        "2. If a word is not found in our corpus it can be an error:\n",
        "\n",
        ">>\n",
        "- We Apply the longest-first strategy for rewriting\n",
        "- We Apply all the rules, converting the word into several candidates, where possible\n",
        "- We check which candidates exist in Wikipedia dictionary\n",
        "\n",
        "3. We print all candidates for annotation in a spreadsheet\n",
        "4. Record which rules are most productive ...\n",
        "\n",
        "\n",
        "Further investigation:\n",
        "- introduce Levenshtein distance?\n",
        "- introduce language model to check which rewriting operation to apply?\n"
      ],
      "metadata": {
        "id": "pS0vQT9LRLss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Service functions"
      ],
      "metadata": {
        "id": "GLb73s8m2hSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, os, sys\n"
      ],
      "metadata": {
        "id": "2yzWvD6lW-78"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# critical path function...\n",
        "# function for Armenian tokenization\n",
        "def tokenizeTextHY(SFIn):\n",
        "    LLParagraphs = []\n",
        "    with open(SFIn, 'r') as FIn:\n",
        "        countpara = 0\n",
        "        for SLine in FIn:\n",
        "            countpara += 1\n",
        "            if countpara % 100000 == 0: print(countpara)\n",
        "            SLine = SLine.strip()\n",
        "            if SLine == '': continue\n",
        "            LLine = re.split('([ ,\\.\\:։;\\'\\\"\\(\\)\\-\\–\\!\\?\\{\\}\\t\\«\\»]+)', SLine)\n",
        "            # if LLine == '': continue\n",
        "            if LLine: LLParagraphs.append(LLine)\n",
        "    return LLParagraphs\n",
        "\n",
        "\n",
        "# This section applies corrections to an Armenian texts and records which words have been corrected"
      ],
      "metadata": {
        "id": "93xc2P0iX1rS"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llPara2dict(LLParagraphs):\n",
        "    DFreq = {}\n",
        "    p=0\n",
        "    for LPara in LLParagraphs:\n",
        "        p+=1\n",
        "        if p%200000 == 0:\n",
        "            print(p)\n",
        "        # if LPara == [] or LPara == ['']: continue\n",
        "        if LPara == []: continue\n",
        "        # FOut.write(str(LPara) + '\\n')\n",
        "        i = 0 # counting words\n",
        "        for el in LPara:\n",
        "            i+=1 # index of next word\n",
        "            try:\n",
        "                DFreq[el] += 1\n",
        "            except:\n",
        "                DFreq[el] = 1\n",
        "    return DFreq\n"
      ],
      "metadata": {
        "id": "xGYc-CldQ5_X"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def printFrqDictinary(DWiki, FOut):\n",
        "    for key, val in sorted(DWiki.items(), key=lambda item: item[1], reverse=True):\n",
        "        FOut.write(f'{key}\\t{val}\\n')"
      ],
      "metadata": {
        "id": "93dCDfd8OMF-"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading Armenian corpus, correcting lines, tokenizing"
      ],
      "metadata": {
        "id": "QUdj_vsYzF5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# core starts here - critical\n",
        "\n",
        "!wget https://heibox.uni-heidelberg.de/f/c977e87cf2b244e6801b/?dl=1\n",
        "!mv index.html?dl=1 KorpusARM.tgz\n"
      ],
      "metadata": {
        "id": "z-5mB-CUSTTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xvzf KorpusARM.tgz\n",
        "!mkdir KorpusARM1\n",
        "!mkdir KorpusARM1/stage01\n",
        "# concatenating files\n",
        "!cat korpusARM/hyFiktion/* >KorpusARM1/stage01/hyFiktion.txt\n",
        "!cat korpusARM/hyNatur/* >KorpusARM1/stage01/hyNatur.txt\n",
        "!cat korpusARM/hyRecht/* >KorpusARM1/stage01/hyRecht.txt\n",
        "!mkdir KorpusARM1/stage02\n",
        "\n",
        "# function for Armenian line breaks:\n",
        "\n",
        "def correctLineBreaksHY(FName, FNameOut):\n",
        "    FIn = open(FName, 'r')\n",
        "    FOut = open(FNameOut, 'w')\n",
        "    countHyphens = 0\n",
        "    for SLine in FIn:\n",
        "        SLine = SLine.strip()\n",
        "        if SLine == '':\n",
        "            FOut.write('\\n\\n')\n",
        "            continue\n",
        "        if SLine[-1] == '-':\n",
        "            SLine2write = SLine[:-1]\n",
        "            FOut.write(SLine2write)\n",
        "            countHyphens +=1\n",
        "            continue\n",
        "        FOut.write(SLine + ' ')\n",
        "    FOut.flush()\n",
        "    print(str(countHyphens) + ' hyphens corrected')\n",
        "    return\n",
        "\n",
        "correctLineBreaksHY('KorpusARM1/stage01/hyFiktion.txt', 'KorpusARM1/stage02/hyFiktion.txt')\n",
        "correctLineBreaksHY('KorpusARM1/stage01/hyNatur.txt', 'KorpusARM1/stage02/hyNatur.txt')\n",
        "correctLineBreaksHY('KorpusARM1/stage01/hyRecht.txt', 'KorpusARM1/stage02/hyRecht.txt')\n"
      ],
      "metadata": {
        "id": "pywrhPCXSVjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!wc KorpusARM1/stage02/hyFiktion.txt\n",
        "!wc KorpusARM1/stage02/hyNatur.txt\n",
        "!wc KorpusARM1/stage02/hyRecht.txt"
      ],
      "metadata": {
        "id": "kUqWQMklS2oD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea81b350-76d4-4163-cd98-2b4e720f5beb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   6208   92131 1081755 KorpusARM1/stage02/hyFiktion.txt\n",
            "  3642  67142 870081 KorpusARM1/stage02/hyNatur.txt\n",
            "   8940   86621 1288655 KorpusARM1/stage02/hyRecht.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### now we tokenize the Armenian corpora"
      ],
      "metadata": {
        "id": "R5p98dttxoIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    del LLParaHyF\n",
        "except:\n",
        "    sys.stderr.write(f'LLParaHyF doens\\'t need to be deleted, not yet created...\\n')\n",
        "\n",
        "try:\n",
        "    del LLParaHyN\n",
        "except:\n",
        "    sys.stderr.write(f'LLParaHyN doens\\'t need to be deleted, not yet created...\\n')\n",
        "\n",
        "try:\n",
        "    del LLParaHyR\n",
        "except:\n",
        "    sys.stderr.write(f'LLParaHyR doens\\'t need to be deleted, not yet created...\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKN29fknxriB",
        "outputId": "1ec82fe4-4e4d-4332-f98c-5de80592a179"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "LLParaHyF doens't need to be deleted, not yet created...\n",
            "LLParaHyN doens't need to be deleted, not yet created...\n",
            "LLParaHyR doens't need to be deleted, not yet created...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LLParaHyF = tokenizeTextHY('/content/KorpusARM1/stage02/hyFiktion.txt')\n",
        "LLParaHyN = tokenizeTextHY('/content/KorpusARM1/stage02/hyNatur.txt')\n",
        "LLParaHyR = tokenizeTextHY('/content/KorpusARM1/stage02/hyRecht.txt')"
      ],
      "metadata": {
        "id": "fhxY0A7JzlAP"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we check how our specialized corpora was tokenized...\n",
        "print(LLParaHyF[1])\n",
        "print(len(LLParaHyF[1]))\n",
        "print(len(LLParaHyF))\n",
        "\n",
        "print(LLParaHyN[1])\n",
        "print(len(LLParaHyN[1]))\n",
        "print(len(LLParaHyN))\n",
        "\n",
        "print(LLParaHyR[1])\n",
        "print(len(LLParaHyR[1]))\n",
        "print(len(LLParaHyR))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7baZ5H509DU",
        "outputId": "fd91fb4e-8ed9-43fe-f179-c77a5a2cdc53"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '-- ', 'Բայց', ' ', 'դու', ' ', 'ինճ', ' ', 'անմիջապես', ' ', 'ամբողջովին', ' ', 'չպիտի', ' ', 'կուլ', ' ', 'տաս', ',--- ', 'ասաց', ' ', 'նա', ' ', 'մեղմորեն', ':', '']\n",
            "25\n",
            "3104\n",
            "['Կան', ' ', 'մահվան', ' ', 'քարոզիչներ', ', ', 'ն', ' ', 'երկիրը', ' ', 'լիքն', ' ', 'է', ' ', 'նրանցով', ', ', 'ում', ' ', 'ոլետք', ' ', 'է', ' ', 'կյանքից', ' ', 'հեռացում', ' ', 'քարոզվի', ':', '']\n",
            "29\n",
            "1821\n",
            "['ԳԼՈՒԽ', ' ', '1', ' ', 'ՀԻՄՆԱԿԱՆ', ' ', 'ԴՐՈՒՅԹՆԵՐ']\n",
            "7\n",
            "4471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DHyF = llPara2dict(LLParaHyF)\n",
        "print(len(DHyF))\n",
        "\n",
        "DHyN = llPara2dict(LLParaHyN)\n",
        "print(len(DHyN))\n",
        "\n",
        "DHyR = llPara2dict(LLParaHyR)\n",
        "print(len(DHyR))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDYWidbXKHyA",
        "outputId": "3327fbf9-e8e3-42d5-9b53-b6e75b381e4d"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20748\n",
            "21272\n",
            "10849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wikipedia\n"
      ],
      "metadata": {
        "id": "tV6irX9rkiE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading wikipedia\n",
        "### downloading Armenian Wikipedia\n",
        "!wget https://heibox.uni-heidelberg.de/f/d1f866a61bd545318213/?dl=1\n",
        "!mv index.html?dl=1 hywiki-20221101-pages-articles.txt.gz\n",
        "!gunzip hywiki-20221101-pages-articles.txt.gz\n",
        "# the length of wikipedia\n",
        "\n"
      ],
      "metadata": {
        "id": "e-6E67OXMyaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wc hywiki-20221101-pages-articles.txt"
      ],
      "metadata": {
        "id": "C4q-mZAMr2FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    del LLParaWiki\n",
        "except:\n",
        "    sys.stderr.write(f'LLParaWiki doens\\'t need to be deleted, not yet created...\\n')"
      ],
      "metadata": {
        "id": "iUBiwliBxnhw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f37eabc4-3bfb-4166-8a11-8c7940a28294"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "LLParaWiki doens't need to be deleted, not yet created...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LLParaWiki = tokenizeTextHY('/content/hywiki-20221101-pages-articles.txt')"
      ],
      "metadata": {
        "id": "UxNe9VvuQfet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(LLParaWiki[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrmWZ2gLRXuy",
        "outputId": "ed23a568-192c-4ad8-95bc-f157d98e46ea"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Հայաստան', ' , ', 'պաշտոնական', ' ', 'անվանումը՝', ' ', 'Հայաստանի', ' ', 'Հանրապետություն', ', ', 'պետություն', ' ', 'Առաջավոր', ' ', 'Ասիայում՝', ' ', 'Հայկական', ' ', 'լեռնաշխարհի', ' ', 'հյուսիսարևելյան', ' ', 'մասում', '։ ', 'Քաղաքական', ' ', 'և', ' ', 'մշակութային', ' ', 'իմաստով', ', ', 'սակայն', ', ', 'գտնվում', ' ', 'է', ' ', 'հարավարևելյան', ' ', 'Եվրոպայի', ' ', 'Կովկասյան', ' ', 'տարածաշրջանում', '։ ', 'Հյուսիսում', ' ', 'սահմանակցում', ' ', 'է', ' ', 'Վրաստանին', ', ', 'արևելքում՝', ' ', 'Ադրբեջանին', ', ', 'հարավում՝', ' ', 'Իրանին', ', ', 'իսկ', ' ', 'արևմուտքում՝', ' ', 'Թուրքիային', '։ ', 'Հարավարևելյան', ' ', 'կողմում', ' ', 'Բերձորի', ' ', 'միջանցքով', ' ', 'կապվում', ' ', 'է', ' ', 'Արցախի', ' ', 'Հանրապետությանը', ', ', 'իսկ', ' ', 'հարավ', '-', 'արևմուտքում', ' ', 'Ադրբեջանի', ' ', 'էքսկլավ', ' ', 'Նախիջևանի', ' ', 'Ինքնավար', ' ', 'Հանրապետությունն', ' ', 'է', '։ ', 'Այժմյան', ' ', 'ՀՀ', '-', 'ն', ' ', 'զբաղեցնում', ' ', 'է', ' ', 'պատմական', ' ', 'Հայաստանի', ' ', 'տարածքի', ' ', 'միայն', ' ', 'մեկ', ' ', 'տասներորդը՝', ' ', 'Այրարատ', ' ', 'և', ' ', 'Սյունիք', ' ', 'նահանգների', ' ', 'մի', ' ', 'մասը', '։', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(LLParaWiki[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93CznixbRn2h",
        "outputId": "761fd9ce-a86e-4e2a-ef3f-47a691b24ce8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "137"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(LLParaWiki)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1nH00HmQ1Ld",
        "outputId": "b62ce7b3-b608-4f81-8ad3-c38f2c212855"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2153019"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DWiki = llPara2dict(LLParaWiki)\n",
        "print(len(DWiki))"
      ],
      "metadata": {
        "id": "MoTwYK30SDw-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a0ce880-6ca7-42a7-ce3c-24ae87ba2ee2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200000\n",
            "400000\n",
            "600000\n",
            "800000\n",
            "1000000\n",
            "1200000\n",
            "1400000\n",
            "1600000\n",
            "1800000\n",
            "2000000\n",
            "2071984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(DWiki)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHnj65aZSaxi",
        "outputId": "0d466727-0b27-4b30-d2a3-031b0b93e968"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2071984"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FOut = open('hywiki-frqDict.txt', 'w')\n",
        "for key, val in sorted(DWiki.items(), key=lambda item: item[1], reverse=True):\n",
        "    FOut.write(f'{key}\\t{val}\\n')"
      ],
      "metadata": {
        "id": "fZpCsWIZShqu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wc hywiki-frqDict.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvTzG-NfTNO9",
        "outputId": "868ef113-4444-4b8e-d0ae-7c65b4e7f537"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 2071974  4211029 42482907 hywiki-frqDict.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=40 hywiki-frqDict.txt"
      ],
      "metadata": {
        "id": "ZXcZTSY4TRtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discover candidate rewriting rules systematically"
      ],
      "metadata": {
        "id": "6p6DaZz45mBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getPrefInfSuf(wrd1, wrd2):\n",
        "    try:\n",
        "        cpref = os.path.commonprefix([wrd1, wrd2])\n",
        "        drw1 = wrd1[::-1]\n",
        "        drw2 = wrd2[::-1]\n",
        "        ffusc = os.path.commonprefix([drw1, drw2])\n",
        "        csuff = ffusc[::-1]\n",
        "    except:\n",
        "        sys.stderr.write('error finding pref- and suffix')\n",
        "        cpref = None\n",
        "        csuff = None\n",
        "\n",
        "    try:\n",
        "        wrd1minpref = wrd1.removeprefix(cpref)\n",
        "        wrd2minpref = wrd2.removeprefix(cpref)\n",
        "        wrd1centre = wrd1minpref.removesuffix(csuff)\n",
        "        wrd2centre = wrd2minpref.removesuffix(csuff)\n",
        "    except:\n",
        "        sys.stderr.write('error finding centre 1 and 2')\n",
        "        wrd1centre = None\n",
        "        wrd2centre = None\n",
        "\n",
        "    return cpref, wrd1centre, wrd2centre, csuff\n",
        "\n",
        "\n",
        "P12, I1, I2, S12 = getPrefInfSuf('[перепливи]', '[перелови]')\n",
        "print(P12, ' | ', I1, ' > ', I2, ' | ', S12)\n",
        "\n",
        "P12, I1, I2, S12 = getPrefInfSuf('[розгубився]', '[розгубивсь]')\n",
        "print(P12, ' | ', I1, ' > ', I2, ' | ', S12)\n",
        "\n",
        "P12, I1, I2, S12 = getPrefInfSuf('[вловив]', '[зловив]')\n",
        "print(P12, ' | ', I1, ' > ', I2, ' | ', S12)\n",
        "\n",
        "P12, I1, I2, S12 = getPrefInfSuf('[переходити]', '[перешкодити]')\n",
        "print(P12, ' | ', I1, ' > ', I2, ' | ', S12)\n",
        "\n",
        "P12, I1, I2, S12 = getPrefInfSuf('[переходити]', '[перешкоджати]')\n",
        "print(P12, ' | ', I1, ' > ', I2, ' | ', S12)\n",
        "\n",
        "P12, I1, I2, S12 = getPrefInfSuf('[ходити]', '[перешкоджати]')\n",
        "print(P12, ' | ', I1, ' > ', I2, ' | ', S12)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "P12, I1, I2, S12 = getPrefInfSuf('[մերճակա]', '[մերձակա]')\n",
        "print(P12, ' | ', I1, ' > ', I2, ' | ', S12)\n",
        "\n",
        "P12, I1, I2, S12 = getPrefInfSuf('[առջն]', '[առջև]')\n",
        "print(P12, ' | ', I1, ' > ', I2, ' | ', S12)\n",
        "\n",
        "P12, I1, I2, S12 = getPrefInfSuf('[թեթնություն]', '[թեթևություն]')\n",
        "print(P12, ' | ', I1, ' > ', I2, ' | ', S12)\n",
        "\n",
        "P12, I1, I2, S12 = getPrefInfSuf('[Եթենա]', '[եթե նա]')\n",
        "print(P12, ' | ', I1, ' > ', I2, ' | ', S12)\n",
        "\n",
        "P12, I1, I2, S12 = getPrefInfSuf('[ննա]', '[նա]')\n",
        "print(P12, ' | ', I1, ' > ', I2, ' | ', S12)\n",
        "\n",
        "P12, I1, I2, S12 = getPrefInfSuf('[ճեռքերն]', '[ձեռքից]')\n",
        "print(P12, ' | ', I1, ' > ', I2, ' | ', S12)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEVPQBz5di9P",
        "outputId": "3416c824-b4b7-42a7-e31d-f3ae08b3517e"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[пере  |  пли  >  ло  |  ви]\n",
            "[розгубивс  |  я  >  ь  |  ]\n",
            "[  |  в  >  з  |  ловив]\n",
            "[пере  |  х  >  шк  |  одити]\n",
            "[пере  |  ходи  >  шкоджа  |  ти]\n",
            "[  |  ходи  >  перешкоджа  |  ти]\n",
            "[մեր  |  ճ  >  ձ  |  ակա]\n",
            "[առջ  |  ն  >  և  |  ]\n",
            "[թեթ  |  ն  >  և  |  ություն]\n",
            "[  |  Եթե  >  եթե   |  նա]\n",
            "[ն  |    >  ա]  |  նա]\n",
            "[  |  ճեռքերն  >  ձեռքից  |  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def createSufList(a):\n",
        "    ''' odyty] --> odyty], odyty, odyt, ody, od, o, \"\" '''\n",
        "    LSuf = []\n",
        "    for j in range(1,len(a)+1):\n",
        "        prefix = a[:j]\n",
        "        LSuf.append(prefix)\n",
        "        # print(j, len(prefix), LSuf)\n",
        "    LSuf.insert(0, '')\n",
        "    return LSuf\n",
        "\n",
        "createSufList('odyty]')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwkewYUg_UVx",
        "outputId": "779c707a-a5d8-43f1-b987-b44974012fdd"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', 'o', 'od', 'ody', 'odyt', 'odyty', 'odyty]']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "createSufList('ություն]')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gHaX0HYnKLS",
        "outputId": "24a370d0-880a-4a10-9c25-7d927a1204f1"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', 'ո', 'ու', 'ութ', 'ությ', 'ությո', 'ությու', 'ություն', 'ություն]']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def createPrefList(s):\n",
        "    LPref = [s[-i:] for i in range(1, len(s) + 1)]\n",
        "    LPref.insert(0, '')\n",
        "    # print(LPref)\n",
        "    return LPref\n",
        "\n",
        "createPrefList('[pere')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5b8Q6y1Ccwa",
        "outputId": "9579c9f4-5729-418c-df13-e80412bb372c"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', 'e', 're', 'ere', 'pere', '[pere']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "createPrefList('[թեթ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrXJiCPknYyq",
        "outputId": "aad228dd-e778-4696-f9bd-601b710a68fd"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', 'թ', 'եթ', 'թեթ', '[թեթ']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### we create the list of potential rewrite rules\n"
      ],
      "metadata": {
        "id": "72QxAaxMEEpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def twoWords2listOfRules(W1, W2):\n",
        "    DTRules = {}\n",
        "    DTRulesLen = {}\n",
        "    LTRules = []\n",
        "    P12, I1, I2, S12 = getPrefInfSuf(W1, W2)\n",
        "    LPref12 = createPrefList(P12)\n",
        "    LSuf12 = createSufList(S12)\n",
        "\n",
        "    for pref in LPref12:\n",
        "        for suf in LSuf12:\n",
        "            SlhsRule = pref + I1 + suf\n",
        "            SrhsRule = pref + I2 + suf\n",
        "            TRules = (SlhsRule, SrhsRule)\n",
        "            LTRules.append(TRules)\n",
        "            try:\n",
        "                DTRules[TRules] += 1\n",
        "            except:\n",
        "                DTRules[TRules] = 1\n",
        "\n",
        "            try:\n",
        "                DTRulesLen[TRules] = len(SlhsRule)\n",
        "            except:\n",
        "                continue\n",
        "    return LTRules, DTRules, DTRulesLen, P12, I1, I2, S12\n",
        "\n",
        "def printFrqTDict(DTFrq, FOut = None):\n",
        "    for key, val in sorted(DTFrq.items(), key=lambda x:x[1], reverse=True):\n",
        "        LHS, RHS = key\n",
        "        if FOut:\n",
        "            FOut.write(f'{LHS}, {RHS}, {str(val)}\\n')\n",
        "        else:\n",
        "            print(LHS, RHS, str(val))\n",
        "    if FOut:\n",
        "        FOut.flush()\n",
        "\n",
        "def printTList(LTuples, FOut = None):\n",
        "    for el in LTuples:\n",
        "        LHS, RHS = el\n",
        "        if FOut:\n",
        "            FOut.write(f'{LHS}, {RHS}\\n')\n",
        "        else:\n",
        "            print(LHS, RHS)\n",
        "    if FOut:\n",
        "        FOut.flush()\n",
        "\n"
      ],
      "metadata": {
        "id": "yr6ic-xSEO_2"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LTRules, DTRules, DTRulesLen, P12, I1, I2, S12 = twoWords2listOfRules('[перепливи]', '[перелови]')\n",
        "# FOut = open('printTRules0-test-uk.txt', 'w')\n",
        "# printFrqTDict(DTRules, FOut)\n",
        "FOut0 = open('printTRulesLen-test-uk.txt', 'w')\n",
        "FOut0.write(f'{P12}|{I1}>{I2}|{S12}\\n')\n",
        "printFrqTDict(DTRulesLen, FOut0)\n",
        "FOut1 = open('printTRules-test-uk.txt', 'w')\n",
        "printTList(LTRules, FOut1)"
      ],
      "metadata": {
        "id": "iZb5I0TkvAhX"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LTRules, DTRules, DTRulesLen, P12, I1, I2, S12 = twoWords2listOfRules('[թեթնություն]', '[թեթևություն]')\n",
        "FOut0 = open('printTRulesLen-test-hy.txt', 'w')\n",
        "FOut0.write(f'{P12}|{I1}>{I2}|{S12}\\n')\n",
        "printFrqTDict(DTRulesLen, FOut0)\n",
        "\n"
      ],
      "metadata": {
        "id": "2T94x2cboVls"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ... todo: check if we need this function (possibly -- substring)\n",
        "def findLongestMatch(SInput, DTRulesLength):\n",
        "    SOutput = None\n",
        "    for key, val in sorted(DTRulesLength.items(), key=lambda x:x[1], reverse=True):\n",
        "        LHS, RHS = key\n",
        "        if LHS in SInput:\n",
        "            SOutput = SInput.replace(LHS, RHS, 1)\n",
        "            break\n",
        "    return SOutput\n",
        "\n",
        "SOutput1 = findLongestMatch('[перепливи]', DTRulesLen)\n",
        "if SOutput1: print(SOutput1)\n",
        "\n",
        "SOutput1 = findLongestMatch('[перешкоджав]', DTRulesLen)\n",
        "if SOutput1: print(SOutput1)\n",
        "\n",
        "SOutput1 = findLongestMatch('[проходжав]', DTRulesLen)\n",
        "if SOutput1: print(SOutput1)\n"
      ],
      "metadata": {
        "id": "M9o5f8XpJ6Fo",
        "outputId": "8d220051-2a16-464b-cf90-63e14095352e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[перелови]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algorithm:\n",
        "- Further we create a common dictionary for all the rewrite strings, we record their frequencies, then we resort them by their length..."
      ],
      "metadata": {
        "id": "k69EUsAaxR3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File with corrections"
      ],
      "metadata": {
        "id": "DWpjU8WNq_bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dowloading the file with corrections\n",
        "# !wget https://heibox.uni-heidelberg.de/f/14706c04a4024b2f937d/?dl=1\n",
        "# without ճեր\n",
        "!wget https://heibox.uni-heidelberg.de/f/4a24540473564788853d/?dl=1\n",
        "\n",
        "!mv index.html?dl=1 Pilot-Corrections-all.tsv\n"
      ],
      "metadata": {
        "id": "n2qCSxAKT8qT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wc Pilot-Corrections-all.tsv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBeYpGDerFSB",
        "outputId": "e255f5f2-85da-44f1-9b3c-09b2b8caaf0c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  324  2854 26730 Pilot-Corrections-all.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# critical path function\n",
        "def readCorrectionsFrq(colNumberOri, colNumberCorrect, colNumberFrq, SFIn, SFOut = None):\n",
        "    LTWrongCorrect = []\n",
        "    '''\n",
        "    if type(LTWrongCorrect) == list:\n",
        "        pass\n",
        "    '''\n",
        "\n",
        "    DWrongCorrect = {}\n",
        "    FOut = open(SFOut, 'w')\n",
        "    with open(SFIn, 'r') as FIn:\n",
        "        count = 0\n",
        "        for SLine in FIn:\n",
        "            count += 1\n",
        "            if count == 1: continue\n",
        "            SLine = SLine.rstrip('\\n')\n",
        "            LLine = SLine.split('\\t')\n",
        "            SWrong = LLine[colNumberOri]\n",
        "            SCorrect = LLine[colNumberCorrect]\n",
        "            SFrq = LLine[colNumberFrq]\n",
        "            if SWrong != '' and SCorrect != '' and SWrong != SCorrect:\n",
        "                # TWrongCorrect = (f'[{SWrong}]', f'[{SCorrect}]', f'{SFrq}')\n",
        "                TWrongCorrect = (f'[{SWrong}]', f'[{SCorrect}]', int(SFrq))\n",
        "                LTWrongCorrect.append(TWrongCorrect)\n",
        "                if SWrong in DWrongCorrect.keys():\n",
        "                    SCorrect1 = DWrongCorrect[SWrong]\n",
        "                    if SCorrect1 != SCorrect:\n",
        "                        print(SWrong + '\\t' + SCorrect1 + '\\t' + SCorrect)\n",
        "                DWrongCorrect[SWrong] = SCorrect\n",
        "    if SFOut:\n",
        "        for SWrong, SCorrect, SFrq in LTWrongCorrect:\n",
        "            FOut.write(f'{SWrong}\\t{SCorrect}\\t{SFrq}\\n')\n",
        "        FOut.flush()\n",
        "    print(len(DWrongCorrect))\n",
        "\n",
        "    return LTWrongCorrect, DWrongCorrect\n"
      ],
      "metadata": {
        "id": "cuDRPHpQUGxD"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading corrections for word forms, with the purpose of generalizing them\n",
        "# goal: to display candidates for correction -- based on existing corrections (?)\n",
        "LTWrongCorrectWordF, DWrongCorrectWordF = readCorrectionsFrq(1, 4, 9, '/content/Pilot-Corrections-all.tsv', SFOut = 'Pilot-Corrections-all-WordForm.tsv')\n",
        "# LTWrongCorrectLemmaF, DWrongCorrectLemmaF = readCorrectionsFrq(3, 6, 9, '/content/Pilot-Corrections-all.tsv', SFOut = 'Pilot-Corrections-all-Lemma.tsv')\n",
        "print(LTWrongCorrectWordF)\n",
        "# print(LTWrongCorrectLemmaF)\n",
        "# ինձ|ինչ\n",
        "# առջև|առջևից\n",
        "# ինչ|ինձ\n",
        "# ինչ|ես\n",
        "# գիտենալ|իմանալ\n"
      ],
      "metadata": {
        "id": "6q3Qd74vUVET",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3229bd75-bf3c-4226-a1a3-8af529a572ef"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ինճ\tինձ\tինչ\n",
            "առջնից\tառջև\tառջևից\n",
            "ինճ\tինչ\tինձ\n",
            "171\n",
            "[('[մերճակա]', '[մերձակա]', 4), ('[առջն]', '[առջև]', 4), ('[թեթնություն]', '[թեթևություն]', 4), ('[Եթենա]', '[եթե նա]', 4), ('[ննա]', '[նա]', 4), ('[ճեռքերն]', '[ձեռքից]', 4), ('[ճայն]', '[ձայն]', 4), ('[ճեռքով]', '[ձեռքով]', 4), ('[այլնս]', '[այլևս]', 4), ('[ետնի]', '[ետևի]', 4), ('[կեղնները]', '[կեղևները]', 4), ('[ճկան]', '[ձկան]', 4), ('[ննա]', '[նա]', 4), ('[բանաձնի]', '[բանաձևի]', 4), ('[առնտրական]', '[առևտրական]', 4), ('[արնելյան]', '[արևելյան]', 4), ('[կուղնորվի]', '[կուղևորվի]', 4), ('[նս]', '[ևս]', 4), ('[հետնեց]', '[հետևել]', 4), ('[նուխիսկ]', '[նույնիսկ]', 4), ('[երնույթ]', '[երևույթ]', 4), ('[քրտնքով]', '[քրտինքով]', 4), ('[արվարճանում]', '[արվարձանում]', 4), ('[անճամբ]', '[անձամբ]', 4), ('[ճայնը]', '[ձայնը]', 4), ('[ճգվում]', '[ձգվում]', 4), ('[ճիու]', '[ձիու]', 4), ('[դարճնում]', '[դարձնում]', 4), ('[ուղնորվում]', '[ուղևորվում ]', 4), ('[իջնանատան]', '[իջևանատան]', 4), ('[ճգտում]', '[ձգտում]', 4), ('[դրսնորում]', '[դրսևորում]', 3), ('[արվարճանի]', '[արվարձանի]', 3), ('[ետնում]', '[ետևում]', 3), ('[ճնավորված]', '[ձևավորված]', 3), ('[Հետնաբար]', '[հետևաբար]', 3), ('[այլնս]', '[այլևս]', 3), ('[Շավանաբար]', '[հավանաբար]', 3), ('[սկզբիցնեթ]', '[սկզբիցևեթ]', 3), ('[միջն]', '[միջև]', 3), ('[համաճայնության]', '[համաձայնություն]', 3), ('[Թերնս]', '[թերևս]', 3), ('[տերնի]', '[տերև]', 3), ('[թնատակերի]', '[թևատակ]', 3), ('[դոււս]', '[դուրս]', 3), ('[բարճրացավ]', '[բարձրանալ]', 3), ('[երկարատն]', '[երկարատև]', 3), ('[նախնառաջ]', '[նախևառաջ]', 3), ('[ներքնից]', '[ներքև]', 3), ('[առջնից]', '[առջև]', 3), ('[ճեռնոցագործական]', '[ձեռնոցագործական]', 3), ('[հարնան]', '[գարնան]', 3), ('[ճգտումը]', '[ձգտումը]', 3), ('[առջնում]', '[առջևում]', 3), ('[ճմռանը]', '[ձմռանը]', 3), ('[սնահեր]', '[սևահեր]', 3), ('[ծանրութեթն]', '[ծանրութեթև]', 3), ('[ճագի]', '[ձագի]', 3), ('[Այլնս]', '[այլևս]', 3), ('[ինճ]', '[ինձ]', 3), ('[ճեռքն]', '[ձեռքն]', 3), ('[արճագանք]', '[արձագանք]', 3), ('[ճմեռ]', '[ձմեռ]', 2), ('[պառկածէրգերեզմանատանվրա]', '[պառկած էր գերեզմանատան վրա]', 2), ('[ճմերուկների]', '[ձմերուկների]', 2), ('[ունողկալի]', '[ու նողկալի]', 2), ('[գլխապտույտնե]', '[գլխապտույտներ]', 2), ('[ճայնի]', '[ձայնի]', 2), ('[ճկների]', '[ձկների]', 2), ('[այլնայլ]', '[այլևայլ]', 2), ('[ճնականություններից]', '[ձևականություններից]', 2), ('[չուննորներին]', '[չունևորներին]', 2), ('[արճակվող]', '[արձակվել]', 2), ('[խամարդ]', '[տղամարդ]', 2), ('[Միգուցենա]', '[միգուցե նա]', 2), ('[ջղաճգութու]', '[Ջղաձգություն]', 2), ('[Դետեսնում]', '[դե տեսնում ]', 2), ('[բարճրաց]', '[բարձրացնել]', 2), ('[ինճ]', '[ինձ]', 2), ('[Որովհետն]', '[որովհետև]', 2), ('[ննույնիսկ]', '[նույնիսկ]', 2), ('[հուսահատեգնում]', '[հեւսահատեցնել]', 2), ('[որնրան]', '[որ նրան]', 2), ('[Ջգազմունքներից]', '[Զգացմունք]', 2), ('[մնահավատությոն]', '[սնահավատություն]', 2), ('[հեթանոսա]', '[հեթանոսացում]', 2), ('[խարույկՄ]', '[խարույկ]', 2), ('[տուցում]', '[մատուցում]', 2), ('[նականռթյան]', '[բանականություն]', 2), ('[անձր]', '[անձ, անձրև]', 2), ('[տարօրի]', '[տարօրինակ]', 2), ('[թնիկները]', '[թևիկ]', 2), ('[աստվածավախու]', '[աստվածավախություն]', 2), ('[բարճր]', '[բարձր]', 2), ('[լավէ]', '[լավ է]', 2), ('[Թռենել]', '[թռնել]', 2), ('[հանճնեց]', '[հանձնել]', 2), ('[Դհյոյում]', '[Դյո]', 2), ('[տակիզ]', '[տակից]', 2), ('[առջն]', '[առջև]', 36), ('[ետնից]', '[ետևից]', 30), ('[միջն]', '[միջև]', 22), ('[այնուհետն]', '[այնուհետև]', 22), ('[բացարճակապես]', '[բացարձակապես]', 21), ('[որնէ]', '[որևէ]', 20), ('[այլնս]', '[այլևս]', 19), ('[թեթն]', '[թեթև]', 18), ('[թեթնակի]', '[թեթևակի]', 17), ('[որնէ]', '[որևէ]', 16), ('[ետնում]', '[ետևում]', 16), ('[արնի]', '[արևի]', 14), ('[միննույն]', '[միևնույն]', 13), ('[վերնում]', '[վերևում]', 13), ('[ինճ]', '[ինչ]', 13), ('[արճակում]', '[արձակում]', 12), ('[թերնս]', '[թերևս]', 12), ('[երբնիցե]', '[երբևիցե]', 11), ('[ճեռքը]', '[ձեռքը]', 10), ('[երնակայության]', '[երևակայության]', 10), ('[արնմուտք]', '[արևմուտք]', 10), ('[ճեռք]', '[ձեռք]', 10), ('[ճեռքի]', '[ձեռքի]', 9), ('[ճայնով]', '[ձայնով]', 9), ('[նան]', '[նաև]', 8), ('[դեռնս]', '[դեռևս]', 8), ('[ճեռքերը]', '[ձեռքերը]', 8), ('[հետնում]', '[հետևում]', 8), ('[արճակող]', '[արձակող]', 8), ('[որնիցե]', '[որևիցե]', 8), ('[դեռնս]', '[դեռևս]', 8), ('[այլնս]', '[այլևս]', 8), ('[ճեռքին]', '[ձեռքին]', 7), ('[արնմտյան]', '[արևմտյան]', 7), ('[այլես]', '[այլևս]', 7), ('[Նախնառաջ]', '[նախևառաջ]', 7), ('[հոգնոր]', '[հոգևոր]', 6), ('[ճեռքերով]', '[ձեռքերով]', 6), ('[ներքնում]', '[ներքևում]', 6), ('[համարճակվում]', '[համարձակվում]', 5), ('[արնելք]', '[արևելք]', 5), ('[առանճին]', '[առանձին]', 5), ('[արնմուտքից]', '[արևմուտքից]', 5), ('[երնակայական]', '[երևակայական]', 5), ('[թնածում]', '[թևածում]', 5), ('[բանաճնի]', '[բանաձևի]', 5), ('[դարճավ]', '[դարձավ]', 5), ('[բանաճնը]', '[բանաձևը]', 5), ('[թեթնացած]', '[թեթևացած]', 5), ('[հետնելով]', '[հետևելով]', 5), ('[հետնել]', '[հետևել]', 5), ('[կարնոր]', '[կարևոր]', 5), ('[ճիու]', '[ձիու]', 5), ('[հեղճուցիչ]', '[հեղձուցիչ]', 5), ('[անճնական]', '[անձնական]', 5), ('[առջնից]', '[առջևից]', 3), ('[երնում]', '[երևում]', 3), ('[ինճ]', '[ինձ]', 3), ('[փորճ]', '[փորձ]', 3), ('[բանաձնը]', '[բանաձևը]', 3), ('[եթենա]', '[եթե]', 3), ('[ինճ]', '[ինձ]', 3), ('[փորճն]', '[փորձն]', 3), ('[բանաձն]', '[բանաձևն]', 3), ('[բանաճն]', '[բանաձևն]', 3), ('[բարճրացնել]', '[բարձրացնել]', 3), ('[Այլնս]', '[այլևս]', 3), ('[սնահեր]', '[սևահեր]', 3), ('[բացարճակ]', '[բացարձակ]', 3), ('[ճգտելով]', '[ձգտելով]', 3), ('[բանաճներ]', '[բանաձևեր]', 3), ('[թեն]', '[թեև]', 3), ('[տերնները]', '[տերևերը]', 3), ('[բարճունքներում]', '[բարձունքներում]', 3), ('[առնտուրը]', '[առևտուրը]', 3), ('[անճամբ]', '[անձամբ]', 3), ('[օթնան]', '[օթևան]', 3), ('[բանաճներով]', '[բանաձներով]', 3), ('[արճակել]', '[արձակել]', 3), ('[բնեռը]', '[բևեռը]', 3), ('[օճեր]', '[օձեր]', 3), ('[ոջ]', '[ոչ]', 3), ('[անձրնը]', '[անձրևը]', 3), ('[թեթն]', '[թեթև]', 3), ('[ճգում]', '[ձգում]', 3), ('[տաքագնում]', '[տաքացնում]', 3), ('[Հնայած]', '[Չնայած]', 3), ('[ճեռքերի]', '[Ձեռքերի]', 3), ('[այխքան]', '[այդքան]', 3), ('[ճնացրեց]', '[ձևացրեց]', 3), ('[երնակայությունների]', '[երևակայությունների]', 3), ('[առջն]', '[առջև]', 3), ('[բարճր]', '[բարձր]', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(DWrongCorrectWordF))\n",
        "# print(len(DWrongCorrectLemmaF))\n"
      ],
      "metadata": {
        "id": "-_FYBzw7U42k",
        "outputId": "9b1c15d3-3b2b-4f06-902d-daeebdf0f297",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in sorted(DWrongCorrectWordF.items()):\n",
        "    print(f'{key}\\t{value}')"
      ],
      "metadata": {
        "id": "pT2fX38FAVve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for SWrong, SCorrect, Frq in sorted(LTWrongCorrectWordF, key=lambda x:x[2], reverse=True):\n",
        "    print(SWrong, SCorrect, Frq)\n",
        "\n"
      ],
      "metadata": {
        "id": "RBhvZXOI5OlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(LTWrongCorrectWordF))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5WB0-F6_Q-b",
        "outputId": "109d89e1-d44f-44db-ae62-4bde2600dd29"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting a common dictionary of rewrite rules\n",
        "- from file with corrections"
      ],
      "metadata": {
        "id": "3T4ROdUF9T1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extractRulesFromLTCorrections(LTWrongCorrect, FOut = None, FOutLen = None):\n",
        "    # the common dictionary of rewrite ruels;\n",
        "    '''\n",
        "      structure:\n",
        "        DRewriteRules[ (LHS, RHS) ] = Frq_CountSame\n",
        "      dictionary for full information\n",
        "        DRewriteRulesInfo []\n",
        "\n",
        "    '''\n",
        "    DRewriteRules = {} # production dictionary\n",
        "    DRewriteRulesLen = {} # sorted by the length\n",
        "\n",
        "    # DRewriteRulesInfo = {}  # full information associated with dictionary entries (in case we need them)\n",
        "\n",
        "    for SWrong, SCorrect, Frq in sorted(LTWrongCorrectWordF, key=lambda x:x[2], reverse=True):\n",
        "        LTRules, DTRules, DTRulesLen, P12, I1, I2, S12 = twoWords2listOfRules(SWrong, SCorrect)\n",
        "        for el in LTRules:\n",
        "            try:\n",
        "                LHS, RHS = el\n",
        "                if LHS == RHS or LHS == '' or RHS == '': continue\n",
        "\n",
        "                # dictionary sorted by the length\n",
        "                DRewriteRulesLen[el] = len(LHS)\n",
        "\n",
        "                try:\n",
        "                    DRewriteRules[el] += 1\n",
        "                except:\n",
        "                    DRewriteRules[el] = 1\n",
        "            except:\n",
        "                sys.stderr.write(f'{el} - cannot be recognised!\\n')\n",
        "\n",
        "    if FOut:\n",
        "        printFrqTDict(DRewriteRules, FOut)\n",
        "\n",
        "    if FOutLen:\n",
        "        printFrqTDict(DRewriteRulesLen, FOutLen)\n",
        "\n",
        "    return DRewriteRules, DRewriteRulesLen\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bzNLVj4p9bm4"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FOutDRewriteRules = open('printTRulesAllFrq-hy.txt', 'w')\n",
        "FOutDRewriteRulesLen = open('printTRulesAllLen-hy.txt', 'w')\n",
        "\n",
        "DRewriteRules, DRewriteRulesLen = extractRulesFromLTCorrections(LTWrongCorrectWordF, FOut = FOutDRewriteRules, FOutLen = FOutDRewriteRulesLen)\n",
        "\n",
        "print(len(DRewriteRules))\n",
        "print(len(DRewriteRulesLen))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6uAoDoTEz2x",
        "outputId": "23aec570-8d00-49f5-e541-bbf22aab85f1"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2725\n",
            "2725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing dictionaries\n",
        "- collecting a frq list of potential OCR errors: words in corpus which are not in Wikipedia"
      ],
      "metadata": {
        "id": "wTmvlcojLYdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dictDifference(D1, D2, FOut1 = None, FOut2 = None, FOut12 = None):\n",
        "    DinD1 = {}\n",
        "    DinD2 = {}\n",
        "    DCommon = {}\n",
        "\n",
        "\n",
        "    for key, frqD1 in D1.items():\n",
        "        if key in D2.keys():\n",
        "            frqD2 = D2[key]\n",
        "            TFrqs = (frqD1, frqD2)\n",
        "            DCommon[key] = TFrqs\n",
        "        else:\n",
        "            DinD1[key] = frqD1\n",
        "\n",
        "    for key, frqD2 in D2.items():\n",
        "        if key in D1.keys():\n",
        "            frqD1 = D1[key]\n",
        "            TFrqs = (frqD1, frqD2)\n",
        "            DCommon[key] = TFrqs\n",
        "        else:\n",
        "            DinD2[key] = frqD2\n",
        "\n",
        "    if FOut1:\n",
        "        printFrqDictinary(DinD1, FOut1)\n",
        "    if FOut2:\n",
        "        printFrqDictinary(DinD2, FOut2)\n",
        "    if FOut12:\n",
        "        printFrqDictinary(DCommon, FOut12)\n",
        "\n",
        "    return DinD1, DinD2, DCommon\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "eahTrvuYLjKi"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(DHyF))\n",
        "print(len(DHyN))\n",
        "print(len(DHyR))\n",
        "print(len(DWiki))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2r30CmUUKYa",
        "outputId": "9edfea56-9a50-488c-bce9-4fab047a3a12"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20748\n",
            "21272\n",
            "10849\n",
            "2071984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FD1WF = open('dictFD1.txt', 'w')\n",
        "FD2WF = open('dictFD2.txt', 'w')\n",
        "FDCommonWF = open('dictFDCommon.txt', 'w')\n",
        "DinD1WF, DinD2WF, DCommonWF = dictDifference(DWiki, DHyF, FOut1 = FD1WF, FOut2 = FD2WF, FOut12 = FDCommonWF)\n",
        "print(len(DinD1WF))\n",
        "print(len(DinD2WF))\n",
        "print(len(DCommonWF))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk2OX9KMUBre",
        "outputId": "ba2fd6c2-d840-4254-afd1-bcfd36b6981d"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2055434\n",
            "4198\n",
            "16550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FD1WN = open('dictND1.txt', 'w')\n",
        "FD2WN = open('dictND2.txt', 'w')\n",
        "FDCommonWN = open('dictNDCommon.txt', 'w')\n",
        "DinD1WN, DinD2WN, DCommonWN = dictDifference(DWiki, DHyN, FOut1 = FD1WN, FOut2 = FD2WN, FOut12 = FDCommonWN)\n",
        "print(len(DinD1WN))\n",
        "print(len(DinD2WN))\n",
        "print(len(DCommonWN))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXnipHXuUdKn",
        "outputId": "ac5b26d8-6db9-4b21-f7e2-b9704fdc904d"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2059280\n",
            "8568\n",
            "12704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FD1WR = open('dictRD1.txt', 'w')\n",
        "FD2WR = open('dictRD2.txt', 'w')\n",
        "FDCommonWR = open('dictRDCommon.txt', 'w')\n",
        "DinD1WR, DinD2WR, DCommonWR = dictDifference(DWiki, DHyR, FOut1 = FD1WR, FOut2 = FD2WR, FOut12 = FDCommonWR)\n",
        "print(len(DinD1WR))\n",
        "print(len(DinD2WR))\n",
        "print(len(DCommonWR))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fnmTME2VT0Q",
        "outputId": "5b6660df-4849-445c-9ed3-d5b3783f13d9"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2062989\n",
            "1854\n",
            "8995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attempting to correct non-found items\n",
        "Algorithm:\n",
        "- for each non-found item:\n",
        "  - go over each rule\n",
        "  - find if it has a substring among LHS rules (one or more)\n",
        "  - apply that, check if the correction exists in wikipedia\n",
        "\n",
        "- save all corrections, from longest to shortest\n",
        "\n"
      ],
      "metadata": {
        "id": "sQz6psqEYfs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(DRewriteRulesLen))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AL_IcrPsZmpd",
        "outputId": "d4756d22-ed3d-4c1b-d965-e18b8d2181ce"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def applyRule2String(TRule, SInput):\n",
        "    '''\n",
        "    applies one rule (tuple: left-right hand side) to a string,\n",
        "      returns all possible candidates which result from rewriting (later to be checked)\n",
        "    '''\n",
        "    LOutput = []\n",
        "\n",
        "    try:\n",
        "        LHS, RHS = TRule\n",
        "    except:\n",
        "        sys.stderr.write(f'{TRule} RuleNotRecognized\\n')\n",
        "        return None\n",
        "    try:\n",
        "        EscLHS = re.escape(LHS)\n",
        "        # print(EscLHS)\n",
        "        RLHS = re.compile(EscLHS)\n",
        "    except:\n",
        "        sys.stderr.write(f'{TRule} Compilation failed\\n')\n",
        "        return None\n",
        "\n",
        "\n",
        "    if LHS in SInput:\n",
        "        SNew = re.sub(RLHS, RHS, SInput)\n",
        "        LOutput.append(SNew)\n",
        "        SNew1 = re.sub(RLHS, RHS, SInput, count=1)\n",
        "        LOutput.append(SNew1)\n",
        "        SNew2 = re.sub(RLHS, RHS, SInput, count=2)\n",
        "        LOutput.append(SNew2)\n",
        "\n",
        "        EOutputUniq = set(LOutput)\n",
        "        LOutputUniq = list(EOutputUniq)\n",
        "    else:\n",
        "        LOutputUniq = []\n",
        "\n",
        "    return LOutputUniq\n",
        "\n",
        "LOutputUniq = applyRule2String(('пли', 'ло'), '[перепливи]')\n",
        "print(LOutputUniq)\n",
        "\n",
        "LOutputUniq = applyRule2String(('[առանճ', '[առանձ'), '[առանճնահատուկ]')\n",
        "print(LOutputUniq)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRoKNQilZBS9",
        "outputId": "998f4899-ce1f-480d-b3d5-955fe2386da3"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[перелови]']\n",
            "['[առանձնահատուկ]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def applyAllRules2Word(SInput, DRewriteRulesLen, DWiki):\n",
        "    '''\n",
        "    apply all existing rules to a given word,\n",
        "      check them in wikipedia\n",
        "      save uniq rewritings\n",
        "    '''\n",
        "    LCandidates = []\n",
        "    SInputBR = f'[{SInput}]' # adding brackets to match brackets in rules\n",
        "    for TRule, Len in sorted(DRewriteRulesLen.items(), key=lambda item: item[1], reverse=True ): # for each rule, sorted by length\n",
        "        LOutputUniq = applyRule2String(TRule, SInputBR)\n",
        "        for SOutputWordBR in LOutputUniq:\n",
        "            SOutputWord = SOutputWordBR.strip('[]')\n",
        "            if SOutputWord in DWiki.keys():\n",
        "                TOutput = (SOutputWord,TRule,Len)\n",
        "                LCandidates.append(TOutput)\n",
        "\n",
        "    return LCandidates\n",
        "\n",
        "print(len(DWiki))\n",
        "print(len(DRewriteRulesLen))\n",
        "\n",
        "LCandidates =  applyAllRules2Word('առանճնահատուկ', DRewriteRulesLen, DWiki)\n",
        "print(LCandidates)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwbyKzT_mQaH",
        "outputId": "a0c42315-2e3d-4663-ee18-b5e532bb1a8c"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2071984\n",
            "2725\n",
            "[('առանձնահատուկ', ('[առանճ', '[առանձ'), 6), ('առանձնահատուկ', ('առանճ', 'առանձ'), 5), ('առանձնահատուկ', ('անճնա', 'անձնա'), 5), ('առանձնահատուկ', ('ռանճ', 'ռանձ'), 4), ('առանձնահատուկ', ('նճնա', 'նձնա'), 4), ('առանձնահատուկ', ('անճն', 'անձն'), 4), ('առանձնահատուկ', ('անճ', 'անձ'), 3), ('առանձնահատուկ', ('ճնա', 'ձնա'), 3), ('առանձնահատուկ', ('նճն', 'նձն'), 3), ('առանձնահատուկ', ('նճ', 'նձ'), 2), ('առանձնահատուկ', ('ճն', 'ձն'), 2), ('առանձնահատուկ', ('ճ', 'ձ'), 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def applyAllrules3ListUniq(LCandidates):\n",
        "    LUniqCandidates = []\n",
        "    DUniqCandidates = {}\n",
        "\n",
        "    for TCandRuleFreq in LCandidates:\n",
        "        try:\n",
        "            SCand, TLHSRHS, IFrq = TCandRuleFreq\n",
        "            TOut = TLHSRHS, IFrq\n",
        "            # LHS, RHS = TLHSRHS\n",
        "        except:\n",
        "            sys.stderr.write(f'{TCandRuleFreq} - tuple not recognized\\n')\n",
        "            TOut = None\n",
        "\n",
        "        try:\n",
        "            TValues = DUniqCandidates[SCand]\n",
        "\n",
        "\n",
        "        except:\n",
        "            TValues = []\n",
        "\n",
        "        TValues.append(TOut)\n",
        "        DUniqCandidates[SCand] = TValues\n",
        "\n",
        "    for SCand, TVal in sorted(DUniqCandidates.items(), key=lambda item: item[1][1], reverse=True ):\n",
        "        LUniqCandidates.append((SCand, TVal))\n",
        "\n",
        "\n",
        "\n",
        "    return(LUniqCandidates)\n",
        "\n",
        "LUniqCandidates = applyAllrules3ListUniq(LCandidates)\n",
        "print(LUniqCandidates)\n",
        "for el in LUniqCandidates:\n",
        "    print(el)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AS18y94yuJfl",
        "outputId": "93fc11ed-ed91-404f-e4b7-c6c2c7749aa1"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('առանձնահատուկ', [(('[առանճ', '[առանձ'), 6), (('առանճ', 'առանձ'), 5), (('անճնա', 'անձնա'), 5), (('ռանճ', 'ռանձ'), 4), (('նճնա', 'նձնա'), 4), (('անճն', 'անձն'), 4), (('անճ', 'անձ'), 3), (('ճնա', 'ձնա'), 3), (('նճն', 'նձն'), 3), (('նճ', 'նձ'), 2), (('ճն', 'ձն'), 2), (('ճ', 'ձ'), 1)])]\n",
            "('առանձնահատուկ', [(('[առանճ', '[առանձ'), 6), (('առանճ', 'առանձ'), 5), (('անճնա', 'անձնա'), 5), (('ռանճ', 'ռանձ'), 4), (('նճնա', 'նձնա'), 4), (('անճն', 'անձն'), 4), (('անճ', 'անձ'), 3), (('ճնա', 'ձնա'), 3), (('նճն', 'նձն'), 3), (('նճ', 'նձ'), 2), (('ճն', 'ձն'), 2), (('ճ', 'ձ'), 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply corrections to all items in the list not in wikipedia\n",
        "\n",
        "\n",
        "def applyCorrectionRules(DinD2, DRewriteRulesLen, DWiki):\n",
        "\n",
        "    for key, val in DinD2.items(): # for each word that is not found in Wikipedia\n",
        "        LPossibleCandidates = []\n",
        "        SWord = f'[{key}]' # adding being/end symbols\n",
        "        for TRule, Len in sorted(DRewriteRulesLen.items(), key=lambda item: item[1], reverse=True ):\n",
        "            LOutputUniq = applyRule2String(TRule, SWord)\n",
        "            for STestWord in LOutputUniq:\n",
        "                STestWord0 = STestWord.strip('][')\n",
        "                if STestWord0 in DWiki.keys():\n",
        "                    LPossibleCandidates.append(STestWord0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "Lb7I1mdUjxES"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
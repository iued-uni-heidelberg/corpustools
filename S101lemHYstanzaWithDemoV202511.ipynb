{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRWh79vZq4fAIMh3bSjnMY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/corpustools/blob/main/S101lemHYstanzaWithDemoV202511.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Armenian lemmatization with Stanza\n",
        "\n",
        "Version 2025 09 - adjusted for Colab update with PyTorch >2.5 (with the Torch version downgrading route selected)."
      ],
      "metadata": {
        "id": "skix7t6sFaZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## downloading evaluation sets\n",
        "- 420 words: test with about 420 words of Armenian text\n",
        "- Armenian \"Brown-type\" corpus b"
      ],
      "metadata": {
        "id": "fFBRX6lTFfcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download some sample Armenian sentences, or upload your own, change the\n",
        "!wget https://heibox.uni-heidelberg.de/f/ce6096da570f47b99500/?dl=1\n",
        "!mv index.html?dl=1 evaluation-set-v01.txt\n",
        "!wget https://heibox.uni-heidelberg.de/f/a847a12bffd4491f9070/?dl=1\n",
        "!mv index.html?dl=1 TED2020-dehy-hy-aa.txt\n",
        "\n",
        "\n",
        "!wget https://heibox.uni-heidelberg.de/f/7a091a3c0372428a9aa2/?dl=1\n",
        "!mv index.html?dl=1 udhr-all-languages.zip\n",
        "!unzip udhr-all-languages.zip"
      ],
      "metadata": {
        "id": "m549clSLFHs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "# Downloading UDHR\n",
        "# !wget https://unicode.org/udhr/assemblies/udhr_txt.zip\n",
        "# Alternatively, downloading from the UN website, converting pdf to txt\n",
        "# the link aboout pdf2txt conversion: https://chatgpt.com/share/68bbeed8-0d08-800e-8418-b0816c6393d4\n",
        "# !rm --recursive udhr-all-languages\n",
        "\n",
        "# delete lines which are not translations in some files (hy)\n",
        "# delete between lines $a and $b inclusive\n",
        "a=9\n",
        "b=21\n",
        "awk -v m=$a -v n=$b 'm <= NR && NR <= n {next} {print}' < /content/udhr-all-languages/udhr_hye.txt >/content/udhr-all-languages/udhr_hye_v02.txt\n",
        "\n",
        "a=1\n",
        "b=6\n",
        "awk -v m=$a -v n=$b 'm <= NR && NR <= n {next} {print}' < /content/udhr-all-languages/udhr_hye_v02.txt >/content/udhr-all-languages/udhr_hye_v03.txt\n",
        "\n",
        "\n",
        "\n",
        "# put paragraph tags\n",
        "# awk '{print \"<p>\\n\"$0 ; print \"</p>\"}' udhr/udhr_hye2.txt >udhr/udhr_hye_v03.txt\n",
        "\n",
        "\n",
        "cp /content/udhr-all-languages/udhr_hye_v03.txt /content/udhr_hye_v03.txt\n",
        "head --lines=10 /content/udhr_hye_v03.txt"
      ],
      "metadata": {
        "id": "ITpc6TbeZPgL",
        "outputId": "62c4da15-b2b2-41d8-ee34-f76ecd9ada06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ՄԱՐԴՈՒ ԻՐԱՎՈՒՆՔՆԵՐԻ ՀԱՄԸՆԴՀԱՆՈՒՐ ՀՌՉԱԿԱԳԻՐ\n",
            "    ՄԻԱՎՈՐՎԱԾ ԱԶԳԵՐԻ ԿԱԶՄԱԿԵՐՊՈՒԹՅՈՒՆ\n",
            "    ՆԵՐԱԾԱԿԱՆ\n",
            "    Քանզի մարդկային ընտանիքի բոլոր անդամներին ներհատուկ արժանապատվությունը և հավասար ու անօտարելի իրավունքները աշխարհի ազատության, արդարության ու խաղաղության հիմքն են․\n",
            "    Քանզի մարդու իրավունքների նկատմամբ քամահրանքն ու արհամարհանքը հանգեցրել են մարդկության խիղճը խռոված բարբարոսական գործողությունների, և քանի որ այնպիսի աշխարհի ստեղծումը, ուր մարդիկ կվայելեն խոսքի ու համոզմունքների ազատություն և զերծ կլինեն վախից ու կարիքից հռչակվել է որպես մարդկանց բարձրագույն ձգտում․\n",
            "    Քանզի անհրաժեշտ է, որպեսզի մարդը, որպես մի վերջին միջոցի, չդիմի ապստամբության ընդդեմ բռնության ու ճնշման, օրենքի իշխանությամբ պաշտպանել մարդու իրավունքները․\n",
            "    Քանզի անհրաժեշտ է նպաստել ազգերի միջև բարեկամական հարաբերությունների զարգացմանը․\n",
            "    Քանզի Միավորված ազգերի ժողովուրդները կանոնադրության մեջ վերահավաստել են իրենց հավատը մարդու հիմնական իրավունքների, անձի արժանապատվության ու արժեքի, տղամարդու ու կնոջ հավասար իրավունքների նկատմամբ և որոշել են ավելի մեծ ազատության պայմաններում նպաստել սոցիալական առաջընթացին ու կյանքի պայմանների բարելավմանը․\n",
            "    Քանզի անդամ պետությունները պարտավորվել են Միավորված ազգերի հետ համագործակցությամբ հասնել մարդու իրավունքների ու հիմնական ազատությունների նկատմամբ համընդհանուր հարգանքի ու դրանց պահպանման․\n",
            "    Քանզի այս իրավունքների ու ազատությունների համընդհանուր ըմբռնումը կարևորագույն նշանակություն ունի այս պարտավորության լիարժեք իրագործման համար․\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing stanza"
      ],
      "metadata": {
        "id": "i__aUXulFkw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation:\n",
        "\n",
        "It is important to downgrade to Torch 2.5, because Stanza doesn't work with later versions without setting some non-default parameters. See the ChatGPT explanation at: https://chatgpt.com/share/68bad9f0-fc38-800e-a17d-898e6fef0e35\n",
        "\n",
        "This will take some time (around 4 min).\n",
        "\n",
        "You can ignore the error message ``` ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. ``` The script is still working.\n",
        "\n",
        "Note: After running, accept re-launch of the Runtime, if it is requested!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vbn97IPV49NF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdzVArLUF3cb"
      },
      "outputs": [],
      "source": [
        "# it is important to downgrade to Torch 2.5, because Stanza doesn't work with later versions. See the ChatGPT explanation at https://chatgpt.com/share/68bad9f0-fc38-800e-a17d-898e6fef0e35\n",
        "!pip install spacy-stanza\n",
        "!pip install torch==2.5.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "import spacy_stanza\n",
        "import torch\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "# this should be \"Torch version: 2.5.1+cu124\" for Stanza to work!"
      ],
      "metadata": {
        "id": "-VN9g4N4GAR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3573b0c2-0c1c-46a2-cb77-d2d6c1489310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.5.1+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### testing English stanza (optional)"
      ],
      "metadata": {
        "id": "w6Kfwj63FzYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optional\n",
        "# Download the stanza model if necessary\n",
        "stanza.download(\"en\")\n",
        "\n",
        "# Initialize the pipeline\n",
        "nlp = spacy_stanza.load_pipeline(\"en\")\n",
        "\n",
        "doc = nlp(\"Barack Obama was born in Hawaii. He was elected president in 2008.\")\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.dep_, token.ent_type_)\n",
        "print(doc.ents)"
      ],
      "metadata": {
        "id": "dEA7KJdZPrWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading and testing Armenian stanza"
      ],
      "metadata": {
        "id": "HGyKxEJNGG8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stanza.download(\"hy\")\n",
        "# Load Armenian pipeline (make sure stanza models are downloaded first)\n",
        "nlp_hy = spacy_stanza.load_pipeline(\"hy\")"
      ],
      "metadata": {
        "id": "xq53mDsUGumV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optional\n",
        "# Example Armenian text\n",
        "text = \"Ես գնում եմ դպրոց։\"\n",
        "doc = nlp_hy(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(f\"\\n=== Token: {token.text} ===\")\n",
        "\n",
        "    # Core text info\n",
        "    print(\"Text:        \", token.text)\n",
        "    print(\"Lemma:       \", token.lemma_)\n",
        "    print(\"POS (UPOS):  \", token.pos_)\n",
        "    print(\"Tag (XPOS):  \", token.tag_)\n",
        "    print(\"Morph:       \", token.morph)\n",
        "    print(\"Dep:         \", token.dep_)\n",
        "    print(\"Head:        \", token.head.text)\n",
        "    print(\"HeadI:       \", token.head.i)\n",
        "\n",
        "    # Entity info\n",
        "    print(\"Ent type:    \", token.ent_type_)\n",
        "    print(\"Ent IOB:     \", token.ent_iob_)\n",
        "\n",
        "    # Document & sentence info\n",
        "    print(\"Is sent start:\", token.is_sent_start)\n",
        "    print(\"Sentence:    \", token.sent.text)\n",
        "\n",
        "    # Orthographic info\n",
        "    print(\"Lower:       \", token.lower_)\n",
        "    print(\"Shape:       \", token.shape_)\n",
        "    print(\"Whitespace:  \", repr(token.whitespace_))\n",
        "\n",
        "    # Booleans\n",
        "    print(\"is_alpha:    \", token.is_alpha)\n",
        "    print(\"is_digit:    \", token.is_digit)\n",
        "    print(\"is_punct:    \", token.is_punct)\n",
        "    print(\"is_space:    \", token.is_space)\n",
        "    print(\"is_stop:     \", token.is_stop)\n",
        "\n",
        "    # Position info\n",
        "    print(\"Index:       \", token.i)\n",
        "    print(\"Char offset: \", token.idx)\n"
      ],
      "metadata": {
        "id": "ZmT1a-cK8W5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### optional\n",
        "doc = nlp_hy(\"ՄԱՐԴՈՒ ԻՐԱՎՈՒՆՔՆԵՐԻ ՀԱՄԸՆԴՀԱՆՈՒՐ ՀՌՉԱԿԱԳԻՐ. ՆԵՐԱԾԱԿԱՆ. Քանզի մարդկային ընտանիքի բոլոր անդամներին ներհատուկ արժանապատվությունըև հավասար ու անօտարելի իրավունքները աշխարհի ազատության, արդարության ու խաղաղության հիմքն են.\")"
      ],
      "metadata": {
        "id": "m022wp2JHvOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### optional\n",
        "for token in doc:\n",
        "    # print(token.i, token.text, token.lemma_, token.pos_, token.morph, token.dep_, token.head.i, token.head.text)\n",
        "    print(token.text, token.pos_, token.lemma_)\n"
      ],
      "metadata": {
        "id": "LPhSOX15ICmG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bfa47b8-92be-4e0c-b67a-c9b7a0e36ae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ՄԱՐԴՈՒ NOUN մարդ\n",
            "ԻՐԱՎՈՒՆՔՆԵՐԻ ADJ իրավունքների\n",
            "ՀԱՄԸՆԴՀԱՆՈՒՐ ADJ համընդհանուր\n",
            "ՀՌՉԱԿԱԳԻՐ NOUN հայրանահ\n",
            ". PUNCT .\n",
            "ՆԵՐԱԾԱԿԱՆ NOUN ներախական\n",
            ". PUNCT .\n",
            "Քանզի SCONJ քանզի\n",
            "մարդկային ADJ մարդկային\n",
            "ընտանիքի NOUN ընտանիք\n",
            "բոլոր DET բոլոր\n",
            "անդամներին NOUN անդամ\n",
            "ներհատուկ ADJ ներհատուկ\n",
            "արժանապատվությունը NOUN արժանապատվություն\n",
            "և CCONJ և\n",
            "հավասար ADJ հավասար\n",
            "ու CCONJ ու\n",
            "անօտարելի ADJ անօտարելի\n",
            "իրավունքները NOUN իրավունք\n",
            "աշխարհի NOUN աշխարհ\n",
            "ազատության NOUN ազատություն\n",
            ", PUNCT ,\n",
            "արդարության NOUN արդարություն\n",
            "ու CCONJ ու\n",
            "խաղաղության NOUN խաղաղություն\n",
            "հիմքն NOUN հիմք\n",
            "են AUX եմ\n",
            ". PUNCT .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### full analysis of the file (optional)\n",
        "- includes dependency parsing\n",
        "\n",
        "Note: a warning about NER possible: ``` /usr/local/lib/python3.12/dist-packages/spacy/language.py:1041: UserWarning: Can't set named entities because of multi-word token expansion  ```, just ingnore it, see the following explanation for more details: https://chatgpt.com/share/68baeaad-03bc-800e-a29c-e90f7a028996"
      ],
      "metadata": {
        "id": "NVyvpuMDQXYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### optional\n",
        "with open('/content/TED2020-dehy-hy-aa.txt', 'r', encoding='utf-8') as infile, open('/content/TED2020-dehy-hy-aa-ANALYSIS-full-v01.txt', 'w') as outfile:\n",
        "    # read sample.txt an and write its content into sample2.txt\n",
        "    # outfile.write(\"{token.i}\\t{token.text}\\t{token.lemma_}\\t{token.pos_}\\t{token.morph}\\t{token.dep_}\\t{token.head.i}\\t{token.head.text}\\t{parentLem}\\t{SLAncestors}\\n\")\n",
        "    outfile.write(\"{token.text}\\t{token.pos_}\\t{token.lemma_}\\n\")\n",
        "\n",
        "    j=0\n",
        "    for line in infile:\n",
        "        line = line.strip()\n",
        "        j = j+1\n",
        "        if (j % 20 == 0): print(str(j), line)\n",
        "\n",
        "        doc = nlp_hy(line)\n",
        "        # outfile.write(line + '\\n')\n",
        "\n",
        "        for token in doc:\n",
        "            LAncestors = list(token.ancestors)\n",
        "\n",
        "            # print(str(LAncestors))\n",
        "            try:\n",
        "                SLAncestors = str(list(token.ancestors))\n",
        "                parent = LAncestors[0]\n",
        "                parentLem = parent.lemma_\n",
        "            except:\n",
        "                parentLem = \"NONE\"\n",
        "            # outfile.write(f\"{token.i}\\t{token.text}\\t{token.lemma_}\\t{token.pos_}\\t{token.morph}\\t{token.dep_}\\t{token.head.i}\\t{token.head.text}\\t{parentLem}\\t{SLAncestors}\\n\")\n",
        "            outfile.write(f\"{token.text}\\t{token.pos_}\\t{token.lemma_}\\n\")\n"
      ],
      "metadata": {
        "id": "HJdW66EmJI2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### function for lemmatization"
      ],
      "metadata": {
        "id": "_w7MrFvNQqxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parseFile(iFileName, oFileName, nlp_model = nlp_hy):\n",
        "    with open(iFileName, 'r', encoding='utf-8') as infile, open(oFileName, 'w') as outfile:\n",
        "        # read sample.txt an and write its content into sample2.txt\n",
        "        # write the name of the fields (does not change actual fields printed, see last comment below)\n",
        "        # outfile.write(\"{token.text}\\t{token.pos_}\\t{token.morph}\\t{token.lemma_}\\n\")\n",
        "        outfile.write(\"{token.text}\\t{token.pos_}\\t{token.lemma_}\\n\")\n",
        "        c = 0\n",
        "        for line in infile:\n",
        "            c+=1\n",
        "            if c%20 == 0: print('line no. ' + str(c))\n",
        "            line = line.strip()\n",
        "            doc = nlp_model(line)\n",
        "            # outfile.write(line + '\\n')\n",
        "            for token in doc:\n",
        "                LAncestors = list(token.ancestors)\n",
        "                # print(str(LAncestors))\n",
        "                try:\n",
        "                    SLAncestors = str(list(token.ancestors))\n",
        "                    parent = LAncestors[0]\n",
        "                    parentLem = parent.lemma_\n",
        "                except:\n",
        "                    parentLem = \"NONE\"\n",
        "                # here you can adjust which features you need in your lemmatization file !!!!!\n",
        "                # outfile.write(f\"{token.text}\\t{token.pos_}\\t{token.morph}\\t{token.lemma_}\\n\")\n",
        "                outfile.write(f\"{token.text}\\t{token.pos_}\\t{token.lemma_}\\n\")\n",
        "\n",
        "    return\n"
      ],
      "metadata": {
        "id": "QT0tpHwjY4O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### command to lemmatize the file"
      ],
      "metadata": {
        "id": "FJoxNaZ7vfVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parseFile('evaluation-set-v01.txt', 'evaluation-set-v01.vert', nlp_hy)"
      ],
      "metadata": {
        "id": "lrSvU_NbsZ9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parseFile('/content/TED2020-dehy-hy-aa.txt', '/content/TED2020-dehy-hy-aa--lemmatization-v01.txt', nlp_hy)"
      ],
      "metadata": {
        "id": "rgdb1fl3a6F4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
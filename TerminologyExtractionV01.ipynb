{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8JBbTWLW11/RATjd+p8gE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/corpustools/blob/main/TerminologyExtractionV01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install morphological analysis tools\n"
      ],
      "metadata": {
        "id": "Ojp0QhEHhCyQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgCBgAOdg2o1"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# installing TreeTagger (en, de, ka)\n",
        "mkdir treetagger\n",
        "cd treetagger\n",
        "# Download the tagger package for your system (PC-Linux, Mac OS-X, ARM64, ARMHF, ARM-Android, PPC64le-Linux).\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/tree-tagger-linux-3.2.4.tar.gz\n",
        "tar -xzvf tree-tagger-linux-3.2.4.tar.gz\n",
        "# Download the tagging scripts into the same directory.\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/tagger-scripts.tar.gz\n",
        "gunzip tagger-scripts.tar.gz\n",
        "# Download the installation script install-tagger.sh.\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/install-tagger.sh\n",
        "# Download the parameter files for the languages you want to process.\n",
        "# list of all files (parameter files) https://cis.lmu.de/~schmid/tools/TreeTagger/#parfiles\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/english.par.gz\n",
        "sh install-tagger.sh\n",
        "cd ..\n",
        "sudo pip install treetaggerwrapper\n",
        "# changing options: no-unknown, sgml, lemma\n",
        "mv /content/treetagger/cmd/tree-tagger-english /content/tree-tagger-english0\n",
        "awk '{ if (NR == 9) print \"OPTIONS=\\\"-token -lemma -sgml -no-unknown\\\"\"; else print $0}' /content/tree-tagger-english0 > /content/treetagger/cmd/tree-tagger-english\n",
        "chmod a+x ./treetagger/cmd/tree-tagger-english\n",
        "# downloading German and Georgian \n",
        "wget https://heibox.uni-heidelberg.de/f/ec8226edebb64a359407/?dl=1\n",
        "mv index.html?dl=1 /content/treetagger/lib/german-utf8.par\n",
        "wget https://heibox.uni-heidelberg.de/f/9183090d2bdb41e09055/?dl=1\n",
        "mv index.html?dl=1 /content/treetagger/lib/georgian.par\n",
        "wget https://heibox.uni-heidelberg.de/f/9cafab0509d64ed1ac4b/?dl=1\n",
        "mv index.html?dl=1 /content/treetagger/cmd/tree-tagger-georgian2\n",
        "# German2 = -no-unknown \n",
        "# note: tree-tagger-german will not work, as parameter files have not been downloaded, only use tree-tagger-german2 with utf8 encoding\n",
        "wget https://heibox.uni-heidelberg.de/f/acb9b8a2fa4f40e08f8a/?dl=1\n",
        "mv index.html?dl=1 /content/treetagger/cmd/tree-tagger-german2\n",
        "chmod a+x /content/treetagger/cmd/tree-tagger-georgian2\n",
        "chmod a+x /content/treetagger/cmd/tree-tagger-german2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy-stanza\n",
        "\n",
        "import stanza\n",
        "import spacy_stanza\n",
        "\n",
        "# optional\n",
        "# Download the stanza model if necessary\n",
        "stanza.download(\"en\")\n",
        "\n",
        "# Initialize the pipeline\n",
        "nlp = spacy_stanza.load_pipeline(\"en\")\n",
        "\n",
        "doc = nlp(\"Barack Obama was born in Hawaii. He was elected president in 2008.\")\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.dep_, token.ent_type_)\n",
        "print(doc.ents)\n",
        "\n",
        "\n",
        "stanza.download(\"hy\")\n",
        "\n",
        "nlp_hy = spacy_stanza.load_pipeline(\"hy\")\n",
        "\n",
        "doc = nlp_hy(\"ՄԱՐԴՈՒ ԻՐԱՎՈՒՆՔՆԵՐԻ ՀԱՄԸՆԴՀԱՆՈՒՐ ՀՌՉԱԿԱԳԻՐ. ՆԵՐԱԾԱԿԱՆ. Քանզի մարդկային ընտանիքի բոլոր անդամներին ներհատուկ արժանապատվությունըև հավասար ու անօտարելի իրավունքները աշխարհի ազատության, արդարության ու խաղաղության հիմքն են.\")\n",
        "\n",
        "\n",
        "### optional\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.dep_, token.ent_type_)\n",
        "\n",
        "\n",
        "def parseFile(iFileName, oFileName, nlp_model = nlp_hy):\n",
        "    with open(iFileName, 'r', encoding='utf-8') as infile, open(oFileName, 'w') as outfile:\n",
        "        # read sample.txt an and write its content into sample2.txt\n",
        "        outfile.write(\"{token.text}\\t{token.pos_}\\t{token.lemma_}\\n\")\n",
        "        c = 0\n",
        "        for line in infile:\n",
        "            c+=1\n",
        "            if c%10 == 0: print(str(c))\n",
        "            line = line.strip()\n",
        "            doc = nlp_model(line)\n",
        "            # outfile.write(line + '\\n')\n",
        "            for token in doc:\n",
        "                LAncestors = list(token.ancestors)\n",
        "                # print(str(LAncestors))\n",
        "                try:\n",
        "                    SLAncestors = str(list(token.ancestors))\n",
        "                    parent = LAncestors[0]\n",
        "                    parentLem = parent.lemma_\n",
        "                except:\n",
        "                    parentLem = \"NONE\"\n",
        "                outfile.write(f\"{token.text}\\t{token.pos_}\\t{token.lemma_}\\n\")\n",
        " \n",
        "    return\n",
        "\n",
        "!wget https://heibox.uni-heidelberg.de/f/a847a12bffd4491f9070/?dl=1\n",
        "!mv index.html?dl=1 TED2020-dehy-hy-aa\n"
      ],
      "metadata": {
        "id": "OO-xZs9M9dIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parseFile('/content/TED2020-dehy-hy-aa', '/content/TED2020-dehy-hy-aa--lemmatization-v01.txt', nlp_hy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQ_pdn2991sX",
        "outputId": "d451a702-e712-4753-a2df-1e8cbefd56b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "110\n",
            "120\n",
            "130\n",
            "140\n",
            "150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Due to multiword token expansion or an alignment issue, the original text has been replaced by space-separated expanded tokens.\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Դուք', 'երբևէ', 'ծանրաբեռնված', 'զգացել', '՞', 'եք', 'մի', 'որևէ', 'բարդ', 'խնդրի', 'հանդիպելիս', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160\n",
            "170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ակնհայտ', 'է', ',', 'որ', 'բարդ', 'առաջադրանք', 'է', ',', 'բայց', 'արդյոք', '՞', 'այն', 'խճճված', 'է', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "180\n",
            "190\n",
            "200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Անշուշտ', ',', 'շատ', 'մարդիկ', 'կարծում', 'են', ',', 'թե', 'արդեն', 'գիտեն', '&lt;&lt', '<UNK>Ինչ', '՞', 'է', 'գեղեցկությունը', '&gt;&gt;', 'հարցի', 'ճշգրիտ', 'պատասխանը', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ինչպես', '՞', 'կարող', 'ենք', 'բացատրել', 'այդ', 'համընդհանրությունը', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "220\n",
            "230\n",
            "240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ինչ', '՞', 'կասեք', 'գեղարվեստական', 'գեղեցկության', 'մասին', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "250\n",
            "260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Այսպիսով', ',', 'ինչ', '՞', 'էին', 'իրենցից', 'ներկայացնում', 'այս', 'հնագույն', 'գործիքները', ',', 'ես', 'նկատի', 'ունեմ', 'դրանք', 'հնագույն', 'են', ',', 'որովհետև', 'անծանոթ', 'են', ',', 'բայց', 'միևնույն', 'ժամանակ', 'դրանք', 'ինչ', '-', 'որ', 'կերպ', 'ծանոթ', 'են', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Գիտեք', ',', 'սա', 'մի', 'քիչ', 'հին', 'կատակ', 'է', ',', 'բայց', 'պարզվում', 'է', ',', 'որ', 'այն', 'գործում', 'է', '.', '.', '&lt;&lt', ';', 'ինչու', '՞', 'չես', 'մտնում', 'իմ', 'քարանձավը', ',', 'որ', 'քեզ', 'ցույց', 'տամ', 'իմ', 'կացինները', '&gt;&gt;', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Այդ', 'առարկան', 'պատրաստվել', 'էր', 'կամ', '՛', 'ուղիղ', 'քայլվածքով', ',', 'կամ', '՛', 'աշխատող', 'մարդ', 'նախահոր', 'կողմից', 'լեզվի', 'ստեղծումից', '50', '.', '000', '-', '100', '.', '000', 'տարի', 'առաջ', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Գեղեցկությունը', 'ականատեսի', 'աչքքրում', '՞', 'է', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ոչ', '՛', ',', 'այն', 'մեր', 'ուղեղի', 'խորքում', 'է', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "290\n",
            "300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Գիտեիք', '՞', ',', 'որ', 'գենետիկորեն', 'պատրաստված', 'հացահատիկ', 'կերած', 'առնետների', 'մոտ', 'լյարդի', 'և', 'երիկամների', 'թունավորման', 'ախտանշաններ', 'են', 'ի', 'հայտ', 'եկել', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "310\n",
            "320\n",
            "330\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ոմանք', 'ասում', 'են', ',', 'որ', 'օրգանական', 'կամ', 'տեղական', 'սնունդը', 'շատ', 'ավելի', 'թանկ', 'է', ',', 'բայց', 'դա', 'իրոք', '՞', 'այդպես', 'է', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Եվ', 'գիտեք', '՞', ',', 'թե', 'նա', 'ինչ', 'է', 'պատասխանել', 'հորը', ':', 'Ասել', 'է', ',', 'որ', 'նախընտրում', 'է', 'օրգանական', 'կարմրացրած', 'հացի', 'կտորներ', ',', 'քանի', 'որ', 'Բարկն', 'ասել', 'էր', ',', 'որ', 'նա', 'չպետք', 'է', 'ուտի', 'եգիպտացորենի', 'փաթիլներ', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "350\n",
            "360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ես', 'տղային', 'մոտ', '30', 'վայրկյան', 'ժամանակ', 'էի', 'տալիս', ',', 'ինչը', 'նշանակում', 'է', ',', 'որ', 'մինչ', 'նա', 'կմոտենար', 'ինձ', ',', 'ես', 'արդեն', 'ասում', 'էի', 'նրան', '․', '«', 'ինչու', '՞', 'ես', 'լացում', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['ինչու', '՞', 'ես', 'լացում', '»', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Գնա', 'նստիր', ',', 'հավաքիր', 'քեզ', ',', 'և', 'վերադարձիր', 'և', 'խոսա', 'ինձ', 'հետ', ',', 'երբ', 'կարող', 'ես', 'խոսել', 'ինչպես', '․․․', '»', 'Ինչ', '՞', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "370\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Երբ', 'անցան', 'տարիներ', ',', 'ես', 'սկսեցի', 'ասել', 'ինքս', 'ինձ', '․', '«', 'Աստված', 'իմ', ',', 'ինչ', '՞', 'է', 'կատարվում', 'ինձ', 'հետ', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ինչ', '՞', 'եմ', 'ես', 'անում', '։', 'ինչու', '՞', 'եմ', 'ես', 'այսպես', 'վարվում', '»', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "380\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ես', 'հիշում', 'եմ', ',', 'որ', 'խոսում', 'էի', '12', 'տարեկան', 'ֆուտբոլ', 'խաղացող', 'տղայի', 'հետ', ',', 'և', 'ես', 'հարցեցի', 'նրան', '․', '«', 'Ինչ', '՞', 'դու', 'կզգայիր', ',', 'եթե', 'մյուս', 'խաղացողների', 'ներկայությամբ', ',', 'քո', 'մարզիչն', 'ասեր', ',', 'որ', 'դու', 'խաղում', 'ես', 'աղջկա', 'պես', '»', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Եվ', 'ես', 'ասացի', 'ինքս', 'ինձ', '․', '«', 'Աստված', 'իմ', ',', 'եթե', 'աղջիկ', 'կոչվելը', 'կոչնչացներ', 'նրան', ',', 'ապա', 'ինչ', '՞', 'ենք', 'մենք', 'սովորեցնում', 'նրան', 'աղջիկների', 'մասին', '»', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Նա', 'այն', 'տղաներից', 'էր', ',', 'ում', 'մասին', 'ծնողները', 'մտածում', 'էին', '․', '«', 'Ինչ', '՞', 'է', 'այս', '16', 'տարեկանն', 'անում', 'բոլոր', 'այս', '12', 'տարեկան', 'տղաների', 'հետ', '»', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400\n",
            "410\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Նա', 'նայում', 'է', 'պատուհանից', 'և', 'ինձ', 'վերև', 'կանչում', '․', '«', 'Էյ', '՜', ',', 'Էնթոնի', '»', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['«', 'Էյ', '՜', ',', 'Էնթոնի', ',', 'վերև', 'բարձրացիր', '»', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Նա', 'բացում', 'է', 'դուռն', 'ու', 'ասում', '․', '«', 'Ուզզու', '՞', 'ես', '»', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Որովհետև', 'ինձ', 'համար', ',', 'մի', 'տղայի', 'համար', ',', 'ով', 'մեծանում', 'էր', 'այդ', 'ժամանակներում', ',', 'հաշվի', 'առնելով', 'տղամարդ', 'լինելու', 'պատկերացումները', ',', '«', 'ուզզու', '՞', 'ես', '»', 'կարող', 'էր', 'նշանակել', 'միայն', 'երկու', 'բան', '՝', 'սեքս', 'կամ', 'թմրադեղ', ',', 'իսկ', 'թմրադեղեր', 'մենք', 'չէինք', 'օգտագործում', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "420\n",
            "430\n",
            "440\n",
            "450\n",
            "460\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Այն', 'աշխարհում', ',', 'որ', 'ես', 'պատկերում', 'եմ', 'նրա', 'համար', ',', 'ինչպես', '՞', 'պիտի', 'տղամարդիկ', 'իրենց', 'պահեն', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ես', 'ուզում', 'եմ', ',', 'որ', 'դուք', 'միանաք', 'ինձ', 'և', 'ես', '՝', 'ձեզ', ',', 'և', 'մենք', 'միասին', 'դաստիարակենք', 'մեր', 'տղաներին', ',', 'և', 'սովորեցնենք', 'նրանք', 'տղամարդ', 'լինել', ',', 'սովորեցնենք', ',', 'որ', 'նորմալ', 'է', 'իշխող', 'չլինել', ',', 'նորմալ', 'է', 'ունենալ', 'զգացմունքներ', 'և', 'հույզեր', ',', 'նորմալ', 'է', 'հավասարություն', 'քարոզել', '․', 'նորմալ', 'է', 'կին', 'ընկերներ', 'ունենալ', ',', 'նորմալ', 'է', 'պատկառելի', 'լինել', ',', 'որ', 'իմ', 'ազատությունը', 'որպես', 'տղամարդ', ',', 'կապված', 'է', 'քո', 'ազատությանը', 'որպես', 'կին', '։', 'Ես', 'հիշում', 'եմ', ',', 'որ', 'հարցրեցի', 'իննը', 'տարեկան', 'մի', 'տղայի', '։', 'Ես', 'հարցրեցի', 'իննը', 'տարեկան', 'մի', 'տղայի', '․', '«', 'Ինչպիսին', '՞', 'կլիներ', 'քո', 'կյանքը', ',', 'եթե', 'դու', 'ստիպված', 'չլինեիր', 'հավատարիմ', 'մնալ', 'տղամարդ', 'լինելու', 'պատկերացումներին', '»', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "470\n",
            "480\n",
            "490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ավելի', 'տարեց', 'ուսուցիչները', '`', 'ավելի', 'փորձառուները', ',', 'ինձ', 'էին', 'նայում', 'ու', 'ասում', '.', '«', 'Օ', '՜', ',', 'ահա', 'և', 'նա', '՜', ':', '<UNK>ատ', '՜', 'հուզիչ', 'է', '.ջանք', 'ու', 'եռանդ', 'չի', '՛', 'խնայում', 'գործն', 'ավարտին', 'հասցնելու', 'համար', '»', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n",
            "510\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Հանձնարարության', 'վեջին', 'հարցը', 'հետևյալն', 'է', '.', '«', 'Ինչպես', '՞', 'եք', 'դուք', 'նախատեսում', 'ապրել', 'ձեր', 'կյանքը', '`', 'այլ', 'մարդկանց', 'վրա', 'դրական', 'ազդեցություն', 'ունենալու', 'համար', '»', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Այսպիսով', ',', 'ինչ', '՞', 'եք', 'դուք', 'անում', ',', 'երբ', 'ձեր', 'շուրջ', 'բոլորը', 'տեղեկատվություն', 'է', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['ինչու', '՞', 'եք', 'երեխաներին', 'ուղղարկում', 'դպրոց', ',', 'եթե', 'նրանք', 'այլևս', 'այդտեղից', 'տեղեկատվություն', 'ստանալու', 'կարիք', 'չունեն', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "530\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['գնացեք', '՛', ',', 'ստեղծեք', '՛', ':', 'գնացեք', '՛', ',', 'գլուխ', '՛', 'բերք', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "540\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['«', 'Ումն', '՞', 'է', 'ամենալավը', '»', ':', 'Եվ', 'նրանք', 'անմիջապես', 'պատասխանեցին', '.«', 'Ահա', ',', 'այյն', '՜', 'մեկը', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Առանց', 'որևէ', 'բան', 'կարդալու', ':', 'Ահա', ',', 'այյն', '՜', 'մեկը', '»', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Եվ', 'ես', 'ասացի', '.«', 'Լավ', ',', 'իսկ', 'ինչով', '՞', 'է', 'այն', 'լավը', '»', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Եվ', 'ես', 'ասացի', '.', '«', 'Այժմ', 'կարդաացե', '՛', 'դա', '»', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Իսկ', 'նրանք', '.', '«', 'Օ', '՜', ',', 'այ', 'դա', 'այդքան', '՜', 'էլ', 'լավ', 'միտք', 'չէ', '»', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "550\n",
            "560\n",
            "570\n",
            "580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Եվ', 'ես', 'ասացի', '․', '«', 'Դուք', 'հենց', 'նոր', '՞', 'եք', 'տեղափոխվել', 'այս', 'գրասենյակ', '»', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ես', 'ասացի', '․', '«', 'Ուզում', 'եք', 'ասել', ',', 'որ', 'ես', 'միակ', 'կինն', 'եմ', ',', 'որ', 'գործարք', 'եմ', 'փորձում', 'կնքել', 'ձեր', 'գրասենյակում', 'մեկ', 'տարվա', 'ընթացքու', '՞', '»', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['(', 'Ծիծաղ', ')', 'Հարցը', 'հետևյալն', 'է', '․', 'ինչպես', '՞', 'ենք', 'մենք', 'պատրաստվում', 'ուղղել', 'դա', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ինչպես', '՞', 'կարող', 'ենք', 'փոխել', 'ղեկավար', 'պաշտոնների', 'թվերը', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ինչպես', '՞', 'փոխել', 'իրավիճակը', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "590\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Այսօր', 'ես', 'ուզում', 'եմ', 'կենտրոնանալ', 'նրա', 'վրա', ',', 'թե', 'ինչ', '՞', 'ենք', 'մենք', 'կարող', 'անել', ',', 'որպես', 'անհատներ', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ինչ', '՞', 'խորհուրդ', 'մենք', 'պետք', 'է', 'տանք', 'ինքներս', 'մեզ', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ինչ', '՞', 'խորհուրդ', 'մենք', 'պետք', 'է', 'տանք', 'կանանց', ',', 'որոնք', 'աշխատում', 'են', 'մեզ', 'հետ', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ինչ', '՞', 'խորհուրդ', 'մենք', 'պետք', 'է', 'տանք', 'մեր', 'աղջիկներին', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600\n",
            "610\n",
            "620\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Երբ', 'դուրս', 'ենք', 'գալիս', 'քննությունից', ',', 'նայում', 'իրար', ',', 'և', 'հարցնում', '․', '«', 'Ինչպես', '՞', 'էր', '»', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['«', 'Դու', 'ամենաաաձր', '՞', 'ես', 'ստացել', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Դա', 'ակնհայտ', 'է', '։', 'Ինչ', '՞', 'կարիք', 'կա', 'հարցնելու', '»', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ինչ', '՞', 'է', 'սա', 'նշանակում', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "640\n",
            "650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Իսկ', 'Հեյդին', '՞', '։', 'Ուսանողներն', 'այդքան', 'էլ', 'համոզված', 'չեն', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "660\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ես', 'սովորեցի', ',', 'որ', 'պետք', 'է', 'ձեռքս', 'բարձրացրած', 'պահել', '»', '։', 'Ես', 'ասացի', '․', '«', 'Ինչ', '՞', 'նկատի', 'ունեք', '»', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "670\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['եթե', 'նույնիսկ', 'ես', ',', 'ում', 'համար', 'սա', 'կարևոր', 'է', ',', 'ակնհայտ', 'է', ',', 'չէ', 'որ', 'ես', 'ելույթ', 'եմ', 'ունենում', ',', 'այդ', 'ելույթի', 'ընթացքում', ',', 'ես', 'չեմ', 'կարող', 'նույնիսկ', 'նկատել', ',', 'որ', 'տղամարդկանց', 'ձեռքերը', 'դեռ', 'բարձրացրած', 'են', ',', 'և', 'կանանց', 'ձեռքերը', 'դեռ', 'բարձրացրած', 'են', ',', 'ինչպես', '՞', 'մենք', ',', 'որպես', 'ղեկավարներ', 'մեր', 'կազմակերպություններում', 'կարող', 'ենք', 'նկատել', ',', 'որ', 'տղամարդիկ', 'ավելի', 'են', 'ձգտում', 'օգտագործել', 'հնարավորությունները', 'քան', 'կանայք', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ինչ', '՞', 'եք', 'կարծում', ',', 'ով', 'է', 'լքում', 'աշխատավայրը', ',', 'երբ', 'անհրաժեշտ', 'է', 'տանը', 'ավելի', 'շատ', 'լինել', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "680\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Եվ', 'եթե', 'դա', 'բավարար', 'դրդապատճառ', 'չի', 'հանդիսանում', 'բոլորի', 'համար', ',', 'նրանք', 'նաև', 'ավելի', 'շատ․․', '․', 'ինչպես', '՞', 'կարող', 'եմ', 'ասել․․․', '․']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "690\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['«', 'Ինչպես', '՞', 'ես', 'կարող', 'եմ', 'շարունակել', 'անել', 'այն', 'ամենն', 'ինչ', 'անում', 'եմ', 'այժմ', '»', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ես', 'ասացի', '․', '«', 'Դուք', 'և', 'ձեր', 'ամուսինը', 'մտածում', 'եք', 'երեխա', 'ունենալու', 'մասինն', '՞', '»', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ինչ', '՞', 'է', 'տեղի', 'ունենում', ',', 'երբ', 'դուք', 'սկսում', 'եք', 'կամաց', '-', 'կամաց', 'դիրքերը', 'զիջել', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "710\n",
            "720\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ես', 'էլ', 'մտածեցի', '.', '«', 'Լավ', ',', 'որն', '՞', 'է', 'խնդիրը', '»', ':', 'Իսկ', 'նա', 'պատասխանեց', '.']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "730\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Եվ', 'այդ', 'պահին', 'իմ', 'մեջ', 'անվստահ', 'խոսեց', 'գիտնականի', 'հպարտությունը', '.', '«', 'Ինչպես', '՞', 'եք', 'ներկայացնելու', '»', ':', 'Եվ', 'նա', 'ասաց', '.']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Եվ', 'ես', 'մտածեցի', '.', '«', 'ինչու', '՞', 'ոչ', 'հեքիաթասաց', '»', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "740\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ուստի', 'ասացի', '.', '«', 'Գիտեք', 'ինչ', '՞', ',']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['ինչու', '՞', 'ուղղակի', 'ինձ', 'հետազոտող', '-', 'պատմիչ', 'չեք', 'անվանում', '»', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['«', 'Իսկապես', '՞', '»', ',', '-', 'հարցրի', 'ես', ':', '«', 'Միանշանակ', '»', ',', '-', 'եղավ', 'պատասխանը', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ուստի', 'ասեմ', ',', 'որ', 'ես', 'ունեմ', 'սոցիոլոգիայի', 'ոլորտում', 'բակալավրի', 'և', 'մագիստրոսի', 'աստիճան', ',', 'իսկ', 'հիմա', 'գիտությունների', 'թեկնածու', 'եմ', 'այնպես', 'որ', ',', 'իմ', 'ողջ', 'ակադեմիական', 'կարիերայի', 'ընթացքում', 'ես', 'շրջապատված', 'եմ', 'եղել', 'այնպիսի', 'մարդկանցով', ',', 'որոնք', 'հավատացած', 'էին', '«', 'կյանքը', 'բազմազան', 'է', ',', 'սիրիր', '՛', 'այն', '»', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ես', 'հետյալ', 'կարծիքին', 'եմ', '.', '«', 'Կյանքը', 'խառնաշփոթ', 'է', ',', 'կանոակակարի', '՛', 'այն', ',', 'դասակարգի', '՛', 'և', 'ամեն', 'ինչ', \"դի'ր\", 'իրենց', 'դարակներում', '»', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "760\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['(', 'Ծիծաղ', ')', 'Եվ', 'դուք', 'սկսում', 'եք', 'մտածել', 'առաջխաղացման', 'մասին', ',', 'ճիշտ', '՞', 'է', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Իրականում', 'հենց', 'այսպես', 'էլ', 'լինում', 'է', ':', 'չէ', '՞', 'որ', 'երբ', 'հարցնում', 'ես', 'մարդկանց', 'սիրո', 'մասին', ',', 'նրանք', 'պատմում', 'են', 'իրենց', 'կոտրված', 'սրտերի', 'մասին', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "770\n",
            "780\n",
            "790\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ինչ', '՞', 'ընդհանրություն', 'ունեն', 'այս', 'մարդիկ', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Որն', '՞', 'է', 'թեման', ',', 'որն', '՞', 'է', 'նմուշը', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "810\n",
            "820\n",
            "830\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['դու', 'սկսում', 'ես', 'իրականում', 'քեզ', 'ճանաչել', ',', 'երբ', 'զանգում', 'ես', 'ընկերներիդ', 'և', 'ասում', '.', '«', 'Ինչ', '-', 'որ', 'մեկի', 'կարիքն', 'ունեմ', ':', 'Ինչ', '՞', 'կառաջարկեք', '»', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Նա', 'ասաց', '.', '«', 'Ինչպես', '՞', 'եք', '»', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Դիանան', 'հարցրեց', '.', '«', 'Ինչ', '՞', 'է', 'պատահել', '»', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Նա', 'ասաց', '.', '«', 'Որն', '՞', 'է', '»', ':', 'Ես', 'էլ', 'ասացի', '.']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['(', 'Ծիծաղ', ')', 'Եվ', 'հետո', 'ես', 'հարցրի', '.', '«', 'Դա', 'վատ', 'է', ',', 'այնպես', '՞', 'չէ', '»']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "860\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['խոցելիության', 'հետ', ':', 'ինչու', '՞', 'ենք', 'մենք', 'այդքան', 'պայքարում', 'դրա', 'հետ', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Միայն', 'ես', '՞', 'եմ', 'պայքարում', 'խոցելիության', 'դեմ', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ոչ', '<UNK>', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Շատ', 'ծիծաղելի', 'էր', ':', 'Ես', 'ֆեյսբուքով', 'և', 'թվիթերով', 'մի', 'հարց', 'էի', 'տեղադրել', '.', '«', 'Ինչպես', '՞', 'կսահմանեիք', 'խոցելիությունը', ':', 'Ինչն', '՞', 'է', 'ձեր', 'խոցելիության', 'պատճառը', '»', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "870\n",
            "880\n",
            "890\n",
            "900\n",
            "910\n",
            "920\n",
            "930\n",
            "940\n",
            "950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Քիչ', 'էր', 'մնում', 'ասեի', '(', 'չնայած', 'չասեցի', ')', '..', 'ուզում', 'էի', 'ասել', '.', '«', 'գիտես', '՞', 'ինչ', ':', 'Եթե', 'գոնե', 'մի', 'հինգ', 'ժամ', 'քնեիր', ',', 'այս', 'ճաշը', 'գուցե', 'շատ', 'ավելի', 'հետաքրքիր', 'լիներ', '»', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Հատկապես', 'այստեղ', ',', 'Վաշինգտոնում', ',', 'եթե', 'որևէ', 'մեկի', 'հրավիրել', 'նախաճաշի', ',', 'և', 'ասել', '.', '«', 'Հանդիպենք', 'ժամը', '<UNK><UNK>ին', '՞', '»', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "960\n",
            "970\n",
            "980\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Մոտավորապես', '1852', 'թ', '.', 'նրանք', 'մտածում', 'էին', '.', '&lt;&lt', '<UNK>Գուցեե', '՞', 'ես', 'աշխարհի', 'ամենահիմար', 'մարդն', 'եմ', ',', 'որ', 'չեմ', 'շտապում', 'Կալիֆորնիա', ':', '&gt;&gt', ';Եվ', 'սկսում', 'են', 'մտածել', ',', 'որ', 'իրոք', 'այդպիսին', 'են', ':']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "990\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "wget https://heibox.uni-heidelberg.de/f/95a3875771c040db959a/?dl=1\n",
        "mv index.html?dl=1 humanrights02.txt\n",
        "\n",
        "wget https://heibox.uni-heidelberg.de/f/a79b829e15c24dbd9e77/?dl=1\n",
        "mv index.html?dl=1 covid3m-en.txt\n",
        "\n",
        "wget https://heibox.uni-heidelberg.de/f/e3c1edbcec9649f5b4c4/?dl=1\n",
        "mv index.html?dl=1 TED2020-enka-en.txt\n",
        "\n",
        "!wget https://heibox.uni-heidelberg.de/f/cdf240db84ca4718b718/?dl=1\n",
        "!mv index.html?dl=1 go1984.txt\n"
      ],
      "metadata": {
        "id": "xDIWClMViG6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!./treetagger/cmd/tree-tagger-english humanrights02.txt >humanrights02_vert.txt\n",
        "\n",
        "!./treetagger/cmd/tree-tagger-english covid3m-en.txt >covid3m-en_vert.txt\n",
        "\n",
        "!./treetagger/cmd/tree-tagger-english TED2020-enka-en.txt >TED2020-enka-en_vert.txt\n",
        "\n",
        "!./treetagger/cmd/tree-tagger-english go1984.txt >go1984_vert.txt"
      ],
      "metadata": {
        "id": "V6aS-zsfiUJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=20 humanrights02_vert.txt"
      ],
      "metadata": {
        "id": "6H-vbWJmho36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=20 TED2020-enka-en_vert.txt"
      ],
      "metadata": {
        "id": "xctPRHHPhqTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head --lines=20 covid3m-en_vert.txt"
      ],
      "metadata": {
        "id": "ENxIWodWhteB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Terminology extraction scripts\n"
      ],
      "metadata": {
        "id": "jEZNlzgbic-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, sys, time\n",
        "def printDict(DictionaryFrq, FileOut, FileOut1):\n",
        "    for Word, Frq in sorted( DictionaryFrq.items() , key=lambda x: x[1], reverse=True):\n",
        "        if re.search(' ', Word):\n",
        "            FileOut.write(Word + '\\t' + str(Frq) + '\\n')\n",
        "        else:\n",
        "            FileOut1.write(Word + '\\t' + str(Frq) + '\\n')\n",
        "\n",
        "    FileOut.flush()\n",
        "    FileOut1.flush()\n",
        "        \n",
        "\n",
        "\n",
        "def openFiles(SFIn, SFOut, SFOut1):\n",
        "    FInStream = open(SFIn, 'r')\n",
        "    FOutStream = open(SFOut, 'w')\n",
        "    FOutStream1 = open(SFOut1, 'w')\n",
        "    return FInStream, FOutStream, FOutStream1\n",
        "\n",
        "\n",
        "class clProcCorpus(object):\n",
        "    ''' we will read a text file and return a dictionary\n",
        "    this will be done on the line by line basis\n",
        "    The dictionary can be sorted later...\n",
        "    '''\n",
        "    # this is a class for processing a corpus\n",
        "\n",
        "    def __init__(self, FileIN):\n",
        "        self.DictFrq = {}\n",
        "        self.processCorpus(FileIN)\n",
        "\n",
        "    def processCorpus(self, FileIN):\n",
        "        LTerm = []\n",
        "        k = 0\n",
        "        for Line in FileIN:\n",
        "            k+=1\n",
        "            if k%500000 == 0: print(str(k))\n",
        "            Line = Line.strip()\n",
        "            LLine = re.split('\\t', Line)\n",
        "            try:\n",
        "                Word = LLine[0]\n",
        "                PoS = LLine[1]\n",
        "                Lemma = LLine[2]\n",
        "            except:\n",
        "                Word = \"\"\n",
        "                PoS = \"\"\n",
        "                Lemma = \"\"\n",
        "            \n",
        "      #Select the Tags for your langauge\n",
        "            #if re.match('N.*', PoS) or re.match('A.*', PoS): #Arm\n",
        "            #if re.match('N.*', PoS) or re.match('ADJ.*', PoS): #DE\n",
        "            if re.match('N.*', PoS) or re.match('J.*', PoS): #EN\n",
        "\n",
        "      #Terms as Words or Lemmas\n",
        "                LTerm.append(Word)\n",
        "                # LTerm.append(Lemma)\n",
        "            else:\n",
        "                STerm = ' '.join(LTerm)\n",
        "                LTerm = []\n",
        "\n",
        "                try:\n",
        "                    self.DictFrq[STerm] += 1\n",
        "                except:\n",
        "                    self.DictFrq[STerm] = 1        \n",
        "\n",
        "        return"
      ],
      "metadata": {
        "id": "mCe4cUdlifox"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "FIn, FOut, FOut1 = openFiles('humanrights02_vert.txt', 'humanrights02_terms.txt', 'humanrights02_terms1.txt')\n"
      ],
      "metadata": {
        "id": "q779D48Fi_s4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "FIn, FOut, FOut1 = openFiles('covid3m-en_vert.txt', 'covid3m-en_terms.txt', 'covid3m-en_terms1.txt')"
      ],
      "metadata": {
        "id": "HQ75nTIOjG5Y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "FIn, FOut, FOut1 = openFiles('TED2020-enka-en_vert.txt', 'TED2020-enka-en_terms.txt', 'TED2020-enka-en_terms1.txt')"
      ],
      "metadata": {
        "id": "54dAZD1fjKcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OCorpus = clProcCorpus(FIn)\n",
        "printDict(OCorpus.DictFrq, FOut, FOut1)\n"
      ],
      "metadata": {
        "id": "0ngy3j_sjLIB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
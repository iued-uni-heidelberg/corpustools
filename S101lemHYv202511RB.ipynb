{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/corpustools/blob/main/S101lemHYv202511RB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rule-based lemmatization for Armenian\n",
        "\n",
        "## Stage 1: Downloading example file\n",
        "Example: Universal declaration of human rights\n",
        "\n",
        "If downloading from the UN website, the link aboout pdf2txt conversion is https://chatgpt.com/share/68bbeed8-0d08-800e-8418-b0816c6393d4\n",
        "\n"
      ],
      "metadata": {
        "id": "j2SZAuuE1441"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "wget https://heibox.uni-heidelberg.de/f/7a091a3c0372428a9aa2/?dl=1\n",
        "mv index.html?dl=1 udhr-all-languages.zip\n",
        "unzip udhr-all-languages.zip"
      ],
      "metadata": {
        "id": "kvSipdQ7yz6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "# Downloading UDHR\n",
        "# !wget https://unicode.org/udhr/assemblies/udhr_txt.zip\n",
        "# Alternatively, downloading from the UN website, converting pdf to txt\n",
        "# the link aboout pdf2txt conversion: https://chatgpt.com/share/68bbeed8-0d08-800e-8418-b0816c6393d4\n",
        "# !rm --recursive udhr-all-languages\n",
        "\n",
        "# delete lines which are not translations in some files (hy)\n",
        "# delete between lines $a and $b inclusive\n",
        "a=9\n",
        "b=21\n",
        "awk -v m=$a -v n=$b 'm <= NR && NR <= n {next} {print}' < /content/udhr-all-languages/udhr_hye.txt >/content/udhr-all-languages/udhr_hye_v02.txt\n",
        "\n",
        "a=1\n",
        "b=6\n",
        "awk -v m=$a -v n=$b 'm <= NR && NR <= n {next} {print}' < /content/udhr-all-languages/udhr_hye_v02.txt >/content/udhr-all-languages/udhr_hye_v03.txt\n",
        "\n",
        "\n",
        "\n",
        "# put paragraph tags\n",
        "# awk '{print \"<p>\\n\"$0 ; print \"</p>\"}' udhr/udhr_hye2.txt >udhr/udhr_hye_v03.txt\n",
        "\n",
        "\n",
        "cp /content/udhr-all-languages/udhr_hye_v03.txt /content/udhr_hye_v03.txt\n",
        "head --lines=10 /content/udhr_hye_v03.txt\n"
      ],
      "metadata": {
        "id": "gWOntGq42JvQ",
        "outputId": "c9d4d419-4045-40a9-94c0-010704b1abef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ՄԱՐԴՈՒ ԻՐԱՎՈՒՆՔՆԵՐԻ ՀԱՄԸՆԴՀԱՆՈՒՐ ՀՌՉԱԿԱԳԻՐ\n",
            "    ՄԻԱՎՈՐՎԱԾ ԱԶԳԵՐԻ ԿԱԶՄԱԿԵՐՊՈՒԹՅՈՒՆ\n",
            "    ՆԵՐԱԾԱԿԱՆ\n",
            "    Քանզի մարդկային ընտանիքի բոլոր անդամներին ներհատուկ արժանապատվությունը և հավասար ու անօտարելի իրավունքները աշխարհի ազատության, արդարության ու խաղաղության հիմքն են․\n",
            "    Քանզի մարդու իրավունքների նկատմամբ քամահրանքն ու արհամարհանքը հանգեցրել են մարդկության խիղճը խռոված բարբարոսական գործողությունների, և քանի որ այնպիսի աշխարհի ստեղծումը, ուր մարդիկ կվայելեն խոսքի ու համոզմունքների ազատություն և զերծ կլինեն վախից ու կարիքից հռչակվել է որպես մարդկանց բարձրագույն ձգտում․\n",
            "    Քանզի անհրաժեշտ է, որպեսզի մարդը, որպես մի վերջին միջոցի, չդիմի ապստամբության ընդդեմ բռնության ու ճնշման, օրենքի իշխանությամբ պաշտպանել մարդու իրավունքները․\n",
            "    Քանզի անհրաժեշտ է նպաստել ազգերի միջև բարեկամական հարաբերությունների զարգացմանը․\n",
            "    Քանզի Միավորված ազգերի ժողովուրդները կանոնադրության մեջ վերահավաստել են իրենց հավատը մարդու հիմնական իրավունքների, անձի արժանապատվության ու արժեքի, տղամարդու ու կնոջ հավասար իրավունքների նկատմամբ և որոշել են ավելի մեծ ազատության պայմաններում նպաստել սոցիալական առաջընթացին ու կյանքի պայմանների բարելավմանը․\n",
            "    Քանզի անդամ պետությունները պարտավորվել են Միավորված ազգերի հետ համագործակցությամբ հասնել մարդու իրավունքների ու հիմնական ազատությունների նկատմամբ համընդհանուր հարգանքի ու դրանց պահպանման․\n",
            "    Քանզի այս իրավունքների ու ազատությունների համընդհանուր ըմբռնումը կարևորագույն նշանակություն ունի այս պարտավորության լիարժեք իրագործման համար․\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing packages\n",
        "\n",
        "- gensim word2vec for similarity-based disambiguation\n",
        "- Eastern Armenian Analyser from https://github.com/timarkh/uniparser-grammar-eastern-armenian , which outputs translations into English\n",
        "\n",
        "\n",
        "Note, please restart the session if requested (gensim needs another version of the default libraries).\n",
        "\n"
      ],
      "metadata": {
        "id": "PGF35NFNxe-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installing Armenian morphological analyser\n",
        "!git clone https://github.com/timarkh/uniparser-grammar-eastern-armenian\n",
        "# Python classes\n",
        "!pip3 install uniparser-eastern-armenian\n",
        "# disambiguation\n",
        "!sudo apt-get install cg3\n",
        "\n",
        "!pip install gensim\n"
      ],
      "metadata": {
        "id": "jF1Sa8dm3WFG",
        "outputId": "41f739e6-fd5b-4be9-aa77-c7ddf7d72c53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'uniparser-grammar-eastern-armenian'...\n",
            "remote: Enumerating objects: 181, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 181 (delta 12), reused 40 (delta 12), pack-reused 141 (from 1)\u001b[K\n",
            "Receiving objects: 100% (181/181), 52.66 MiB | 10.30 MiB/s, done.\n",
            "Resolving deltas: 100% (78/78), done.\n",
            "Collecting uniparser-eastern-armenian\n",
            "  Downloading uniparser_eastern_armenian-2.1.2-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting uniparser-morph>=2.2.0 (from uniparser-eastern-armenian)\n",
            "  Downloading uniparser_morph-2.9.4-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from uniparser-eastern-armenian) (6.5.2)\n",
            "Collecting textdistance>=4.0.0 (from uniparser-morph>=2.2.0->uniparser-eastern-armenian)\n",
            "  Downloading textdistance-4.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Downloading uniparser_eastern_armenian-2.1.2-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uniparser_morph-2.9.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.8/59.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading textdistance-4.6.3-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: textdistance, uniparser-morph, uniparser-eastern-armenian\n",
            "Successfully installed textdistance-4.6.3 uniparser-eastern-armenian-2.1.2 uniparser-morph-2.9.4\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libcg3-1\n",
            "The following NEW packages will be installed:\n",
            "  cg3 libcg3-1\n",
            "0 upgraded, 2 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 423 kB of archives.\n",
            "After this operation, 1,278 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcg3-1 amd64 1.3.2-1build2 [305 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 cg3 amd64 1.3.2-1build2 [118 kB]\n",
            "Fetched 423 kB in 3s (141 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libcg3-1:amd64.\n",
            "(Reading database ... 126374 files and directories currently installed.)\n",
            "Preparing to unpack .../libcg3-1_1.3.2-1build2_amd64.deb ...\n",
            "Unpacking libcg3-1:amd64 (1.3.2-1build2) ...\n",
            "Selecting previously unselected package cg3.\n",
            "Preparing to unpack .../cg3_1.3.2-1build2_amd64.deb ...\n",
            "Unpacking cg3 (1.3.2-1build2) ...\n",
            "Setting up libcg3-1:amd64 (1.3.2-1build2) ...\n",
            "Setting up cg3 (1.3.2-1build2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "9cceeb9997fb4d1e925d9daf681b865c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing python libraries\n",
        "import os, re, sys\n",
        "\n",
        "from uniparser_eastern_armenian import EasternArmenianAnalyzer\n",
        "a = EasternArmenianAnalyzer()\n",
        "analyses = a.analyze_words('Ձևաբանություն')\n",
        "for ana in analyses:\n",
        "    print(ana.wf, ana.lemma, ana.gramm, ana.gloss, ana.stem, ana.subwords, ana.wfGlossed, ana.otherData)"
      ],
      "metadata": {
        "id": "dRFuA6sN3Z62",
        "outputId": "3812f86a-0e04-432f-f6b3-acb4f9c41db5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ձևաբանություն ձեւաբանություն N,inanim,sg,nom,nonposs morphology ձևաբանություն. [] ձևաբանություն [('trans_en', 'morphology')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional part: trying different output format with disambiguation, etc.\n",
        "\n",
        "This can be skipped. You can go directly to the \"File annotation\" section"
      ],
      "metadata": {
        "id": "b9_WoVsxg3Ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trying out:\n",
        "# nonexisting word\n",
        "analyses2 = a.analyze_words('Ձևաբայու')\n",
        "for ana2 in analyses2:\n",
        "    if ana2.lemma:\n",
        "      print(ana2.wf, ana2.lemma, ana2.gramm, ana2.gloss, ana2.stem, ana2.subwords, ana2.wfGlossed, ana2.otherData)\n",
        "    else:\n",
        "      print(ana2.wf, ana2.wf, \"N\", \"x\", ana2.stem, ana2.subwords, ana2.wfGlossed, ana2.otherData)"
      ],
      "metadata": {
        "id": "jMu1kHN03mxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyses = a.analyze_words([['և'], ['Ես', 'սիրում', 'եմ', 'քեզ', ':']],\n",
        "                           format='xml')\n",
        "for ana in analyses:\n",
        "    print(str(ana))"
      ],
      "metadata": {
        "id": "5b0wetvb3zvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyses = a.analyze_words(['Ձևաբանություն', [['և'], ['Ես', 'սիրում', 'եմ', 'քեզ', ':']]],\n",
        "                           format='json')\n",
        "for ana in analyses:\n",
        "    print(str(ana))"
      ],
      "metadata": {
        "id": "R735YSlN3_rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# analysis with disambiguation\n",
        "analyses = a.analyze_words(['Ես', 'սիրում', 'եմ', 'քեզ'], disambiguate=True)\n",
        "for ana in analyses:\n",
        "    if len(ana) > 1: tab = \"  \"\n",
        "    else: tab = \"\"\n",
        "    for wfo in ana:\n",
        "        print(tab, wfo.wf, wfo.lemma, wfo.gramm, wfo.gloss)"
      ],
      "metadata": {
        "id": "401nl-i84GMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Str = \"Սառը, վճիտ ապրիլյան օր էր, ու ժամացույցը խփում էր տասներեքը։ Չար քամուց թաքնվելու համար կզակը սեղմելով կրծքին՝ Ուինսթոն Սմիթն արագ ներս խցկվեց «Հաղթանակ» բնակելի տան ապակե շքադռնից՝ իր ետևից ներս թողնելով հատիկավոր փոշու մի ամբողջ փոթորիկ։\"\n",
        "\n",
        "StrDe = ' „Es war ein kalter, trostloser Apriltag, und die Uhr schlug dreizehn. Das Kinn an die Brust gedrückt, um sich vor dem bitteren Wind zu schützen, eilte Winston Smith durch die gläserne Veranda des Wohnhauses Victory und hinterließ einen körnigen Sturm Staub.\" '\n",
        "\n",
        "StrEn = ' \"It was a cold, dreary April day, and the clock struck thirteen. Tucking his chin to his chest to shield himself from the bitter wind, Winston Smith hurried through the glass porch of the Victory apartment building, leaving behind him a storm of granular dust.\" '"
      ],
      "metadata": {
        "id": "xByNh1fO4XKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Str = \"Սառը, վճիտ. ապրիլյան օր էր, ու ժամացույցը խփում էր տասներեքը։ Չար քամուց թաքնվելու համար կզակը սեղմելով կրծքին՝ Ուինսթոն Սմիթն արագ ներս խցկվեց «Հաղթանակ» բնակելի տան ապակե շքադռնից՝ իր ետևից ներս թողնելով հատիկավոր փոշու մի ամբողջ փոթորիկ։\""
      ],
      "metadata": {
        "id": "hySmqfbM4KBQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Lst = re.split(r'([ ,\\.:;\\!\\(\\)\\\"\\[\\]՞՝«»\\-\\—՝։\\։]+)', Str)\n",
        "LstTok = []\n",
        "for el in Lst:\n",
        "    el = el.strip()\n",
        "    if el != '': LstTok.append(el)\n"
      ],
      "metadata": {
        "id": "LD8NIcoycI9S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(LstTok)\n",
        "# does disambiguation work? not yet..."
      ],
      "metadata": {
        "id": "JK8FDH6A4nD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyses = a.analyze_words(LstTok, disambiguate=True)\n",
        "for ana in analyses:\n",
        "    if len(ana) > 1: tab = \"  \"\n",
        "    else: tab = \"\"\n",
        "    for wfo in ana:\n",
        "        print(tab, wfo.wf, wfo.lemma, wfo.gramm, wfo.gloss)"
      ],
      "metadata": {
        "id": "xI7ZHTveiMn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File annotation\n",
        "\n",
        "### defining functions to process the file"
      ],
      "metadata": {
        "id": "aeazgS7ShLAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec # The word2vec model class\n",
        "import gensim.downloader as api # Allows us to download some free training data"
      ],
      "metadata": {
        "id": "E8DhZznElVRD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vahram's model for English\n",
        "!wget https://heibox.uni-heidelberg.de/f/38de0eb8b2ec41d284d7/?dl=1\n",
        "!mv index.html?dl=1 WIKI_EN.model\n"
      ],
      "metadata": {
        "id": "Uy8itpV9ka1B",
        "outputId": "afde6ac9-d8ff-4af3-a03a-0d3ffb0a14f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-06 11:44:24--  https://heibox.uni-heidelberg.de/f/38de0eb8b2ec41d284d7/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/881c52e8-8f71-4c40-ad6d-9fa00342c7fb/WIKI_EN.model [following]\n",
            "--2025-09-06 11:44:25--  https://heibox.uni-heidelberg.de/seafhttp/files/881c52e8-8f71-4c40-ad6d-9fa00342c7fb/WIKI_EN.model\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 90291014 (86M) [application/octet-stream]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>]  86.11M  13.1MB/s    in 8.0s    \n",
            "\n",
            "2025-09-06 11:44:34 (10.7 MB/s) - ‘index.html?dl=1’ saved [90291014/90291014]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_WIKI_EN = Word2Vec.load(\"/content/WIKI_EN.model\")\n",
        "word_vectors_WIKI_EN = model_WIKI_EN.wv"
      ],
      "metadata": {
        "id": "SujtHRcJlM0L"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distance = word_vectors_WIKI_EN.similarity('obama', 'barak')\n",
        "distance2 = word_vectors_WIKI_EN.similarity('obama', 'zone')\n",
        "print('distance = %.4f' % distance)\n",
        "print('distance2 = %.4f' % distance2)"
      ],
      "metadata": {
        "id": "s4XFkcwDmIRb",
        "outputId": "72a31cb1-2e92-48c0-9895-0011d18e2dc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distance = 0.7685\n",
            "distance2 = 0.3015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizeHy(Str2tokenise, rePattern = r'([ ,\\.:;\\!\\(\\)\\\"\\[\\]՞՝«»\\-\\—՝։\\։]+)'):\n",
        "    LTokens = []\n",
        "    Lst = re.split(rePattern, Str2tokenise)\n",
        "    # LstTok = []\n",
        "    for el in Lst:\n",
        "        el = el.strip()\n",
        "        if el != '': LTokens.append(el)\n",
        "    return LTokens"
      ],
      "metadata": {
        "id": "_FMKB1I7v2ba"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merging together different lines of code\n",
        "\n",
        "def disambiguate(LTok2disambiguate, Window = 4, show_variants = False):\n",
        "    SDisambig = ''\n",
        "\n",
        "    analysesN = a.analyze_words(LTok2disambiguate, disambiguate=False)\n",
        "\n",
        "    # preparing data structures\n",
        "    LLContext = [] # empty list of contexts, indices are the same as with the Text list\n",
        "    LLText = [] # test to disambiguate, ambiguous interpretations are double entries\n",
        "    for ana in analysesN:\n",
        "        # creating context window from glosses\n",
        "        # if len(ana) > 1: tab = \"~\"\n",
        "        # else: tab = \"!\"\n",
        "        LwfoContext = []\n",
        "        LwfoText = []\n",
        "        for wfo in ana: # preserve lemmas / word forms which are not found in dictionary\n",
        "            if wfo.gramm == '': wfo.gramm = 'N'\n",
        "            if wfo.lemma == '': wfo.lemma = wfo.wf\n",
        "            if wfo.gloss == '': wfo.gloss = '[unknown]'\n",
        "\n",
        "            SWfo = f'{wfo.wf}\\t{wfo.gramm}\\t{wfo.lemma}\\t{wfo.gloss}'\n",
        "            # print(SWfo)\n",
        "            # FNoD.write(SWfo + '\\n')\n",
        "\n",
        "            # find the first part of the gloss, which may be in the word vectors model\n",
        "            REPart = re.match('([A-Za-z]+)', wfo.gloss)\n",
        "            if REPart:\n",
        "                SGlossMin = REPart.group(1)\n",
        "                SGlossMin = SGlossMin.lower()\n",
        "                # print(SGlossMin)\n",
        "            else:\n",
        "                SGlossMin = '[NONE]'\n",
        "            LwfoContext.append(SGlossMin)\n",
        "            LwfoText.append(SWfo)\n",
        "        LLContext.append(LwfoContext)\n",
        "        LLText.append(LwfoText)\n",
        "\n",
        "    # for el in LLContext: print(el)\n",
        "    # for el in LLText: print(el)\n",
        "\n",
        "    # print(len(LLContext))\n",
        "    # print(len(LLText))\n",
        "\n",
        "    sys.stderr.write('.')\n",
        "\n",
        "\n",
        "    for i in range(len(LLText)):\n",
        "        if len(LLText[i]) > 1:\n",
        "            # print(LLText[i])\n",
        "            # print(LLContext[i])\n",
        "            # collect context window +- 3 words\n",
        "            iwStart = i-Window\n",
        "            if iwStart <0: iwStart=0\n",
        "            iwEnd = i+Window\n",
        "            if iwEnd > len(LLText): iwEnd = len(LLText)\n",
        "            # iwLen = iwEnd - iwStart\n",
        "            winContext = LLContext[iwStart:iwEnd]\n",
        "            # print(winContext)\n",
        "            LScores = []\n",
        "            LScCand = []\n",
        "\n",
        "            for candidate in LLContext[i]:\n",
        "                ScoreCand = 0\n",
        "                for LCtx in winContext:\n",
        "                    for Ctx in LCtx:\n",
        "                        try: distance = word_vectors_WIKI_EN.similarity(candidate, Ctx)\n",
        "                        except: distance = 0\n",
        "                        ScoreCand += distance\n",
        "                LScores.append((candidate,ScoreCand))\n",
        "                LScCand.append(ScoreCand)\n",
        "            LScores.sort(key=lambda a: a[1], reverse=True)\n",
        "            # print(LScores)\n",
        "\n",
        "            max_value = max(LScCand)\n",
        "            #  Return the max value of the list\n",
        "            max_index = LScCand.index(max_value)\n",
        "            StoWrite = LLText[i][max_index] + '\\n'\n",
        "            SDisambig += StoWrite\n",
        "\n",
        "            if show_variants == True:\n",
        "                for el in LLText[i]:\n",
        "                    StoWrite = '\\t~\\t' + el + '\\n'\n",
        "                    SDisambig += StoWrite\n",
        "                for el in LScores:\n",
        "                    StoWrite = '\\t~sc:\\t' + str(el) + '\\n'\n",
        "                    SDisambig += StoWrite\n",
        "        else:\n",
        "            StoWrite = LLText[i][0] + '\\n'\n",
        "            SDisambig += StoWrite\n",
        "\n",
        "    return SDisambig"
      ],
      "metadata": {
        "id": "TpxYmxdTwGyH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### give names for the input and output file"
      ],
      "metadata": {
        "id": "Kb_7hsYchZZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# FInText = open('/content/udhr/udhr_hye_v03.txt','r')\n",
        "# FOutText = open('/content/udhrTT/udhr_hye_vert.txt','w')\n",
        "\n",
        "\n",
        "FInText = open('udhr_hye_v03.txt','r')\n",
        "FOutText = open('udhr_hye_v03.vert','w')"
      ],
      "metadata": {
        "id": "YMgnQ_crwuQw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for SLine in FInText:\n",
        "\n",
        "    count += 1\n",
        "    if count % 100 == 0: sys.stderr.write('\\n')\n",
        "    SLine = SLine.strip()\n",
        "    LTok = tokenizeHy(SLine)\n",
        "    # SDisambig = disambiguate(LTok, show_variants = True)\n",
        "    SDisambig = disambiguate(LTok, show_variants = False)\n",
        "    # FOutText.write('<p>\\n')\n",
        "    # FOutText.write('\\n')\n",
        "    FOutText.write(SDisambig)\n",
        "    # FOutText.write('</p>\\n')\n",
        "    FOutText.write('\\n')\n",
        "\n",
        "FOutText.flush()"
      ],
      "metadata": {
        "id": "61sqTRpmxOFF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "403f310d-1577-4764-9a1d-fa4864599418"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "...................................................................................................\n",
            "....................................................................................................\n",
            "................"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### optional: Creating a text file with lemmas\n",
        "\n",
        "all word forms are converted into lemmas (might be used for training word2vec models)"
      ],
      "metadata": {
        "id": "c9GN7kuzhfBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !awk -F '\\t' '(NF==4){printf \"%s \", $3}(NF!=4){printf \"\\n\"}' < /content/udhrTT/udhr_hye_vert.txt >/content/udhrTT/udhr_hye_lem.txt\n",
        "!awk -F '\\t' '(NF==4){printf \"%s \", $3}(NF!=4){printf \"\\n\"}' < udhr_hye_v03.vert >udhr_hye_v03_lem.txt"
      ],
      "metadata": {
        "id": "ScstreEf1RDn"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S01LemmatizationEnHyV01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPpWi3t3hob6A82tTncN/c+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/corpustools/blob/main/S01LemmatizationEnHyV01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1dRDzwnNTst"
      },
      "source": [
        "# Morphological analysis for English and Armenian\n",
        "\n",
        "We will create a workflow for analysing English and Armenian texts\n",
        "\n",
        "For English we will use the TreeTagger \n",
        "\n",
        "For Armenian we will use the git repository with Armenian morphological analyser: \n",
        "https://github.com/timarkh/uniparser-grammar-eastern-armenian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nX7LqgRytTb"
      },
      "source": [
        "# importing python libraries\n",
        "import os, re, sys"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HjwCMIqsztj"
      },
      "source": [
        "## English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp8FKqOANJ6A"
      },
      "source": [
        "# installing TreeTagger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUlWsn6UmgZb"
      },
      "source": [
        "%%bash\n",
        "mkdir treetagger\n",
        "cd treetagger\n",
        "# Download the tagger package for your system (PC-Linux, Mac OS-X, ARM64, ARMHF, ARM-Android, PPC64le-Linux).\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/tree-tagger-linux-3.2.4.tar.gz\n",
        "tar -xzvf tree-tagger-linux-3.2.4.tar.gz\n",
        "# Download the tagging scripts into the same directory.\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/tagger-scripts.tar.gz\n",
        "gunzip tagger-scripts.tar.gz\n",
        "# Download the installation script install-tagger.sh.\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/install-tagger.sh\n",
        "# Download the parameter files for the languages you want to process.\n",
        "# list of all files (parameter files) https://cis.lmu.de/~schmid/tools/TreeTagger/#parfiles\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/english.par.gz\n",
        "sh install-tagger.sh\n",
        "cd ..\n",
        "sudo pip install treetaggerwrapper\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VR2d5n_mo8R"
      },
      "source": [
        "%%bash\n",
        "wget https://heibox.uni-heidelberg.de/f/95a3875771c040db959a/?dl=1\n",
        "mv index.html?dl=1 humanrights02.txt\n",
        "\n",
        "wget https://heibox.uni-heidelberg.de/f/cdf240db84ca4718b718/?dl=1\n",
        "mv index.html?dl=1 en1984.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMxbGb2-m58C"
      },
      "source": [
        "!head --lines=20 humanrights02.txt\n",
        "!wc humanrights02.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HYUJAY5rAte"
      },
      "source": [
        "!./treetagger/cmd/tree-tagger-english en1984.txt >en1984_vert.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PNvmwwVrwyn"
      },
      "source": [
        "!head --lines=20 en1984_vert.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7stVR-G1rFat"
      },
      "source": [
        "!./treetagger/cmd/tree-tagger-english humanrights02.txt >humanrights02_vert.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf0ZClfLrZd3"
      },
      "source": [
        "!head --lines=20 humanrights02_vert.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC5cBgvas4vQ"
      },
      "source": [
        "## Armenian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW7HUG3TQzjU"
      },
      "source": [
        "# installing Armenian morphological analyser\n",
        "!git clone https://github.com/timarkh/uniparser-grammar-eastern-armenian"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgrVLqTTStuN"
      },
      "source": [
        "# Python classes\n",
        "!pip3 install uniparser-eastern-armenian"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bALlnO8gY4cF"
      },
      "source": [
        "# disambiguation\n",
        "!sudo apt-get install cg3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5-Y0JQES0oT"
      },
      "source": [
        "from uniparser_eastern_armenian import EasternArmenianAnalyzer\n",
        "a = EasternArmenianAnalyzer()\n",
        "analyses = a.analyze_words('Ձևաբանություն')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1gZH7s3XfMR",
        "outputId": "9af3f611-dba1-4f2f-e92a-e5c3ed6ef487",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for ana in analyses:\n",
        "    print(ana.wf, ana.lemma, ana.gramm, ana.gloss, ana.stem, ana.subwords, ana.wfGlossed, ana.otherData)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ձևաբանություն ձեւաբանություն N,inanim,sg,nom,nonposs morphology ձևաբանություն. [] ձևաբանություն [('trans_en', 'morphology')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nonexisting word\n",
        "analyses2 = a.analyze_words('Ձևաբայու')"
      ],
      "metadata": {
        "id": "NjugFV12tbrz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ana2 in analyses2:\n",
        "    if ana2.lemma:\n",
        "      print(ana2.wf, ana2.lemma, ana2.gramm, ana2.gloss, ana2.stem, ana2.subwords, ana2.wfGlossed, ana2.otherData)\n",
        "    else:\n",
        "      print(ana2.wf, ana2.wf, \"N\", \"x\", ana2.stem, ana2.subwords, ana2.wfGlossed, ana2.otherData)"
      ],
      "metadata": {
        "id": "nB0YgGgUtg78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# which fields we have in analysis:"
      ],
      "metadata": {
        "id": "HmBpgm-Bt1jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqmYRwfBlbNo"
      },
      "source": [
        "dir(ana)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ddvunxGTXQf"
      },
      "source": [
        "analyses = a.analyze_words([['և'], ['Ես', 'սիրում', 'եմ', 'քեզ', ':']],\n",
        "                           format='xml')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuOyUBXmTbep",
        "outputId": "6de2d309-5975-4912-d659-cef6a97ca133",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for ana in analyses:\n",
        "    print(str(ana))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<w><ana lex=\"եւ\" gr=\"CONJ\" parts=\"և\" gloss=\"and\" trans_en=\"and, too, either\"></ana>և</w>']\n",
            "['<w><ana lex=\"ես\" gr=\"PRON,S,hum,sg,nom\" parts=\"ես\" gloss=\"me\" trans_en=\"I\"></ana><ana lex=\"է\" gr=\"V,intr,prs,sg,2\" parts=\"ե-ս\" gloss=\"be-PRS.2SG\" trans_en=\"be\"></ana>Ես</w>', '<w><ana lex=\"սիրել\" gr=\"V,tr,cvb,ipfv\" parts=\"սիր-ում\" gloss=\"love-CVB.IPFV\" trans_en=\"love, have a passion/an affection for, like\"></ana>սիրում</w>', '<w><ana lex=\"է\" gr=\"V,intr,prs,sg,1\" parts=\"ե-մ\" gloss=\"be-PRS.1SG\" trans_en=\"be\"></ana>եմ</w>', '<w><ana lex=\"դու\" gr=\"PRON,S,hum,sg,dat\" parts=\"քեզ\" gloss=\"thou\" trans_en=\"you, thou\"></ana>քեզ</w>', '<w><ana lex=\"\" gr=\"\" parts=\"\" gloss=\"\"></ana>:</w>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWxToCMSTyL6"
      },
      "source": [
        "analyses = a.analyze_words(['Ձևաբանություն', [['և'], ['Ես', 'սիրում', 'եմ', 'քեզ', ':']]],\n",
        "                           format='json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxEQfFZvT1KM",
        "outputId": "d710d1fd-adbc-471f-bc40-f75968527b93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for ana in analyses:\n",
        "    print(str(ana))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'wf': 'Ձևաբանություն', 'lemma': 'ձեւաբանություն', 'gramm': ['N', 'inanim', 'sg', 'nom', 'nonposs'], 'wfGlossed': 'ձևաբանություն', 'gloss': 'morphology', 'trans_en': 'morphology'}]\n",
            "[[[{'wf': 'և', 'lemma': 'եւ', 'gramm': ['CONJ'], 'wfGlossed': 'և', 'gloss': 'and', 'trans_en': 'and, too, either'}]], [[{'wf': 'Ես', 'lemma': 'ես', 'gramm': ['PRON', 'S', 'hum', 'sg', 'nom'], 'wfGlossed': 'ես', 'gloss': 'me', 'trans_en': 'I'}, {'wf': 'Ես', 'lemma': 'է', 'gramm': ['V', 'intr', 'prs', 'sg', '2'], 'wfGlossed': 'ե-ս', 'gloss': 'be-PRS.2SG', 'trans_en': 'be'}], [{'wf': 'սիրում', 'lemma': 'սիրել', 'gramm': ['V', 'tr', 'cvb', 'ipfv'], 'wfGlossed': 'սիր-ում', 'gloss': 'love-CVB.IPFV', 'trans_en': 'love, have a passion/an affection for, like'}], [{'wf': 'եմ', 'lemma': 'է', 'gramm': ['V', 'intr', 'prs', 'sg', '1'], 'wfGlossed': 'ե-մ', 'gloss': 'be-PRS.1SG', 'trans_en': 'be'}], [{'wf': 'քեզ', 'lemma': 'դու', 'gramm': ['PRON', 'S', 'hum', 'sg', 'dat'], 'wfGlossed': 'քեզ', 'gloss': 'thou', 'trans_en': 'you, thou'}], [{'wf': ':', 'lemma': '', 'gramm': [], 'wfGlossed': '', 'gloss': ''}]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWNEo_RwUcSB"
      },
      "source": [
        "# analysis with disambiguation\n",
        "analyses = a.analyze_words(['Ես', 'սիրում', 'եմ', 'քեզ'], disambiguate=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRb9dCQoUdJc",
        "outputId": "2a001752-2ce0-4308-cd49-a24a3d89fc01"
      },
      "source": [
        "for ana in analyses:\n",
        "    if len(ana) > 1: tab = \"  \"\n",
        "    else: tab = \"\"\n",
        "    for wfo in ana:\n",
        "        print(tab, wfo.wf, wfo.lemma, wfo.gramm, wfo.gloss)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Ես է V,intr,prs,sg,2 be-PRS.2SG\n",
            "   Ես ես PRON,S,hum,sg,nom me\n",
            " սիրում սիրել V,tr,cvb,ipfv love-CVB.IPFV\n",
            " եմ է V,intr,prs,sg,1 be-PRS.1SG\n",
            " քեզ դու PRON,S,hum,sg,dat thou\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vzlcQSQlw55"
      },
      "source": [
        "print(type(wfo))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0P_zV02ZxQ8"
      },
      "source": [
        "dir(wfo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Str = \"Սառը, վճիտ ապրիլյան օր էր, ու ժամացույցը խփում էր տասներեքը։ Չար քամուց թաքնվելու համար կզակը սեղմելով կրծքին՝ Ուինսթոն Սմիթն արագ ներս խցկվեց «Հաղթանակ» բնակելի տան ապակե շքադռնից՝ իր ետևից ներս թողնելով հատիկավոր փոշու մի ամբողջ փոթորիկ։\"\n",
        "\n",
        "StrDe = ' „Es war ein kalter, trostloser Apriltag, und die Uhr schlug dreizehn. Das Kinn an die Brust gedrückt, um sich vor dem bitteren Wind zu schützen, eilte Winston Smith durch die gläserne Veranda des Wohnhauses Victory und hinterließ einen körnigen Sturm Staub.\" '\n",
        "\n",
        "StrEn = ' \"It was a cold, dreary April day, and the clock struck thirteen. Tucking his chin to his chest to shield himself from the bitter wind, Winston Smith hurried through the glass porch of the Victory apartment building, leaving behind him a storm of granular dust.\" '"
      ],
      "metadata": {
        "id": "SC571ZMVQesT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Str = \"Սառը, վճիտ ապրիլյան օր էր, ու ժամացույցը խփում էր տասներեքը։ Չար քամուց թաքնվելու համար կզակը սեղմելով կրծքին՝ Ուինսթոն Սմիթն արագ ներս խցկվեց «Հաղթանակ» բնակելի տան ապակե շքադռնից՝ իր ետևից ներս թողնելով հատիկավոր փոշու մի ամբողջ փոթորիկ։\""
      ],
      "metadata": {
        "id": "iX8SOtvjOMIq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Lst = re.split('[ ,\\.:;\\!\\(\\)\\\"\\[\\]՞՝«»\\-\\—՝։\\։]+', Str)"
      ],
      "metadata": {
        "id": "C0WZdU0lQAqd"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Lst)"
      ],
      "metadata": {
        "id": "TQoRJrXbQLAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Lst = re.split('([ ,\\.:;\\!\\(\\)\\\"\\[\\]՞՝«»\\-\\—՝։\\։]+)', Str)"
      ],
      "metadata": {
        "id": "GcwU_AmoOSun"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Lst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dymdhgIOd5k",
        "outputId": "43dd2beb-3a53-4a9d-e554-ec3c1b6d5794"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Սառը', ', ', 'վճիտ', ' ', 'ապրիլյան', ' ', 'օր', ' ', 'էր', ', ', 'ու', ' ', 'ժամացույցը', ' ', 'խփում', ' ', 'էր', ' ', 'տասներեքը', '։ ', 'Չար', ' ', 'քամուց', ' ', 'թաքնվելու', ' ', 'համար', ' ', 'կզակը', ' ', 'սեղմելով', ' ', 'կրծքին', '՝ ', 'Ուինսթոն', ' ', 'Սմիթն', ' ', 'արագ', ' ', 'ներս', ' ', 'խցկվեց', ' «', 'Հաղթանակ', '» ', 'բնակելի', ' ', 'տան', ' ', 'ապակե', ' ', 'շքադռնից', '՝ ', 'իր', ' ', 'ետևից', ' ', 'ներս', ' ', 'թողնելով', ' ', 'հատիկավոր', ' ', 'փոշու', ' ', 'մի', ' ', 'ամբողջ', ' ', 'փոթորիկ', '։', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LstTok = []\n",
        "for el in Lst:\n",
        "    el = el.strip()\n",
        "    if el != '': LstTok.append(el)\n"
      ],
      "metadata": {
        "id": "1Sib1_35PJ1Q"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(LstTok)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUp8WOriPSG-",
        "outputId": "2779fe9f-ff4d-4928-d57b-d0f5036d0d7e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Սառը', ',', 'վճիտ', 'ապրիլյան', 'օր', 'էր', ',', 'ու', 'ժամացույցը', 'խփում', 'էր', 'տասներեքը', '։', 'Չար', 'քամուց', 'թաքնվելու', 'համար', 'կզակը', 'սեղմելով', 'կրծքին', '՝', 'Ուինսթոն', 'Սմիթն', 'արագ', 'ներս', 'խցկվեց', '«', 'Հաղթանակ', '»', 'բնակելի', 'տան', 'ապակե', 'շքադռնից', '՝', 'իր', 'ետևից', 'ներս', 'թողնելով', 'հատիկավոր', 'փոշու', 'մի', 'ամբողջ', 'փոթորիկ', '։']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does disambiguation work? Checking..."
      ],
      "metadata": {
        "id": "PHpAVAfcZiQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analysesD = a.analyze_words(LstTok, disambiguate=True)\n",
        "analysesN = a.analyze_words(LstTok, disambiguate=False)"
      ],
      "metadata": {
        "id": "B3-m_jN8RVe4"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Showing ambiguous words (with tilde)"
      ],
      "metadata": {
        "id": "yrjmOcUURP7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FD = open('FD.txt', 'w')\n",
        "for ana in analysesD:\n",
        "    if len(ana) > 1: tab = \"~\"\n",
        "    else: tab = \"!\"\n",
        "    for wfo in ana:\n",
        "        if wfo.gramm == '': wfo.gramm = 'N'\n",
        "        if wfo.lemma == '': wfo.lemma = wfo.wf\n",
        "        if wfo.gloss == '': wfo.gloss = '[unknown]'\n",
        "        SWfo = f'{tab}\\t{wfo.wf}\\t{wfo.gramm}\\t{wfo.lemma}\\t{wfo.gloss}'\n",
        "        print(SWfo)\n",
        "        FD.write(SWfo + '\\n')"
      ],
      "metadata": {
        "id": "D-VS3p0VRmYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FNoD = open('FNoD.txt', 'w')\n",
        "for ana in analysesN:\n",
        "    if len(ana) > 1: tab = \"~\"\n",
        "    else: tab = \"!\"\n",
        "    for wfo in ana:\n",
        "        if wfo.gramm == '': wfo.gramm = 'N'\n",
        "        if wfo.lemma == '': wfo.lemma = wfo.wf\n",
        "        if wfo.gloss == '': wfo.gloss = '[unknown]'\n",
        "        SWfo = f'{tab}\\t{wfo.wf}\\t{wfo.gramm}\\t{wfo.lemma}\\t{wfo.gloss}'\n",
        "        print(SWfo)\n",
        "        FNoD.write(SWfo + '\\n')"
      ],
      "metadata": {
        "id": "B76IvLc3RzV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!diff FD.txt FNoD.txt"
      ],
      "metadata": {
        "id": "9eJBwukwVoL3"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "... did disambiguation work?"
      ],
      "metadata": {
        "id": "BcpRWMymZvsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Translation-based disambiguation\n",
        "Downloading word vector model"
      ],
      "metadata": {
        "id": "gGwDPCJFb06B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec # The word2vec model class\n",
        "import gensim.downloader as api # Allows us to download some free training data"
      ],
      "metadata": {
        "id": "ZZeFEOOsb6za"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vahram's model for English\n",
        "!wget https://heibox.uni-heidelberg.de/f/c2ba64e4ad844f3a99d4/?dl=1\n",
        "!cp index.html?dl=1 WIKI_EN.model"
      ],
      "metadata": {
        "id": "qR1xGbgccek1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_WIKI_EN = Word2Vec.load(\"/content/WIKI_EN.model\")\n",
        "word_vectors_WIKI_EN = model_WIKI_EN.wv"
      ],
      "metadata": {
        "id": "Cu655tGncjuC"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distance = model_WIKI_EN.similarity('obama', 'barak')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPvYVjGRcy9v",
        "outputId": "e1cd4063-85a6-489b-e40c-3fe6775ee00a"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('distance = %.4f' % distance)"
      ],
      "metadata": {
        "id": "4HuIgOx-c5vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing disambiguation\n",
        "4-word window"
      ],
      "metadata": {
        "id": "NbxVy1MUefet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \n",
        "LLContext = [] # empty list of contexts, indices are the same as with the Text list\n",
        "LLText = [] # test to disambiguate, ambiguous interpretations are double entries\n",
        "for ana in analysesN:\n",
        "    # creating context window from glosses\n",
        "    # if len(ana) > 1: tab = \"~\"\n",
        "    # else: tab = \"!\"\n",
        "    LwfoContext = []\n",
        "    LwfoText = []\n",
        "    for wfo in ana:\n",
        "        if wfo.gramm == '': wfo.gramm = 'N'\n",
        "        if wfo.lemma == '': wfo.lemma = wfo.wf\n",
        "        if wfo.gloss == '': wfo.gloss = '[unknown]'\n",
        "\n",
        "        SWfo = f'{wfo.wf}\\t{wfo.gramm}\\t{wfo.lemma}\\t{wfo.gloss}'\n",
        "        # print(SWfo)\n",
        "        # FNoD.write(SWfo + '\\n')\n",
        "\n",
        "        # find the first part of the gloss, which may be in the word vectors model\n",
        "        REPart = re.match('([A-Za-z]+)', wfo.gloss)\n",
        "        if REPart: \n",
        "            SGlossMin = REPart.group(1)\n",
        "            SGlossMin = SGlossMin.lower()\n",
        "            # print(SGlossMin)\n",
        "        else:\n",
        "            SGlossMin = '[NONE]'\n",
        "        LwfoContext.append(SGlossMin)\n",
        "        LwfoText.append(SWfo)\n",
        "    LLContext.append(LwfoContext)\n",
        "    LLText.append(LwfoText)\n",
        "\n",
        "for el in LLContext: print(el)\n",
        "for el in LLText: print(el)\n",
        "\n",
        "print(len(LLContext))\n",
        "print(len(LLText))\n",
        "\n",
        "print('done!...\\n')"
      ],
      "metadata": {
        "id": "KZQCgKWne2IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(LLText)):\n",
        "    if len(LLText[i]) > 1: \n",
        "        print(LLText[i])\n",
        "        print(LLContext[i])"
      ],
      "metadata": {
        "id": "ZABo4fY2nA7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disambiguation algorithm\n",
        "FDisambiguate = open('FDisambiguate.txt', 'w')\n",
        "for i in range(len(LLText)):\n",
        "    if len(LLText[i]) > 1: \n",
        "        # print(LLText[i])\n",
        "        # print(LLContext[i])\n",
        "        # collect context window +- 3 words\n",
        "        iwStart = i-4\n",
        "        if iwStart <0: iwStart=0\n",
        "        iwEnd = i+4\n",
        "        if iwEnd > len(LLText): iwEnd = len(LLText)\n",
        "        # iwLen = iwEnd - iwStart\n",
        "        winContext = LLContext[iwStart:iwEnd]\n",
        "        print(winContext)\n",
        "        LScores = []\n",
        "        LScCand = []\n",
        "\n",
        "        for candidate in LLContext[i]:\n",
        "            ScoreCand = 0\n",
        "            for LCtx in winContext:\n",
        "                for Ctx in LCtx:\n",
        "                    try: distance = model_WIKI_EN.similarity(candidate, Ctx)\n",
        "                    except: distance = 0\n",
        "                    ScoreCand += distance\n",
        "            LScores.append((candidate,ScoreCand))\n",
        "            LScCand.append(ScoreCand)\n",
        "        LScores.sort(key=lambda a: a[1], reverse=True)\n",
        "        print(LScores)\n",
        "\n",
        "        max_value = max(LScCand)\n",
        "        #  Return the max value of the list\n",
        "        max_index = LScCand.index(max_value)\n",
        "        FDisambiguate.write(LLText[i][max_index] + '\\n')\n",
        "        for el in LLText[i]:\n",
        "            FDisambiguate.write('\\t~\\t' + el + '\\n') \n",
        "    else:\n",
        "        FDisambiguate.write(LLText[i][0] + '\\n')\n",
        "\n",
        "    \n",
        "FDisambiguate.flush()\n",
        "        \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTMxA_Zqnhb4",
        "outputId": "77b45c72-e703-40ca-8173-573f0bea24f8"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['thirteen'], ['[NONE]'], ['wicked'], ['wind'], ['hide', 'hide'], ['for', 'number'], ['jaw'], ['press']]\n",
            "[('hide', 4.008937910199165), ('hide', 4.008937910199165)]\n",
            "[['[NONE]'], ['wicked'], ['wind'], ['hide', 'hide'], ['for', 'number'], ['jaw'], ['press'], ['breast']]\n",
            "[('for', 2.7643033862113953), ('number', 2.3636593222618103)]\n",
            "[['[NONE]'], ['[NONE]'], ['[NONE]'], ['fast'], ['sister', 'inside'], ['push', 'squeeze'], ['[NONE]'], ['victory']]\n",
            "[('inside', 2.6393820494413376), ('sister', 1.7206918150186539)]\n",
            "[['[NONE]'], ['[NONE]'], ['fast'], ['sister', 'inside'], ['push', 'squeeze'], ['[NONE]'], ['victory'], ['[NONE]']]\n",
            "[('squeeze', 3.150343343615532), ('push', 3.050136521458626)]\n",
            "[['[NONE]'], ['victory'], ['[NONE]'], ['dwelling'], ['give', 'house'], ['glass'], ['[NONE]'], ['[NONE]']]\n",
            "[('house', 2.2411059141159058), ('give', 2.0052715837955475)]\n",
            "[['glass'], ['[NONE]'], ['[NONE]'], ['thing'], ['from', 'back'], ['sister', 'inside'], ['leave'], ['grainy']]\n",
            "[('back', 3.5721226930618286), ('from', 3.2438770830631256)]\n",
            "[['[NONE]'], ['[NONE]'], ['thing'], ['from', 'back'], ['sister', 'inside'], ['leave'], ['grainy'], ['dust']]\n",
            "[('inside', 3.880270466208458), ('sister', 2.3262811079621315)]\n",
            "[['sister', 'inside'], ['leave'], ['grainy'], ['dust'], ['one', 'proh'], ['all'], ['storm'], ['[NONE]']]\n",
            "[('one', 2.846338912844658), ('proh', 0)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzINXeL7s9EL"
      },
      "source": [
        "# downloading and analysing texts"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJzA4Fg9tCaH"
      },
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/e0bfae444a5a4c76957b/?dl=1\n",
        "!mv index.html?dl=1 hy1984.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOXdvc_9uv0J"
      },
      "source": [
        "FInText = open('hy1984.txt','r')\n",
        "FOutText = open('hy1984_vert.txt','w')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdxZiI7ZyLGW"
      },
      "source": [
        "for SLine in FInText:\n",
        "    SLine = SLine.strip()\n",
        "    ListOfWords = re.split('[ ,\\.:;\\!\\(\\)\\\"\\[\\]՞՝«»\\-\\—՝։\\։]+', SLine) # tokenize: split on white spaces and punctuation\n",
        "    # if len(ListOfWords) > 0: FOutText.write(str(ListOfWords) + '\\n')\n",
        "    analyses = a.analyze_words(ListOfWords, disambiguate=False)\n",
        "    FOutText.write('<p>\\n')\n",
        "    for ana in analyses:\n",
        "        # for wfo in ana:\n",
        "        # how to type all variants + disambiguate ?\n",
        "        for wfo in ana:\n",
        "          # wfo = ana[0]\n",
        "          FOutText.write(wfo.wf + '\\t' + wfo.gramm + '\\t' + wfo.lemma + '\\t' + wfo.gloss + '\\n')\n",
        "          #    FOutText.write(wfo.wf + '\\t' + wfo.gramm + '\\t' + wfo.lemma + '\\t' + wfo.gloss + '\\n')\n",
        "    FOutText.write('</p>\\n')\n",
        "FOutText.flush()"
      ],
      "execution_count": 27,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/corpustools/blob/main/S101lemHYv202511RB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rule-based lemmatization for Armenian\n",
        "\n",
        "## Stage 1: Downloading example file\n",
        "Example: Universal declaration of human rights\n",
        "\n",
        "If downloading from the UN website, the link aboout pdf2txt conversion is https://chatgpt.com/share/68bbeed8-0d08-800e-8418-b0816c6393d4\n",
        "\n"
      ],
      "metadata": {
        "id": "j2SZAuuE1441"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kvSipdQ7yz6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "# Downloading UDHR\n",
        "# !wget https://unicode.org/udhr/assemblies/udhr_txt.zip\n",
        "# Alternatively, downloading from the UN website, converting pdf to txt\n",
        "# the link aboout pdf2txt conversion: https://chatgpt.com/share/68bbeed8-0d08-800e-8418-b0816c6393d4\n",
        "# !rm --recursive udhr-all-languages\n",
        "\n",
        "# delete lines which are not translations in some files (hy)\n",
        "# delete between lines $a and $b inclusive\n",
        "a=9\n",
        "b=21\n",
        "awk -v m=$a -v n=$b 'm <= NR && NR <= n {next} {print}' < /content/udhr-all-languages/udhr_hye.txt >/content/udhr-all-languages/udhr_hye_v02.txt\n",
        "\n",
        "a=1\n",
        "b=6\n",
        "awk -v m=$a -v n=$b 'm <= NR && NR <= n {next} {print}' < /content/udhr-all-languages/udhr_hye_v02.txt >/content/udhr-all-languages/udhr_hye_v03.txt\n",
        "\n",
        "\n",
        "\n",
        "# put paragraph tags\n",
        "# awk '{print \"<p>\\n\"$0 ; print \"</p>\"}' udhr/udhr_hye2.txt >udhr/udhr_hye_v03.txt\n",
        "\n",
        "\n",
        "cp /content/udhr-all-languages/udhr_hye_v03.txt /content/udhr_hye_v03.txt\n",
        "\n"
      ],
      "metadata": {
        "id": "gWOntGq42JvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing packages\n",
        "\n",
        "- gensim word2vec for similarity-based disambiguation\n",
        "- Eastern Armenian Analyser from https://github.com/timarkh/uniparser-grammar-eastern-armenian , which outputs translations into English\n",
        "\n"
      ],
      "metadata": {
        "id": "PGF35NFNxe-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "\n",
        "# installing Armenian morphological analyser\n",
        "!git clone https://github.com/timarkh/uniparser-grammar-eastern-armenian\n",
        "# Python classes\n",
        "!pip3 install uniparser-eastern-armenian\n",
        "# disambiguation\n",
        "!sudo apt-get install cg3"
      ],
      "metadata": {
        "id": "jF1Sa8dm3WFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing python libraries\n",
        "import os, re, sys\n",
        "\n",
        "from uniparser_eastern_armenian import EasternArmenianAnalyzer\n",
        "a = EasternArmenianAnalyzer()\n",
        "analyses = a.analyze_words('Ձևաբանություն')\n",
        "for ana in analyses:\n",
        "    print(ana.wf, ana.lemma, ana.gramm, ana.gloss, ana.stem, ana.subwords, ana.wfGlossed, ana.otherData)"
      ],
      "metadata": {
        "id": "dRFuA6sN3Z62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional part: trying different output format with disambiguation, etc.\n",
        "\n",
        "This can be skipped. You can go directly to the \"File annotation\" section"
      ],
      "metadata": {
        "id": "b9_WoVsxg3Ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trying out:\n",
        "# nonexisting word\n",
        "analyses2 = a.analyze_words('Ձևաբայու')\n",
        "for ana2 in analyses2:\n",
        "    if ana2.lemma:\n",
        "      print(ana2.wf, ana2.lemma, ana2.gramm, ana2.gloss, ana2.stem, ana2.subwords, ana2.wfGlossed, ana2.otherData)\n",
        "    else:\n",
        "      print(ana2.wf, ana2.wf, \"N\", \"x\", ana2.stem, ana2.subwords, ana2.wfGlossed, ana2.otherData)"
      ],
      "metadata": {
        "id": "jMu1kHN03mxE",
        "outputId": "32533d8f-119f-4924-ee13-487e6d1b6330",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ձևաբայու Ձևաբայու N x  []  []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyses = a.analyze_words([['և'], ['Ես', 'սիրում', 'եմ', 'քեզ', ':']],\n",
        "                           format='xml')\n",
        "for ana in analyses:\n",
        "    print(str(ana))"
      ],
      "metadata": {
        "id": "5b0wetvb3zvF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3be079e-dde2-49fd-f3e3-84af301c70ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<w><ana lex=\"եւ\" gr=\"CONJ\" parts=\"և\" gloss=\"and\" trans_en=\"and, too, either\"></ana>և</w>']\n",
            "['<w><ana lex=\"ես\" gr=\"PRON,S,hum,sg,nom\" parts=\"ես\" gloss=\"me\" trans_en=\"I\"></ana><ana lex=\"է\" gr=\"V,intr,prs,sg,2\" parts=\"ե-ս\" gloss=\"be-PRS.2SG\" trans_en=\"be\"></ana>Ես</w>', '<w><ana lex=\"սիրել\" gr=\"V,tr,cvb,ipfv\" parts=\"սիր-ում\" gloss=\"love-CVB.IPFV\" trans_en=\"love, have a passion/an affection for, like\"></ana>սիրում</w>', '<w><ana lex=\"է\" gr=\"V,intr,prs,sg,1\" parts=\"ե-մ\" gloss=\"be-PRS.1SG\" trans_en=\"be\"></ana>եմ</w>', '<w><ana lex=\"դու\" gr=\"PRON,S,hum,sg,dat\" parts=\"քեզ\" gloss=\"thou\" trans_en=\"you, thou\"></ana>քեզ</w>', '<w><ana lex=\"\" gr=\"\" parts=\"\" gloss=\"\"></ana>:</w>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyses = a.analyze_words(['Ձևաբանություն', [['և'], ['Ես', 'սիրում', 'եմ', 'քեզ', ':']]],\n",
        "                           format='json')\n",
        "for ana in analyses:\n",
        "    print(str(ana))"
      ],
      "metadata": {
        "id": "R735YSlN3_rb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89bb4485-2587-418c-fb23-77e3663deffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'wf': 'Ձևաբանություն', 'lemma': 'ձեւաբանություն', 'gramm': ['N', 'inanim', 'sg', 'nom', 'nonposs'], 'wfGlossed': 'ձևաբանություն', 'gloss': 'morphology', 'trans_en': 'morphology'}]\n",
            "[[[{'wf': 'և', 'lemma': 'եւ', 'gramm': ['CONJ'], 'wfGlossed': 'և', 'gloss': 'and', 'trans_en': 'and, too, either'}]], [[{'wf': 'Ես', 'lemma': 'ես', 'gramm': ['PRON', 'S', 'hum', 'sg', 'nom'], 'wfGlossed': 'ես', 'gloss': 'me', 'trans_en': 'I'}, {'wf': 'Ես', 'lemma': 'է', 'gramm': ['V', 'intr', 'prs', 'sg', '2'], 'wfGlossed': 'ե-ս', 'gloss': 'be-PRS.2SG', 'trans_en': 'be'}], [{'wf': 'սիրում', 'lemma': 'սիրել', 'gramm': ['V', 'tr', 'cvb', 'ipfv'], 'wfGlossed': 'սիր-ում', 'gloss': 'love-CVB.IPFV', 'trans_en': 'love, have a passion/an affection for, like'}], [{'wf': 'եմ', 'lemma': 'է', 'gramm': ['V', 'intr', 'prs', 'sg', '1'], 'wfGlossed': 'ե-մ', 'gloss': 'be-PRS.1SG', 'trans_en': 'be'}], [{'wf': 'քեզ', 'lemma': 'դու', 'gramm': ['PRON', 'S', 'hum', 'sg', 'dat'], 'wfGlossed': 'քեզ', 'gloss': 'thou', 'trans_en': 'you, thou'}], [{'wf': ':', 'lemma': '', 'gramm': [], 'wfGlossed': '', 'gloss': ''}]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# analysis with disambiguation\n",
        "analyses = a.analyze_words(['Ես', 'սիրում', 'եմ', 'քեզ'], disambiguate=True)\n",
        "for ana in analyses:\n",
        "    if len(ana) > 1: tab = \"  \"\n",
        "    else: tab = \"\"\n",
        "    for wfo in ana:\n",
        "        print(tab, wfo.wf, wfo.lemma, wfo.gramm, wfo.gloss)"
      ],
      "metadata": {
        "id": "401nl-i84GMn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26991b7b-67b6-4afe-d6be-fc27f54c4c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Ես ես PRON,S,hum,sg,nom me\n",
            "   Ես է V,intr,prs,sg,2 be-PRS.2SG\n",
            " սիրում սիրել V,tr,cvb,ipfv love-CVB.IPFV\n",
            " եմ է V,intr,prs,sg,1 be-PRS.1SG\n",
            " քեզ դու PRON,S,hum,sg,dat thou\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Str = \"Սառը, վճիտ ապրիլյան օր էր, ու ժամացույցը խփում էր տասներեքը։ Չար քամուց թաքնվելու համար կզակը սեղմելով կրծքին՝ Ուինսթոն Սմիթն արագ ներս խցկվեց «Հաղթանակ» բնակելի տան ապակե շքադռնից՝ իր ետևից ներս թողնելով հատիկավոր փոշու մի ամբողջ փոթորիկ։\"\n",
        "\n",
        "StrDe = ' „Es war ein kalter, trostloser Apriltag, und die Uhr schlug dreizehn. Das Kinn an die Brust gedrückt, um sich vor dem bitteren Wind zu schützen, eilte Winston Smith durch die gläserne Veranda des Wohnhauses Victory und hinterließ einen körnigen Sturm Staub.\" '\n",
        "\n",
        "StrEn = ' \"It was a cold, dreary April day, and the clock struck thirteen. Tucking his chin to his chest to shield himself from the bitter wind, Winston Smith hurried through the glass porch of the Victory apartment building, leaving behind him a storm of granular dust.\" '"
      ],
      "metadata": {
        "id": "xByNh1fO4XKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Str = \"Սառը, վճիտ. ապրիլյան օր էր, ու ժամացույցը խփում էր տասներեքը։ Չար քամուց թաքնվելու համար կզակը սեղմելով կրծքին՝ Ուինսթոն Սմիթն արագ ներս խցկվեց «Հաղթանակ» բնակելի տան ապակե շքադռնից՝ իր ետևից ներս թողնելով հատիկավոր փոշու մի ամբողջ փոթորիկ։\""
      ],
      "metadata": {
        "id": "hySmqfbM4KBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Lst = re.split(r'([ ,\\.:;\\!\\(\\)\\\"\\[\\]՞՝«»\\-\\—՝։\\։]+)', Str)\n",
        "LstTok = []\n",
        "for el in Lst:\n",
        "    el = el.strip()\n",
        "    if el != '': LstTok.append(el)\n"
      ],
      "metadata": {
        "id": "LD8NIcoycI9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(LstTok)\n",
        "# does disambiguation work? not yet..."
      ],
      "metadata": {
        "id": "JK8FDH6A4nD4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25349d0b-5e9b-4d2b-fc3c-0b60d28756b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Սառը', ',', 'վճիտ', '.', 'ապրիլյան', 'օր', 'էր', ',', 'ու', 'ժամացույցը', 'խփում', 'էր', 'տասներեքը', '։', 'Չար', 'քամուց', 'թաքնվելու', 'համար', 'կզակը', 'սեղմելով', 'կրծքին', '՝', 'Ուինսթոն', 'Սմիթն', 'արագ', 'ներս', 'խցկվեց', '«', 'Հաղթանակ', '»', 'բնակելի', 'տան', 'ապակե', 'շքադռնից', '՝', 'իր', 'ետևից', 'ներս', 'թողնելով', 'հատիկավոր', 'փոշու', 'մի', 'ամբողջ', 'փոթորիկ', '։']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyses = a.analyze_words(LstTok, disambiguate=True)\n",
        "for ana in analyses:\n",
        "    if len(ana) > 1: tab = \"  \"\n",
        "    else: tab = \"\"\n",
        "    for wfo in ana:\n",
        "        print(tab, wfo.wf, wfo.lemma, wfo.gramm, wfo.gloss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI7ZHTveiMn8",
        "outputId": "6697d391-a99b-43a6-b565-00ea4387f209"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Սառը սառը A,sg,nom,nonposs cold-SG.NOM\n",
            " ,   \n",
            " վճիտ վճիտ A,sg,nom,nonposs transparent_clean_serene\n",
            " .   \n",
            " ապրիլյան ապրիլյան A,sg,nom,nonposs April\n",
            " օր օր N,inanim,sg,nom,nonposs day\n",
            " էր է V,intr,pst,sg,3 be-PST.3SG\n",
            " ,   \n",
            " ու ու CONJ and\n",
            " ժամացույցը ժամացույց N,inanim,sg,nom,def clock-DEF\n",
            " խփում խփել V,tr,cvb,ipfv strike-CVB.IPFV\n",
            " էր է V,intr,pst,sg,3 be-PST.3SG\n",
            " տասներեքը տասներեք NUM,card,sg,nom,def thirteen-DEF\n",
            " ։   \n",
            " Չար չար A,sg,nom,nonposs wicked\n",
            " քամուց քամի N,inanim,sg,abl,nonposs wind-ABL\n",
            "   թաքնվելու թաքնվել V,med,intr,inf,sg,obl,nonposs hide-INF-OBL\n",
            "   թաքնվելու թաքնվել V,med,intr,cvb,dest hide-CVB.DEST\n",
            "   համար համար POST for\n",
            "   համար համար N,inanim,sg,nom,nonposs number\n",
            " կզակը կզակ N,inanim,sg,nom,def jaw-DEF\n",
            " սեղմելով սեղմել V,tr,inf,sg,ins,nonposs press-INF-INS\n",
            " կրծքին կուրծք N,inanim,sg,obl,def breast-OBL-DEF\n",
            " ՝   \n",
            " Ուինսթոն   \n",
            " Սմիթն   \n",
            " արագ արագ ADV fast\n",
            "   ներս ներ N,inanim,sg,nom,poss.1 sister.in.law-1POSS\n",
            "   ներս ներս N,inanim,sg,nom,nonposs inside\n",
            "   խցկվեց խցկվել V,med,intr,aor,sg,3 squeeze.in-AOR.3SG\n",
            "   խցկվեց խցկել V,tr,pass,aor,sg,3 push.in-PASS-AOR.3SG\n",
            " «   \n",
            " Հաղթանակ հաղթանակ N,inanim,sg,nom,nonposs victory\n",
            " »   \n",
            " բնակելի բնակելի A,sg,nom,nonposs dwelling\n",
            "   տան տալ V,tr,sbjv,prs,pl,3 give-SBJV.PRS.3PL\n",
            "   տան տուն N,inanim,sg,obl,nonposs house\n",
            " ապակե ապակե A,sg,nom,nonposs glass\n",
            " շքադռնից   \n",
            " ՝   \n",
            " իր իր N,inanim,sg,nom,nonposs thing\n",
            "   ետևից ետեւ N,inanim,sg,abl,nonposs back_behind-ABL\n",
            "   ետևից ետեւից ADV from.behind\n",
            "   ներս ներ N,inanim,sg,nom,poss.1 sister.in.law-1POSS\n",
            "   ներս ներս N,inanim,sg,nom,nonposs inside\n",
            " թողնելով թողնել V,tr,inf,sg,ins,nonposs leave-INF-INS\n",
            " հատիկավոր հատիկավոր A,sg,nom,nonposs grainy\n",
            " փոշու փոշի N,inanim,sg,obl,nonposs dust-OBL\n",
            "   մի մի NUM,card one\n",
            "   մի մի PART PROH\n",
            " ամբողջ ամբողջ PRON,A,sg,nom,nonposs all\n",
            " փոթորիկ փոթորիկ N,inanim,sg,nom,nonposs storm\n",
            " ։   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File annotation\n",
        "\n",
        "### defining functions to process the file"
      ],
      "metadata": {
        "id": "aeazgS7ShLAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizeHy(Str2tokenise, rePattern = r'([ ,\\.:;\\!\\(\\)\\\"\\[\\]՞՝«»\\-\\—՝։\\։]+)'):\n",
        "    LTokens = []\n",
        "    Lst = re.split(rePattern, Str2tokenise)\n",
        "    # LstTok = []\n",
        "    for el in Lst:\n",
        "        el = el.strip()\n",
        "        if el != '': LTokens.append(el)\n",
        "    return LTokens"
      ],
      "metadata": {
        "id": "_FMKB1I7v2ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vahram's model for English\n",
        "!wget https://heibox.uni-heidelberg.de/f/38de0eb8b2ec41d284d7/?dl=1\n",
        "!mv index.html?dl=1 WIKI_EN.model\n"
      ],
      "metadata": {
        "id": "Uy8itpV9ka1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F16HFn3vl20e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec # The word2vec model class\n",
        "import gensim.downloader as api # Allows us to download some free training data"
      ],
      "metadata": {
        "id": "E8DhZznElVRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_WIKI_EN = Word2Vec.load(\"/content/WIKI_EN.model\")\n",
        "word_vectors_WIKI_EN = model_WIKI_EN.wv"
      ],
      "metadata": {
        "id": "SujtHRcJlM0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distance = word_vectors_WIKI_EN.similarity('obama', 'barak')\n",
        "distance2 = word_vectors_WIKI_EN.similarity('obama', 'zone')\n"
      ],
      "metadata": {
        "id": "s4XFkcwDmIRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('distance = %.4f' % distance)\n",
        "print('distance2 = %.4f' % distance2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7liGqcyXniE1",
        "outputId": "a041ff67-e172-45c5-9ecc-06e26a30ae11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distance = 0.7685\n",
            "distance2 = 0.3015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# merging together different lines of code\n",
        "\n",
        "def disambiguate(LTok2disambiguate, Window = 4, show_variants = False):\n",
        "    SDisambig = ''\n",
        "\n",
        "    analysesN = a.analyze_words(LTok2disambiguate, disambiguate=False)\n",
        "\n",
        "    # preparing data structures\n",
        "    LLContext = [] # empty list of contexts, indices are the same as with the Text list\n",
        "    LLText = [] # test to disambiguate, ambiguous interpretations are double entries\n",
        "    for ana in analysesN:\n",
        "        # creating context window from glosses\n",
        "        # if len(ana) > 1: tab = \"~\"\n",
        "        # else: tab = \"!\"\n",
        "        LwfoContext = []\n",
        "        LwfoText = []\n",
        "        for wfo in ana: # preserve lemmas / word forms which are not found in dictionary\n",
        "            if wfo.gramm == '': wfo.gramm = 'N'\n",
        "            if wfo.lemma == '': wfo.lemma = wfo.wf\n",
        "            if wfo.gloss == '': wfo.gloss = '[unknown]'\n",
        "\n",
        "            SWfo = f'{wfo.wf}\\t{wfo.gramm}\\t{wfo.lemma}\\t{wfo.gloss}'\n",
        "            # print(SWfo)\n",
        "            # FNoD.write(SWfo + '\\n')\n",
        "\n",
        "            # find the first part of the gloss, which may be in the word vectors model\n",
        "            REPart = re.match('([A-Za-z]+)', wfo.gloss)\n",
        "            if REPart:\n",
        "                SGlossMin = REPart.group(1)\n",
        "                SGlossMin = SGlossMin.lower()\n",
        "                # print(SGlossMin)\n",
        "            else:\n",
        "                SGlossMin = '[NONE]'\n",
        "            LwfoContext.append(SGlossMin)\n",
        "            LwfoText.append(SWfo)\n",
        "        LLContext.append(LwfoContext)\n",
        "        LLText.append(LwfoText)\n",
        "\n",
        "    # for el in LLContext: print(el)\n",
        "    # for el in LLText: print(el)\n",
        "\n",
        "    # print(len(LLContext))\n",
        "    # print(len(LLText))\n",
        "\n",
        "    sys.stderr.write('.')\n",
        "\n",
        "\n",
        "    for i in range(len(LLText)):\n",
        "        if len(LLText[i]) > 1:\n",
        "            # print(LLText[i])\n",
        "            # print(LLContext[i])\n",
        "            # collect context window +- 3 words\n",
        "            iwStart = i-Window\n",
        "            if iwStart <0: iwStart=0\n",
        "            iwEnd = i+Window\n",
        "            if iwEnd > len(LLText): iwEnd = len(LLText)\n",
        "            # iwLen = iwEnd - iwStart\n",
        "            winContext = LLContext[iwStart:iwEnd]\n",
        "            # print(winContext)\n",
        "            LScores = []\n",
        "            LScCand = []\n",
        "\n",
        "            for candidate in LLContext[i]:\n",
        "                ScoreCand = 0\n",
        "                for LCtx in winContext:\n",
        "                    for Ctx in LCtx:\n",
        "                        try: distance = word_vectors_WIKI_EN.similarity(candidate, Ctx)\n",
        "                        except: distance = 0\n",
        "                        ScoreCand += distance\n",
        "                LScores.append((candidate,ScoreCand))\n",
        "                LScCand.append(ScoreCand)\n",
        "            LScores.sort(key=lambda a: a[1], reverse=True)\n",
        "            # print(LScores)\n",
        "\n",
        "            max_value = max(LScCand)\n",
        "            #  Return the max value of the list\n",
        "            max_index = LScCand.index(max_value)\n",
        "            StoWrite = LLText[i][max_index] + '\\n'\n",
        "            SDisambig += StoWrite\n",
        "\n",
        "            if show_variants == True:\n",
        "                for el in LLText[i]:\n",
        "                    StoWrite = '\\t~\\t' + el + '\\n'\n",
        "                    SDisambig += StoWrite\n",
        "                for el in LScores:\n",
        "                    StoWrite = '\\t~sc:\\t' + str(el) + '\\n'\n",
        "                    SDisambig += StoWrite\n",
        "        else:\n",
        "            StoWrite = LLText[i][0] + '\\n'\n",
        "            SDisambig += StoWrite\n",
        "\n",
        "    return SDisambig"
      ],
      "metadata": {
        "id": "TpxYmxdTwGyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### give names for the input and output file"
      ],
      "metadata": {
        "id": "Kb_7hsYchZZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# FInText = open('/content/udhr/udhr_hye_v03.txt','r')\n",
        "# FOutText = open('/content/udhrTT/udhr_hye_vert.txt','w')\n",
        "\n",
        "\n",
        "FInText = open('udhr_hye_v03.txt','r')\n",
        "FOutText = open('udhr_hye_v03.vert','w')"
      ],
      "metadata": {
        "id": "YMgnQ_crwuQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for SLine in FInText:\n",
        "    SLine = SLine.strip()\n",
        "    LTok = tokenizeHy(SLine)\n",
        "    SDisambig = disambiguate(LTok, show_variants = True)\n",
        "    # FOutText.write('<p>\\n')\n",
        "    # FOutText.write('\\n')\n",
        "    FOutText.write(SDisambig)\n",
        "    # FOutText.write('</p>\\n')\n",
        "    FOutText.write('\\n')\n",
        "\n",
        "FOutText.flush()"
      ],
      "metadata": {
        "id": "61sqTRpmxOFF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d632168-a776-4d7f-d291-f0eaad76c498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "......................................................................................................................................................................................................................."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### optional: Creating a text file with lemmas\n",
        "\n",
        "all word forms are converted into lemmas (might be used for training word2vec models)"
      ],
      "metadata": {
        "id": "c9GN7kuzhfBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !awk -F '\\t' '(NF==4){printf \"%s \", $3}(NF!=4){printf \"\\n\"}' < /content/udhrTT/udhr_hye_vert.txt >/content/udhrTT/udhr_hye_lem.txt\n",
        "!awk -F '\\t' '(NF==4){printf \"%s \", $3}(NF!=4){printf \"\\n\"}' < arm.vert >arm_lem.txt"
      ],
      "metadata": {
        "id": "ScstreEf1RDn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
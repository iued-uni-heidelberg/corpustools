{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWqMwBwP9eb9D4XkyDXjhL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/corpustools/blob/main/S101lemHYstanzaWithDemoV202509.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Armenian lemmatization with Stanza"
      ],
      "metadata": {
        "id": "skix7t6sFaZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## downloading evaluation sets\n",
        "- 420 words: test with about 420 words of Armenian text\n",
        "- Armenian \"Brown-type\" corpus b"
      ],
      "metadata": {
        "id": "fFBRX6lTFfcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download some sample Armenian sentences, or upload your own, change the\n",
        "!wget https://heibox.uni-heidelberg.de/f/ce6096da570f47b99500/?dl=1\n",
        "!mv index.html?dl=1 evaluation-set-v01.txt\n",
        "!wget https://heibox.uni-heidelberg.de/f/a847a12bffd4491f9070/?dl=1\n",
        "!mv index.html?dl=1 TED2020-dehy-hy-aa.txt"
      ],
      "metadata": {
        "id": "m549clSLFHs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing stanza"
      ],
      "metadata": {
        "id": "i__aUXulFkw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation:\n",
        "\n",
        "It is important to downgrade to Torch 2.5, because Stanza doesn't work with later versions. See the ChatGPT explanation at: https://chatgpt.com/share/68bad9f0-fc38-800e-a17d-898e6fef0e35\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vbn97IPV49NF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdzVArLUF3cb"
      },
      "outputs": [],
      "source": [
        "# it is important to downgrade to Torch 2.5, because Stanza doesn't work with later versions. See the ChatGPT explanation at https://chatgpt.com/share/68bad9f0-fc38-800e-a17d-898e6fef0e35\n",
        "!pip install spacy-stanza\n",
        "!pip install torch==2.5.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "import spacy_stanza\n",
        "import torch\n",
        "print(\"Torch version:\", torch.__version__)"
      ],
      "metadata": {
        "id": "-VN9g4N4GAR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### testing English stanza (optional)"
      ],
      "metadata": {
        "id": "w6Kfwj63FzYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optional\n",
        "# Download the stanza model if necessary\n",
        "stanza.download(\"en\")\n",
        "\n",
        "# Initialize the pipeline\n",
        "nlp = spacy_stanza.load_pipeline(\"en\")\n",
        "\n",
        "doc = nlp(\"Barack Obama was born in Hawaii. He was elected president in 2008.\")\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.dep_, token.ent_type_)\n",
        "print(doc.ents)"
      ],
      "metadata": {
        "id": "dEA7KJdZPrWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading and testing Armenian stanza"
      ],
      "metadata": {
        "id": "HGyKxEJNGG8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stanza.download(\"hy\")\n",
        "# Load Armenian pipeline (make sure stanza models are downloaded first)\n",
        "nlp_hy = spacy_stanza.load_pipeline(\"hy\")"
      ],
      "metadata": {
        "id": "xq53mDsUGumV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Armenian text\n",
        "text = \"Ես գնում եմ դպրոց։\"\n",
        "doc = nlp_hy(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(f\"\\n=== Token: {token.text} ===\")\n",
        "\n",
        "    print(f\"\\n=== Index: {token.idx} ===\")\n",
        "\n",
        "    # Core text info\n",
        "    print(\"Text:        \", token.text)\n",
        "    print(\"Lemma:       \", token.lemma_)\n",
        "    print(\"POS (UPOS):  \", token.pos_)\n",
        "    print(\"Tag (XPOS):  \", token.tag_)\n",
        "    print(\"Morph:       \", token.morph)\n",
        "    print(\"Dep:         \", token.dep_)\n",
        "    print(\"Head:        \", token.head.text)\n",
        "\n",
        "    # Entity info\n",
        "    print(\"Ent type:    \", token.ent_type_)\n",
        "    print(\"Ent IOB:     \", token.ent_iob_)\n",
        "\n",
        "    # Document & sentence info\n",
        "    print(\"Is sent start:\", token.is_sent_start)\n",
        "    print(\"Sentence:    \", token.sent.text)\n",
        "\n",
        "    # Orthographic info\n",
        "    print(\"Lower:       \", token.lower_)\n",
        "    print(\"Shape:       \", token.shape_)\n",
        "    print(\"Whitespace:  \", repr(token.whitespace_))\n",
        "\n",
        "    # Booleans\n",
        "    print(\"is_alpha:    \", token.is_alpha)\n",
        "    print(\"is_digit:    \", token.is_digit)\n",
        "    print(\"is_punct:    \", token.is_punct)\n",
        "    print(\"is_space:    \", token.is_space)\n",
        "    print(\"is_stop:     \", token.is_stop)\n",
        "\n",
        "    # Position info\n",
        "    print(\"Index:       \", token.i)\n",
        "    print(\"Char offset: \", token.idx)\n"
      ],
      "metadata": {
        "id": "ZmT1a-cK8W5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### optional\n",
        "doc = nlp_hy(\"ՄԱՐԴՈՒ ԻՐԱՎՈՒՆՔՆԵՐԻ ՀԱՄԸՆԴՀԱՆՈՒՐ ՀՌՉԱԿԱԳԻՐ. ՆԵՐԱԾԱԿԱՆ. Քանզի մարդկային ընտանիքի բոլոր անդամներին ներհատուկ արժանապատվությունըև հավասար ու անօտարելի իրավունքները աշխարհի ազատության, արդարության ու խաղաղության հիմքն են.\")"
      ],
      "metadata": {
        "id": "m022wp2JHvOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### optional\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.dep_, token.ent_type_)\n"
      ],
      "metadata": {
        "id": "LPhSOX15ICmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### full analysis of the file (optional)\n",
        "- includes dependency parsing"
      ],
      "metadata": {
        "id": "NVyvpuMDQXYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### optional\n",
        "with open('/content/TED2020-dehy-hy-aa.txt', 'r', encoding='utf-8') as infile, open('/content/TED2020-dehy-hy-aa-ANALYSIS-full-v01.txt', 'w') as outfile:\n",
        "    # read sample.txt an and write its content into sample2.txt\n",
        "    outfile.write(\"{token.text}\\t{token.lemma_}\\t{token.pos_}\\t{token.dep_}\\t{parentLem}\\t{LAncestors}\\n\")\n",
        "    for line in infile:\n",
        "        line = line.strip()\n",
        "        doc = nlp_hy(line)\n",
        "        # outfile.write(line + '\\n')\n",
        "        for token in doc:\n",
        "            LAncestors = list(token.ancestors)\n",
        "            print(str(LAncestors))\n",
        "            try:\n",
        "                SLAncestors = str(list(token.ancestors))\n",
        "                parent = LAncestors[0]\n",
        "                parentLem = parent.lemma_\n",
        "            except:\n",
        "                parentLem = \"NONE\"\n",
        "            outfile.write(f\"{token.text}\\t{token.lemma_}\\t{token.pos_}\\t{token.dep_}\\t{parentLem}\\t{SLAncestors}\\n\")\n"
      ],
      "metadata": {
        "id": "HJdW66EmJI2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### function for lemmatization"
      ],
      "metadata": {
        "id": "_w7MrFvNQqxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parseFile(iFileName, oFileName, nlp_model = nlp_hy):\n",
        "    with open(iFileName, 'r', encoding='utf-8') as infile, open(oFileName, 'w') as outfile:\n",
        "        # read sample.txt an and write its content into sample2.txt\n",
        "        outfile.write(\"{token.text}\\t{token.pos_}\\t{token.lemma_}\\n\")\n",
        "        c = 0\n",
        "        for line in infile:\n",
        "            c+=1\n",
        "            if c%10 == 0: print(str(c))\n",
        "            line = line.strip()\n",
        "            doc = nlp_model(line)\n",
        "            # outfile.write(line + '\\n')\n",
        "            for token in doc:\n",
        "                LAncestors = list(token.ancestors)\n",
        "                # print(str(LAncestors))\n",
        "                try:\n",
        "                    SLAncestors = str(list(token.ancestors))\n",
        "                    parent = LAncestors[0]\n",
        "                    parentLem = parent.lemma_\n",
        "                except:\n",
        "                    parentLem = \"NONE\"\n",
        "                outfile.write(f\"{token.text}\\t{token.pos_}\\t{token.lemma_}\\n\")\n",
        "\n",
        "    return\n"
      ],
      "metadata": {
        "id": "QT0tpHwjY4O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### command to lemmatize the file"
      ],
      "metadata": {
        "id": "FJoxNaZ7vfVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parseFile('/content/TED2020-dehy-hy-aa', '/content/TED2020-dehy-hy-aa--lemmatization-v01.txt', nlp_hy)"
      ],
      "metadata": {
        "id": "rgdb1fl3a6F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parseFile('evaluation-set-v01.txt', 'evaluation-set-v01.vert', nlp_hy)"
      ],
      "metadata": {
        "id": "lrSvU_NbsZ9z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
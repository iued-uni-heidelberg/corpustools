{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6ws65EQAagaJivKH5IUSA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/corpustools/blob/main/S101lemHYstanza.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Armenian lemmatization with Stanza"
      ],
      "metadata": {
        "id": "skix7t6sFaZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## downloading evaluation sets\n",
        "- 420 words: test with about 420 words of Armenian text\n",
        "- Armenian \"Brown-type\" corpus b"
      ],
      "metadata": {
        "id": "fFBRX6lTFfcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### optional\n",
        "!wget https://heibox.uni-heidelberg.de/f/ce6096da570f47b99500/?dl=1"
      ],
      "metadata": {
        "id": "m549clSLFHs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### optional\n",
        "!mv index.html?dl=1 evaluation-set-v01.txt"
      ],
      "metadata": {
        "id": "eI_j8KDdFRCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/a847a12bffd4491f9070/?dl=1\n"
      ],
      "metadata": {
        "id": "IWwqNLkxPMfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv index.html?dl=1 TED2020-dehy-hy-aa"
      ],
      "metadata": {
        "id": "0QkNxyeRPb7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### downloading Armenian Wikipedia\n",
        "!wget https://heibox.uni-heidelberg.de/f/d1f866a61bd545318213/?dl=1\n",
        "!mv index.html?dl=1 hywiki-20221101-pages-articles.txt.gz\n",
        "!gunzip hywiki-20221101-pages-articles.txt.gz"
      ],
      "metadata": {
        "id": "j_-b-YYGq8bw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wc hywiki-20221101-pages-articles.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQMJimofsTMd",
        "outputId": "b72457fb-481d-43e0-ce2e-e316e72ec3b8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2446411  56341167 803098410 hywiki-20221101-pages-articles.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing stanza"
      ],
      "metadata": {
        "id": "i__aUXulFkw2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdzVArLUF3cb"
      },
      "outputs": [],
      "source": [
        "!pip install spacy-stanza"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "import spacy_stanza\n"
      ],
      "metadata": {
        "id": "-VN9g4N4GAR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### testing English stanza (optional)"
      ],
      "metadata": {
        "id": "w6Kfwj63FzYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optional\n",
        "# Download the stanza model if necessary\n",
        "stanza.download(\"en\")\n",
        "\n",
        "# Initialize the pipeline\n",
        "nlp = spacy_stanza.load_pipeline(\"en\")\n",
        "\n",
        "doc = nlp(\"Barack Obama was born in Hawaii. He was elected president in 2008.\")\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.dep_, token.ent_type_)\n",
        "print(doc.ents)"
      ],
      "metadata": {
        "id": "dEA7KJdZPrWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### downloading and testing Armenian stanza"
      ],
      "metadata": {
        "id": "HGyKxEJNGG8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stanza.download(\"hy\")\n"
      ],
      "metadata": {
        "id": "xq53mDsUGumV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_hy = spacy_stanza.load_pipeline(\"hy\")"
      ],
      "metadata": {
        "id": "KZKOs0aVG8Kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### optional\n",
        "doc = nlp_hy(\"ՄԱՐԴՈՒ ԻՐԱՎՈՒՆՔՆԵՐԻ ՀԱՄԸՆԴՀԱՆՈՒՐ ՀՌՉԱԿԱԳԻՐ. ՆԵՐԱԾԱԿԱՆ. Քանզի մարդկային ընտանիքի բոլոր անդամներին ներհատուկ արժանապատվությունըև հավասար ու անօտարելի իրավունքները աշխարհի ազատության, արդարության ու խաղաղության հիմքն են.\")"
      ],
      "metadata": {
        "id": "m022wp2JHvOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### optional\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.dep_, token.ent_type_)\n"
      ],
      "metadata": {
        "id": "LPhSOX15ICmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### full analysis of the file (optional)\n",
        "- includes dependency parsing"
      ],
      "metadata": {
        "id": "NVyvpuMDQXYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### optional\n",
        "with open('/content/TED2020-dehy-hy-aa', 'r', encoding='utf-8') as infile, open('/content/TED2020-dehy-hy-aa-ANALYSIS-full-v01.txt', 'w') as outfile:\n",
        "    # read sample.txt an and write its content into sample2.txt\n",
        "    outfile.write(\"{token.text}\\t{token.lemma_}\\t{token.pos_}\\t{token.dep_}\\t{parentLem}\\t{LAncestors}\\n\")\n",
        "    for line in infile:\n",
        "        line = line.strip()\n",
        "        doc = nlp_hy(line)\n",
        "        # outfile.write(line + '\\n')\n",
        "        for token in doc:\n",
        "            LAncestors = list(token.ancestors)\n",
        "            print(str(LAncestors))\n",
        "            try:\n",
        "                SLAncestors = str(list(token.ancestors))\n",
        "                parent = LAncestors[0]\n",
        "                parentLem = parent.lemma_\n",
        "            except:\n",
        "                parentLem = \"NONE\"\n",
        "            outfile.write(f\"{token.text}\\t{token.lemma_}\\t{token.pos_}\\t{token.dep_}\\t{parentLem}\\t{SLAncestors}\\n\")\n",
        " "
      ],
      "metadata": {
        "id": "HJdW66EmJI2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### function for lemmatization"
      ],
      "metadata": {
        "id": "_w7MrFvNQqxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parseFile(iFileName, oFileName, nlp_model = nlp_hy):\n",
        "    with open(iFileName, 'r', encoding='utf-8') as infile, open(oFileName, 'w') as outfile:\n",
        "        # read sample.txt an and write its content into sample2.txt\n",
        "        outfile.write(\"{token.text}\\t{token.pos_}\\t{token.lemma_}\\n\")\n",
        "        c = 0\n",
        "        for line in infile:\n",
        "            c+=1\n",
        "            if c%10 == 0: print(str(c))\n",
        "            line = line.strip()\n",
        "            doc = nlp_model(line)\n",
        "            # outfile.write(line + '\\n')\n",
        "            for token in doc:\n",
        "                LAncestors = list(token.ancestors)\n",
        "                # print(str(LAncestors))\n",
        "                try:\n",
        "                    SLAncestors = str(list(token.ancestors))\n",
        "                    parent = LAncestors[0]\n",
        "                    parentLem = parent.lemma_\n",
        "                except:\n",
        "                    parentLem = \"NONE\"\n",
        "                outfile.write(f\"{token.text}\\t{token.pos_}\\t{token.lemma_}\\n\")\n",
        " \n",
        "    return\n"
      ],
      "metadata": {
        "id": "QT0tpHwjY4O5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### command to lemmatize the file"
      ],
      "metadata": {
        "id": "FJoxNaZ7vfVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parseFile('/content/TED2020-dehy-hy-aa', '/content/TED2020-dehy-hy-aa--lemmatization-v01.txt', nlp_hy)"
      ],
      "metadata": {
        "id": "rgdb1fl3a6F4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf97ebf0-cf21-47d7-a370-efd54489e93a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parseFile('hywiki-20221101-pages-articles.txt', 'hywiki-20221101-pages-articles.vert', nlp_hy)"
      ],
      "metadata": {
        "id": "lrSvU_NbsZ9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking OCR errors\n",
        "### wikipedia lemmatized --> frequency dictionary "
      ],
      "metadata": {
        "id": "Disi8bxMhOD1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lhs5GwRihMxr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
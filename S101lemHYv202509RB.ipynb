{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/corpustools/blob/main/S101lemHYv202509RB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing pipeline for DAAD project\n",
        "\n",
        "## Stage 1: Lemmatization (Armenian)\n",
        "Example: Universal declaration of human rights\n",
        "\n",
        "the link aboout pdf2txt conversion: https://chatgpt.com/share/68bbeed8-0d08-800e-8418-b0816c6393d4\n",
        "\n"
      ],
      "metadata": {
        "id": "j2SZAuuE1441"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading UDHR\n",
        "# !wget https://unicode.org/udhr/assemblies/udhr_txt.zip\n",
        "# downloading from the UN website, converting pdf to txt\n",
        "# the link aboout pdf2txt conversion: https://chatgpt.com/share/68bbeed8-0d08-800e-8418-b0816c6393d4\n",
        "\n",
        "!wget https://www.ohchr.org/sites/default/files/UDHR/Documents/UDHR_Translations/arm.pdf\n"
      ],
      "metadata": {
        "id": "QUnIvH5a136j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "from pypdf import PdfReader"
      ],
      "metadata": {
        "id": "YZZAzlDlR3dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pyPDF2TXT(SFInput, SFOutput):\n",
        "\n",
        "    reader = PdfReader(SFInput)\n",
        "    text = \"\"\n",
        "\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "\n",
        "    with open(SFOutput, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "\n",
        "    print(\"PDF successfully converted to TXT!\")\n",
        "\n",
        "pyPDF2TXT(\"arm.pdf\", \"arm.txt\")"
      ],
      "metadata": {
        "id": "iW-c_zB4R7-p"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir udhr\n",
        "mv udhr_txt.zip udhr\n",
        "cd udhr/\n",
        "unzip udhr_txt.zip"
      ],
      "metadata": {
        "id": "o4UYZhui2HJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# delete lines which are not translations in some files (hy)\n",
        "# delete between lines $a and $b inclusive\n",
        "a=9\n",
        "b=21\n",
        "awk -v m=$a -v n=$b 'm <= NR && NR <= n {next} {print}' < udhr/udhr_hye.txt >udhr/udhr_hye_v03.txt\n",
        "# put paragraph tags\n",
        "# awk '{print \"<p>\\n\"$0 ; print \"</p>\"}' udhr/udhr_hye2.txt >udhr/udhr_hye_v03.txt"
      ],
      "metadata": {
        "id": "gWOntGq42JvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing python libraries\n",
        "import os, re, sys"
      ],
      "metadata": {
        "id": "I3-okSYJ2NI9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# installing Armenian morphological analyser\n",
        "!git clone https://github.com/timarkh/uniparser-grammar-eastern-armenian\n",
        "# Python classes\n",
        "!pip3 install uniparser-eastern-armenian\n",
        "# disambiguation\n",
        "!sudo apt-get install cg3"
      ],
      "metadata": {
        "id": "jF1Sa8dm3WFG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c4be6ae-bc1e-4789-93ed-2f4577e48ce5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'uniparser-grammar-eastern-armenian'...\n",
            "remote: Enumerating objects: 181, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 181 (delta 12), reused 40 (delta 12), pack-reused 141 (from 1)\u001b[K\n",
            "Receiving objects: 100% (181/181), 52.66 MiB | 14.39 MiB/s, done.\n",
            "Resolving deltas: 100% (78/78), done.\n",
            "Collecting uniparser-eastern-armenian\n",
            "  Downloading uniparser_eastern_armenian-2.1.2-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting uniparser-morph>=2.2.0 (from uniparser-eastern-armenian)\n",
            "  Downloading uniparser_morph-2.9.4-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from uniparser-eastern-armenian) (6.5.2)\n",
            "Collecting textdistance>=4.0.0 (from uniparser-morph>=2.2.0->uniparser-eastern-armenian)\n",
            "  Downloading textdistance-4.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Downloading uniparser_eastern_armenian-2.1.2-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uniparser_morph-2.9.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.8/59.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading textdistance-4.6.3-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: textdistance, uniparser-morph, uniparser-eastern-armenian\n",
            "Successfully installed textdistance-4.6.3 uniparser-eastern-armenian-2.1.2 uniparser-morph-2.9.4\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libcg3-1\n",
            "The following NEW packages will be installed:\n",
            "  cg3 libcg3-1\n",
            "0 upgraded, 2 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 423 kB of archives.\n",
            "After this operation, 1,278 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcg3-1 amd64 1.3.2-1build2 [305 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 cg3 amd64 1.3.2-1build2 [118 kB]\n",
            "Fetched 423 kB in 1s (282 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libcg3-1:amd64.\n",
            "(Reading database ... 126374 files and directories currently installed.)\n",
            "Preparing to unpack .../libcg3-1_1.3.2-1build2_amd64.deb ...\n",
            "Unpacking libcg3-1:amd64 (1.3.2-1build2) ...\n",
            "Selecting previously unselected package cg3.\n",
            "Preparing to unpack .../cg3_1.3.2-1build2_amd64.deb ...\n",
            "Unpacking cg3 (1.3.2-1build2) ...\n",
            "Setting up libcg3-1:amd64 (1.3.2-1build2) ...\n",
            "Setting up cg3 (1.3.2-1build2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from uniparser_eastern_armenian import EasternArmenianAnalyzer\n",
        "a = EasternArmenianAnalyzer()\n",
        "analyses = a.analyze_words('Ձևաբանություն')\n",
        "for ana in analyses:\n",
        "    print(ana.wf, ana.lemma, ana.gramm, ana.gloss, ana.stem, ana.subwords, ana.wfGlossed, ana.otherData)"
      ],
      "metadata": {
        "id": "dRFuA6sN3Z62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a62702d0-10bc-44de-a0fb-4173e40e3cc4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ձևաբանություն ձեւաբանություն N,inanim,sg,nom,nonposs morphology ձևաբանություն. [] ձևաբանություն [('trans_en', 'morphology')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trying out:\n",
        "# nonexisting word\n",
        "analyses2 = a.analyze_words('Ձևաբայու')\n",
        "for ana2 in analyses2:\n",
        "    if ana2.lemma:\n",
        "      print(ana2.wf, ana2.lemma, ana2.gramm, ana2.gloss, ana2.stem, ana2.subwords, ana2.wfGlossed, ana2.otherData)\n",
        "    else:\n",
        "      print(ana2.wf, ana2.wf, \"N\", \"x\", ana2.stem, ana2.subwords, ana2.wfGlossed, ana2.otherData)"
      ],
      "metadata": {
        "id": "jMu1kHN03mxE",
        "outputId": "8958d556-71c9-48cb-e30a-26a838489b81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ձևաբայու Ձևաբայու N x  []  []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyses = a.analyze_words([['և'], ['Ես', 'սիրում', 'եմ', 'քեզ', ':']],\n",
        "                           format='xml')\n",
        "for ana in analyses:\n",
        "    print(str(ana))"
      ],
      "metadata": {
        "id": "5b0wetvb3zvF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e01bbcae-e083-4fc2-9bc1-11c49ad9e9b4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<w><ana lex=\"եւ\" gr=\"CONJ\" parts=\"և\" gloss=\"and\" trans_en=\"and, too, either\"></ana>և</w>']\n",
            "['<w><ana lex=\"ես\" gr=\"PRON,S,hum,sg,nom\" parts=\"ես\" gloss=\"me\" trans_en=\"I\"></ana><ana lex=\"է\" gr=\"V,intr,prs,sg,2\" parts=\"ե-ս\" gloss=\"be-PRS.2SG\" trans_en=\"be\"></ana>Ես</w>', '<w><ana lex=\"սիրել\" gr=\"V,tr,cvb,ipfv\" parts=\"սիր-ում\" gloss=\"love-CVB.IPFV\" trans_en=\"love, have a passion/an affection for, like\"></ana>սիրում</w>', '<w><ana lex=\"է\" gr=\"V,intr,prs,sg,1\" parts=\"ե-մ\" gloss=\"be-PRS.1SG\" trans_en=\"be\"></ana>եմ</w>', '<w><ana lex=\"դու\" gr=\"PRON,S,hum,sg,dat\" parts=\"քեզ\" gloss=\"thou\" trans_en=\"you, thou\"></ana>քեզ</w>', '<w><ana lex=\"\" gr=\"\" parts=\"\" gloss=\"\"></ana>:</w>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyses = a.analyze_words(['Ձևաբանություն', [['և'], ['Ես', 'սիրում', 'եմ', 'քեզ', ':']]],\n",
        "                           format='json')\n",
        "for ana in analyses:\n",
        "    print(str(ana))"
      ],
      "metadata": {
        "id": "R735YSlN3_rb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "373c358c-4e69-4143-89eb-debd3908700d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'wf': 'Ձևաբանություն', 'lemma': 'ձեւաբանություն', 'gramm': ['N', 'inanim', 'sg', 'nom', 'nonposs'], 'wfGlossed': 'ձևաբանություն', 'gloss': 'morphology', 'trans_en': 'morphology'}]\n",
            "[[[{'wf': 'և', 'lemma': 'եւ', 'gramm': ['CONJ'], 'wfGlossed': 'և', 'gloss': 'and', 'trans_en': 'and, too, either'}]], [[{'wf': 'Ես', 'lemma': 'ես', 'gramm': ['PRON', 'S', 'hum', 'sg', 'nom'], 'wfGlossed': 'ես', 'gloss': 'me', 'trans_en': 'I'}, {'wf': 'Ես', 'lemma': 'է', 'gramm': ['V', 'intr', 'prs', 'sg', '2'], 'wfGlossed': 'ե-ս', 'gloss': 'be-PRS.2SG', 'trans_en': 'be'}], [{'wf': 'սիրում', 'lemma': 'սիրել', 'gramm': ['V', 'tr', 'cvb', 'ipfv'], 'wfGlossed': 'սիր-ում', 'gloss': 'love-CVB.IPFV', 'trans_en': 'love, have a passion/an affection for, like'}], [{'wf': 'եմ', 'lemma': 'է', 'gramm': ['V', 'intr', 'prs', 'sg', '1'], 'wfGlossed': 'ե-մ', 'gloss': 'be-PRS.1SG', 'trans_en': 'be'}], [{'wf': 'քեզ', 'lemma': 'դու', 'gramm': ['PRON', 'S', 'hum', 'sg', 'dat'], 'wfGlossed': 'քեզ', 'gloss': 'thou', 'trans_en': 'you, thou'}], [{'wf': ':', 'lemma': '', 'gramm': [], 'wfGlossed': '', 'gloss': ''}]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# analysis with disambiguation\n",
        "analyses = a.analyze_words(['Ես', 'սիրում', 'եմ', 'քեզ'], disambiguate=True)\n",
        "for ana in analyses:\n",
        "    if len(ana) > 1: tab = \"  \"\n",
        "    else: tab = \"\"\n",
        "    for wfo in ana:\n",
        "        print(tab, wfo.wf, wfo.lemma, wfo.gramm, wfo.gloss)"
      ],
      "metadata": {
        "id": "401nl-i84GMn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a11e1ece-b136-4b57-d459-6953745d9c9c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Ես ես PRON,S,hum,sg,nom me\n",
            "   Ես է V,intr,prs,sg,2 be-PRS.2SG\n",
            " սիրում սիրել V,tr,cvb,ipfv love-CVB.IPFV\n",
            " եմ է V,intr,prs,sg,1 be-PRS.1SG\n",
            " քեզ դու PRON,S,hum,sg,dat thou\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Str = \"Սառը, վճիտ ապրիլյան օր էր, ու ժամացույցը խփում էր տասներեքը։ Չար քամուց թաքնվելու համար կզակը սեղմելով կրծքին՝ Ուինսթոն Սմիթն արագ ներս խցկվեց «Հաղթանակ» բնակելի տան ապակե շքադռնից՝ իր ետևից ներս թողնելով հատիկավոր փոշու մի ամբողջ փոթորիկ։\"\n",
        "\n",
        "StrDe = ' „Es war ein kalter, trostloser Apriltag, und die Uhr schlug dreizehn. Das Kinn an die Brust gedrückt, um sich vor dem bitteren Wind zu schützen, eilte Winston Smith durch die gläserne Veranda des Wohnhauses Victory und hinterließ einen körnigen Sturm Staub.\" '\n",
        "\n",
        "StrEn = ' \"It was a cold, dreary April day, and the clock struck thirteen. Tucking his chin to his chest to shield himself from the bitter wind, Winston Smith hurried through the glass porch of the Victory apartment building, leaving behind him a storm of granular dust.\" '"
      ],
      "metadata": {
        "id": "xByNh1fO4XKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Str = \"Սառը, վճիտ. ապրիլյան օր էր, ու ժամացույցը խփում էր տասներեքը։ Չար քամուց թաքնվելու համար կզակը սեղմելով կրծքին՝ Ուինսթոն Սմիթն արագ ներս խցկվեց «Հաղթանակ» բնակելի տան ապակե շքադռնից՝ իր ետևից ներս թողնելով հատիկավոր փոշու մի ամբողջ փոթորիկ։\""
      ],
      "metadata": {
        "id": "hySmqfbM4KBQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Lst = re.split(r'([ ,\\.:;\\!\\(\\)\\\"\\[\\]՞՝«»\\-\\—՝։\\։]+)', Str)\n",
        "LstTok = []\n",
        "for el in Lst:\n",
        "    el = el.strip()\n",
        "    if el != '': LstTok.append(el)\n"
      ],
      "metadata": {
        "id": "LD8NIcoycI9S"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(LstTok)\n",
        "# does disambiguation work? not yet..."
      ],
      "metadata": {
        "id": "JK8FDH6A4nD4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d801b9c-4f9e-4a3c-a552-42c8e081a054"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Սառը', ',', 'վճիտ', '.', 'ապրիլյան', 'օր', 'էր', ',', 'ու', 'ժամացույցը', 'խփում', 'էր', 'տասներեքը', '։', 'Չար', 'քամուց', 'թաքնվելու', 'համար', 'կզակը', 'սեղմելով', 'կրծքին', '՝', 'Ուինսթոն', 'Սմիթն', 'արագ', 'ներս', 'խցկվեց', '«', 'Հաղթանակ', '»', 'բնակելի', 'տան', 'ապակե', 'շքադռնից', '՝', 'իր', 'ետևից', 'ներս', 'թողնելով', 'հատիկավոր', 'փոշու', 'մի', 'ամբողջ', 'փոթորիկ', '։']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec # The word2vec model class\n",
        "import gensim.downloader as api # Allows us to download some free training data"
      ],
      "metadata": {
        "id": "2izWzuMquhvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vahram's model for English\n",
        "!wget https://heibox.uni-heidelberg.de/f/c2ba64e4ad844f3a99d4/?dl=1\n",
        "!cp index.html?dl=1 WIKI_EN.model"
      ],
      "metadata": {
        "id": "hnW716PEuraV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_WIKI_EN = Word2Vec.load(\"/content/WIKI_EN.model\")\n",
        "word_vectors_WIKI_EN = model_WIKI_EN.wv"
      ],
      "metadata": {
        "id": "Ce8pWo4eu9VO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distance = model_WIKI_EN.similarity('obama', 'barak')\n",
        "distance2 = model_WIKI_EN.similarity('obama', 'zone')\n"
      ],
      "metadata": {
        "id": "W3d3XxegvGAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('distance = %.4f' % distance)\n",
        "print('distance2 = %.4f' % distance2)"
      ],
      "metadata": {
        "id": "GHlfrbD_vjkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizeHy(Str2tokenise, rePattern = r'([ ,\\.:;\\!\\(\\)\\\"\\[\\]՞՝«»\\-\\—՝։\\։]+)'):\n",
        "    LTokens = []\n",
        "    Lst = re.split(rePattern, Str2tokenise)\n",
        "    # LstTok = []\n",
        "    for el in Lst:\n",
        "        el = el.strip()\n",
        "        if el != '': LTokens.append(el)\n",
        "    return LTokens"
      ],
      "metadata": {
        "id": "_FMKB1I7v2ba"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merging together different lines of code\n",
        "def disambiguate(LTok2disambiguate, Window = 4, show_variants = False):\n",
        "    SDisambig = ''\n",
        "\n",
        "    analysesN = a.analyze_words(LTok2disambiguate, disambiguate=False)\n",
        "\n",
        "    # preparing data structures\n",
        "    LLContext = [] # empty list of contexts, indices are the same as with the Text list\n",
        "    LLText = [] # test to disambiguate, ambiguous interpretations are double entries\n",
        "    for ana in analysesN:\n",
        "        # creating context window from glosses\n",
        "        # if len(ana) > 1: tab = \"~\"\n",
        "        # else: tab = \"!\"\n",
        "        LwfoContext = []\n",
        "        LwfoText = []\n",
        "        for wfo in ana: # preserve lemmas / word forms which are not found in dictionary\n",
        "            if wfo.gramm == '': wfo.gramm = 'N'\n",
        "            if wfo.lemma == '': wfo.lemma = wfo.wf\n",
        "            if wfo.gloss == '': wfo.gloss = '[unknown]'\n",
        "\n",
        "            SWfo = f'{wfo.wf}\\t{wfo.gramm}\\t{wfo.lemma}\\t{wfo.gloss}'\n",
        "            # print(SWfo)\n",
        "            # FNoD.write(SWfo + '\\n')\n",
        "\n",
        "            # find the first part of the gloss, which may be in the word vectors model\n",
        "            REPart = re.match('([A-Za-z]+)', wfo.gloss)\n",
        "            if REPart:\n",
        "                SGlossMin = REPart.group(1)\n",
        "                SGlossMin = SGlossMin.lower()\n",
        "                # print(SGlossMin)\n",
        "            else:\n",
        "                SGlossMin = '[NONE]'\n",
        "            LwfoContext.append(SGlossMin)\n",
        "            LwfoText.append(SWfo)\n",
        "        LLContext.append(LwfoContext)\n",
        "        LLText.append(LwfoText)\n",
        "\n",
        "    # for el in LLContext: print(el)\n",
        "    # for el in LLText: print(el)\n",
        "\n",
        "    # print(len(LLContext))\n",
        "    # print(len(LLText))\n",
        "\n",
        "    print('line done!...\\n')\n",
        "\n",
        "\n",
        "    for i in range(len(LLText)):\n",
        "        if len(LLText[i]) > 1:\n",
        "            # print(LLText[i])\n",
        "            # print(LLContext[i])\n",
        "            # collect context window +- 3 words\n",
        "            iwStart = i-Window\n",
        "            if iwStart <0: iwStart=0\n",
        "            iwEnd = i+Window\n",
        "            if iwEnd > len(LLText): iwEnd = len(LLText)\n",
        "            # iwLen = iwEnd - iwStart\n",
        "            winContext = LLContext[iwStart:iwEnd]\n",
        "            # print(winContext)\n",
        "            LScores = []\n",
        "            LScCand = []\n",
        "\n",
        "            for candidate in LLContext[i]:\n",
        "                ScoreCand = 0\n",
        "                for LCtx in winContext:\n",
        "                    for Ctx in LCtx:\n",
        "                        try: distance = model_WIKI_EN.similarity(candidate, Ctx)\n",
        "                        except: distance = 0\n",
        "                        ScoreCand += distance\n",
        "                LScores.append((candidate,ScoreCand))\n",
        "                LScCand.append(ScoreCand)\n",
        "            LScores.sort(key=lambda a: a[1], reverse=True)\n",
        "            # print(LScores)\n",
        "\n",
        "            max_value = max(LScCand)\n",
        "            #  Return the max value of the list\n",
        "            max_index = LScCand.index(max_value)\n",
        "            StoWrite = LLText[i][max_index] + '\\n'\n",
        "            SDisambig += StoWrite\n",
        "\n",
        "            if show_variants == True:\n",
        "                for el in LLText[i]:\n",
        "                    StoWrite = '\\t~\\t' + el + '\\n'\n",
        "                    SDisambig += StoWrite\n",
        "                for el in LScores:\n",
        "                    StoWrite = '\\t~sc:\\t' + str(el) + '\\n'\n",
        "                    SDisambig += StoWrite\n",
        "        else:\n",
        "            StoWrite = LLText[i][0] + '\\n'\n",
        "            SDisambig += StoWrite\n",
        "\n",
        "    return SDisambig"
      ],
      "metadata": {
        "id": "TpxYmxdTwGyH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir udhrTT"
      ],
      "metadata": {
        "id": "8Oe0qK2z03AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# FInText = open('/content/udhr/udhr_hye_v03.txt','r')\n",
        "# FOutText = open('/content/udhrTT/udhr_hye_vert.txt','w')\n",
        "\n",
        "\n",
        "FInText = open('arm.txt','r')\n",
        "FOutText = open('arm.vert','w')"
      ],
      "metadata": {
        "id": "YMgnQ_crwuQw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for SLine in FInText:\n",
        "    SLine = SLine.strip()\n",
        "    LTok = tokenizeHy(SLine)\n",
        "    SDisambig = disambiguate(LTok)\n",
        "    # FOutText.write('<p>\\n')\n",
        "    # FOutText.write('\\n')\n",
        "    FOutText.write(SDisambig)\n",
        "    # FOutText.write('</p>\\n')\n",
        "    FOutText.write('\\n')\n",
        "\n",
        "FOutText.flush()"
      ],
      "metadata": {
        "id": "61sqTRpmxOFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !awk -F '\\t' '(NF==4){printf \"%s \", $3}(NF!=4){printf \"\\n\"}' < /content/udhrTT/udhr_hye_vert.txt >/content/udhrTT/udhr_hye_lem.txt\n",
        "!awk -F '\\t' '(NF==4){printf \"%s \", $3}(NF!=4){printf \"\\n\"}' < arm.vert >arm_lem.txt"
      ],
      "metadata": {
        "id": "ScstreEf1RDn"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}
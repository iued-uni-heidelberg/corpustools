{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPD2ElNQW9Rc3qn/i6soR4g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1e4e7c10ec384a059dbeb3b580eb5350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0da3ba1d47f84ab4adecb55893d4b66b",
              "IPY_MODEL_95d0ca8418ee45dbaaeafbaa2892fc85",
              "IPY_MODEL_f633a0e788a142209a2e7ca1fc9e109b"
            ],
            "layout": "IPY_MODEL_eba3ac7659cd4218acaa10e79e0c06d7"
          }
        },
        "0da3ba1d47f84ab4adecb55893d4b66b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a03ef28aea3427f993d4c7b3842e00b",
            "placeholder": "​",
            "style": "IPY_MODEL_2f84d1bd2aa1493c8bba66314492b32c",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: "
          }
        },
        "95d0ca8418ee45dbaaeafbaa2892fc85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_938331a8d1c94d7d9cc865784e375a92",
            "max": 28918,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be2d699fe05d4decb2823c5c6269d954",
            "value": 28918
          }
        },
        "f633a0e788a142209a2e7ca1fc9e109b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bce48ae0b1b24539840e67f0fab3d68b",
            "placeholder": "​",
            "style": "IPY_MODEL_c3728c9d63f141c9ac5a2009f43ceafc",
            "value": " 193k/? [00:00&lt;00:00, 6.94MB/s]"
          }
        },
        "eba3ac7659cd4218acaa10e79e0c06d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a03ef28aea3427f993d4c7b3842e00b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f84d1bd2aa1493c8bba66314492b32c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "938331a8d1c94d7d9cc865784e375a92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be2d699fe05d4decb2823c5c6269d954": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bce48ae0b1b24539840e67f0fab3d68b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3728c9d63f141c9ac5a2009f43ceafc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53fd25f8feea47bc87d86d60c25608f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4fe44c3728224eb892f74b28410f6bac",
              "IPY_MODEL_ffff3ec6d0044fe593dfcff5abdc9c05",
              "IPY_MODEL_a3eec149ecc44d56bb0c3982f2d64083"
            ],
            "layout": "IPY_MODEL_c4e7c3a02946433cba5d7ea88ceb62ad"
          }
        },
        "4fe44c3728224eb892f74b28410f6bac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53c9a8825bb74611a643f6de51afb09f",
            "placeholder": "​",
            "style": "IPY_MODEL_1da89a9cf68440fdb44a2ca59702e1cb",
            "value": "Downloading https://huggingface.co/stanfordnlp/stanza-hy/resolve/v1.4.1/models/default.zip: 100%"
          }
        },
        "ffff3ec6d0044fe593dfcff5abdc9c05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3543bb30a0054dab9d797fa840ed8e92",
            "max": 454395920,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93af944e581249faadc92d64944a025d",
            "value": 454395920
          }
        },
        "a3eec149ecc44d56bb0c3982f2d64083": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87c340d5cdcf47859f4b8f5ecd92ea99",
            "placeholder": "​",
            "style": "IPY_MODEL_3d940b3264de4160a9e7e1bab6b66ad3",
            "value": " 454M/454M [00:08&lt;00:00, 57.8MB/s]"
          }
        },
        "c4e7c3a02946433cba5d7ea88ceb62ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53c9a8825bb74611a643f6de51afb09f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1da89a9cf68440fdb44a2ca59702e1cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3543bb30a0054dab9d797fa840ed8e92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93af944e581249faadc92d64944a025d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "87c340d5cdcf47859f4b8f5ecd92ea99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d940b3264de4160a9e7e1bab6b66ad3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/corpustools/blob/main/S101lemHYstanza.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Armenian lemmatization with Stanza"
      ],
      "metadata": {
        "id": "skix7t6sFaZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## downloading evaluation sets\n",
        "- 420 words: test with about 420 words of Armenian text\n",
        "- Armenian \"Brown-type\" corpus b"
      ],
      "metadata": {
        "id": "fFBRX6lTFfcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### optional\n",
        "!wget https://heibox.uni-heidelberg.de/f/ce6096da570f47b99500/?dl=1"
      ],
      "metadata": {
        "id": "m549clSLFHs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### optional\n",
        "!mv index.html?dl=1 evaluation-set-v01.txt"
      ],
      "metadata": {
        "id": "eI_j8KDdFRCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/a847a12bffd4491f9070/?dl=1\n"
      ],
      "metadata": {
        "id": "IWwqNLkxPMfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv index.html?dl=1 TED2020-dehy-hy-aa"
      ],
      "metadata": {
        "id": "0QkNxyeRPb7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### downloading Armenian Wikipedia\n",
        "!wget https://heibox.uni-heidelberg.de/f/d1f866a61bd545318213/?dl=1\n",
        "!mv index.html?dl=1 hywiki-20221101-pages-articles.txt.gz\n",
        "!gunzip hywiki-20221101-pages-articles.txt.gz"
      ],
      "metadata": {
        "id": "j_-b-YYGq8bw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wc hywiki-20221101-pages-articles.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQMJimofsTMd",
        "outputId": "b72457fb-481d-43e0-ce2e-e316e72ec3b8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2446411  56341167 803098410 hywiki-20221101-pages-articles.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing stanza"
      ],
      "metadata": {
        "id": "i__aUXulFkw2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdzVArLUF3cb"
      },
      "outputs": [],
      "source": [
        "!pip install spacy-stanza"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "import spacy_stanza\n"
      ],
      "metadata": {
        "id": "-VN9g4N4GAR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### testing English stanza (optional)"
      ],
      "metadata": {
        "id": "w6Kfwj63FzYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optional\n",
        "# Download the stanza model if necessary\n",
        "stanza.download(\"en\")\n",
        "\n",
        "# Initialize the pipeline\n",
        "nlp = spacy_stanza.load_pipeline(\"en\")\n",
        "\n",
        "doc = nlp(\"Barack Obama was born in Hawaii. He was elected president in 2008.\")\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.dep_, token.ent_type_)\n",
        "print(doc.ents)"
      ],
      "metadata": {
        "id": "dEA7KJdZPrWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### downloading and testing Armenian stanza"
      ],
      "metadata": {
        "id": "HGyKxEJNGG8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stanza.download(\"hy\")\n"
      ],
      "metadata": {
        "id": "xq53mDsUGumV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "1e4e7c10ec384a059dbeb3b580eb5350",
            "0da3ba1d47f84ab4adecb55893d4b66b",
            "95d0ca8418ee45dbaaeafbaa2892fc85",
            "f633a0e788a142209a2e7ca1fc9e109b",
            "eba3ac7659cd4218acaa10e79e0c06d7",
            "2a03ef28aea3427f993d4c7b3842e00b",
            "2f84d1bd2aa1493c8bba66314492b32c",
            "938331a8d1c94d7d9cc865784e375a92",
            "be2d699fe05d4decb2823c5c6269d954",
            "bce48ae0b1b24539840e67f0fab3d68b",
            "c3728c9d63f141c9ac5a2009f43ceafc",
            "53fd25f8feea47bc87d86d60c25608f1",
            "4fe44c3728224eb892f74b28410f6bac",
            "ffff3ec6d0044fe593dfcff5abdc9c05",
            "a3eec149ecc44d56bb0c3982f2d64083",
            "c4e7c3a02946433cba5d7ea88ceb62ad",
            "53c9a8825bb74611a643f6de51afb09f",
            "1da89a9cf68440fdb44a2ca59702e1cb",
            "3543bb30a0054dab9d797fa840ed8e92",
            "93af944e581249faadc92d64944a025d",
            "87c340d5cdcf47859f4b8f5ecd92ea99",
            "3d940b3264de4160a9e7e1bab6b66ad3"
          ]
        },
        "outputId": "7e0e33c1-09ef-437c-8cd9-7dda8f9813a2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e4e7c10ec384a059dbeb3b580eb5350"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloading default packages for language: hy (Armenian) ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://huggingface.co/stanfordnlp/stanza-hy/resolve/v1.4.1/models/default.zip:   0%|          | 0…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53fd25f8feea47bc87d86d60c25608f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Finished downloading models and saved to /root/stanza_resources.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_hy = spacy_stanza.load_pipeline(\"hy\")"
      ],
      "metadata": {
        "id": "KZKOs0aVG8Kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### optional\n",
        "doc = nlp_hy(\"ՄԱՐԴՈՒ ԻՐԱՎՈՒՆՔՆԵՐԻ ՀԱՄԸՆԴՀԱՆՈՒՐ ՀՌՉԱԿԱԳԻՐ. ՆԵՐԱԾԱԿԱՆ. Քանզի մարդկային ընտանիքի բոլոր անդամներին ներհատուկ արժանապատվությունըև հավասար ու անօտարելի իրավունքները աշխարհի ազատության, արդարության ու խաղաղության հիմքն են.\")"
      ],
      "metadata": {
        "id": "m022wp2JHvOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### optional\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.dep_, token.ent_type_)\n"
      ],
      "metadata": {
        "id": "LPhSOX15ICmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### full analysis of the file (optional)\n",
        "- includes dependency parsing"
      ],
      "metadata": {
        "id": "NVyvpuMDQXYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### optional\n",
        "with open('/content/TED2020-dehy-hy-aa', 'r', encoding='utf-8') as infile, open('/content/TED2020-dehy-hy-aa-ANALYSIS-full-v01.txt', 'w') as outfile:\n",
        "    # read sample.txt an and write its content into sample2.txt\n",
        "    outfile.write(\"{token.text}\\t{token.lemma_}\\t{token.pos_}\\t{token.dep_}\\t{parentLem}\\t{LAncestors}\\n\")\n",
        "    for line in infile:\n",
        "        line = line.strip()\n",
        "        doc = nlp_hy(line)\n",
        "        # outfile.write(line + '\\n')\n",
        "        for token in doc:\n",
        "            LAncestors = list(token.ancestors)\n",
        "            print(str(LAncestors))\n",
        "            try:\n",
        "                SLAncestors = str(list(token.ancestors))\n",
        "                parent = LAncestors[0]\n",
        "                parentLem = parent.lemma_\n",
        "            except:\n",
        "                parentLem = \"NONE\"\n",
        "            outfile.write(f\"{token.text}\\t{token.lemma_}\\t{token.pos_}\\t{token.dep_}\\t{parentLem}\\t{SLAncestors}\\n\")\n",
        " "
      ],
      "metadata": {
        "id": "HJdW66EmJI2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### function for lemmatization"
      ],
      "metadata": {
        "id": "_w7MrFvNQqxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parseFile(iFileName, oFileName, nlp_model = nlp_hy):\n",
        "    with open(iFileName, 'r', encoding='utf-8') as infile, open(oFileName, 'w') as outfile:\n",
        "        # read sample.txt an and write its content into sample2.txt\n",
        "        outfile.write(\"{token.text}\\t{token.pos_}\\t{token.lemma_}\\n\")\n",
        "        c = 0\n",
        "        for line in infile:\n",
        "            c+=1\n",
        "            if c%10 == 0: print(str(c))\n",
        "            line = line.strip()\n",
        "            doc = nlp_model(line)\n",
        "            # outfile.write(line + '\\n')\n",
        "            for token in doc:\n",
        "                LAncestors = list(token.ancestors)\n",
        "                # print(str(LAncestors))\n",
        "                try:\n",
        "                    SLAncestors = str(list(token.ancestors))\n",
        "                    parent = LAncestors[0]\n",
        "                    parentLem = parent.lemma_\n",
        "                except:\n",
        "                    parentLem = \"NONE\"\n",
        "                outfile.write(f\"{token.text}\\t{token.pos_}\\t{token.lemma_}\\n\")\n",
        " \n",
        "    return\n"
      ],
      "metadata": {
        "id": "QT0tpHwjY4O5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### command to lemmatize the file"
      ],
      "metadata": {
        "id": "FJoxNaZ7vfVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parseFile('/content/TED2020-dehy-hy-aa', '/content/TED2020-dehy-hy-aa--lemmatization-v01.txt', nlp_hy)"
      ],
      "metadata": {
        "id": "rgdb1fl3a6F4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf97ebf0-cf21-47d7-a370-efd54489e93a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parseFile('hywiki-20221101-pages-articles.txt', 'hywiki-20221101-pages-articles.vert', nlp_hy)"
      ],
      "metadata": {
        "id": "lrSvU_NbsZ9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking OCR errors\n",
        "### wikipedia lemmatized --> frequency dictionary "
      ],
      "metadata": {
        "id": "Disi8bxMhOD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/5b3213f991f84ca496ba/?dl=1\n",
        "!mv index.html?dl=1 hywiki-20221101-pages-articles-v03.vert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhs5GwRihMxr",
        "outputId": "e37abde0-efe6-4b20-bcac-8fff82e9234c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-19 08:42:12--  https://heibox.uni-heidelberg.de/f/5b3213f991f84ca496ba/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/eefa9114-fcbb-4dda-82a5-5d8b57c68a85/hywiki-20221101-pages-articles-v03.vert [following]\n",
            "--2022-12-19 08:42:13--  https://heibox.uni-heidelberg.de/seafhttp/files/eefa9114-fcbb-4dda-82a5-5d8b57c68a85/hywiki-20221101-pages-articles-v03.vert\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75483279 (72M) [application/octet-stream]\n",
            "Saving to: ‘index.html?dl=1’\n",
            "\n",
            "index.html?dl=1     100%[===================>]  71.99M  18.8MB/s    in 5.0s    \n",
            "\n",
            "2022-12-19 08:42:18 (14.5 MB/s) - ‘index.html?dl=1’ saved [75483279/75483279]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wc hywiki-20221101-pages-articles-v03.vert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJlX-J5ij72h",
        "outputId": "44d1002e-b628-4301-bba1-740001910496"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 2735467  8206467 75483279 hywiki-20221101-pages-articles-v03.vert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/350790e66ca24efdab1a/?dl=1\n",
        "!mv index.html?dl=1 hy-texts-vert.tgz \n",
        "!tar xvzf hy-texts-vert.tgz"
      ],
      "metadata": {
        "id": "dtxtpTo_mTSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/d601ceb0af5a4671a8e7/?dl=1\n",
        "!mv index.html?dl=1 Parfum_Arm_ABBY.txt"
      ],
      "metadata": {
        "id": "2LPlwmpYsSt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parseFile('Parfum_Arm_ABBY.txt', 'Parfum_Arm_ABBY.vert.txt', nlp_hy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnrYlLB_uRsH",
        "outputId": "2c7062a5-db00-4c3b-d484-e62b06ddada0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "20\n",
            "30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Due to multiword token expansion or an alignment issue, the original text has been replaced by space-separated expanded tokens.\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Հարցնում', 'են', '՝', 'ինչ', '՞', 'է', 'եդելնրա', 'հետ', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'Ինչ', '՞', 'էնա', 'անում', 'դանակով', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40\n",
            "50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'Սա', 'ինչ', '՞', 'է', '–', 'ասաց', 'Տերյեն', 'և', ',', 'կռանալով', 'զամբյուղի', 'վրա', ',', 'հոտոտեց', 'այն', ',', 'քանի', 'որ', 'ենթադրում', 'էր', 'դրա', 'մեջ', 'ինչ', '-', 'որ', 'ուտելիք', 'հայտնաբերել', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'Նրա', 'համար', 'վնասակար', 'չի', 'լինի', ',', '–', 'շշպռեց', 'ժան', '–', 'նան', ',', '–', 'իսկ', 'ինձ', 'համար', 'կլինի', '։', 'Ես', 'նիհարել', 'եմ', 'տասը', 'ֆունտ', ',', 'չնայած', 'կերել', 'եմ', 'երեք', 'հոգու', 'փոխարեն', '։', 'Իսկ', 'հանուն', 'ինչի', '՞', '։', 'Հանուն', 'շաբաթական', 'երեք', '<UNK>րանկի', '՞', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'Բայց', 'մյուս', 'կողմից', ',', 'լավ', 'չէ', 'երեխային', 'դես', 'ու', 'դեն', 'նետել', '։', 'Ով', '՞', 'գիտի', ',', 'օգտակար', 'կլինի', '\"', 'նրան', 'արդյոք', 'այդ', 'կաթը', '։', 'Աանկիկը', ',', 'հասկանում', 'ես', ',', 'սովորել', 'է', 'քո', 'կրծքի', 'հոտին', 'ու', 'քո', 'սրտի', 'բաբախյունին', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'Ապա', 'որքան', '՞', 'ես', 'պահանջում', ',', '–', 'գոռաց', 'Տերյեն', '։', '–', 'Հինգ', 'ֆրանկը', 'Նման', 'չնչին', 'գործի', 'դիմաց', ',', 'ինչպիսին', 'Նորածնին', 'կերակրելն', 'է', ',', 'մի', 'կույտ', 'փող', 'է', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'Բայց', 'ինչու', '՞', ',', 'սիրելիս', ',', '–', 'ասաց', 'Տերյեն', 'և', 'կրկին', 'մատով', 'շուռումուռ', 'տվեց', 'զամբյուղի', 'պարունակությունը', '։', '–', 'չէ', '՞', 'որ', 'սա', 'հիասքանչ', 'մանկիկ', 'է', '։', 'Այնքան', 'վարդագույն', 'է', ',', 'լաց', 'չի', 'լինում', ',', 'հանգիստ', 'է', 'քնում', ',', 'և', 'կնքված', 'էլ', 'է', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'Անհնար', 'է', '։', 'Բացարձակապես', 'անհնար', 'է', ',', 'որ', 'կրծքի', 'երեխան', 'դիվահար', 'լինի', '։', 'Երեխան', 'մարդ', 'չէ', ',', 'այլ', 'նախամարդ', ',', 'և', 'դեռևս', 'չի', 'տնօրինում', 'ամբողջապես', 'ձևավորված', 'հոգուն', '։', 'Հետևաբար', ',', 'սատանայի', 'համար', 'այն', 'հետաքրքրություն', 'չի', 'ներկայացնում', '։', 'Միգուցե', 'նա', 'արդեն', 'խոսում', '՛', 'է', '։', 'Միգուցե', 'նրա', 'մոտ', 'ջղաձձութթու', '՛', 'է', '։', 'Միգուցե', 'նա', 'տեղաշարու', '՛', 'է', 'սենյակի', 'իրերը', '։', 'Միգուցե', 'Նրանից', 'գարշահոտ', '՛', 'է', 'գալիս', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'Դե', 'տեսնում', '՛', 'ես', '։', 'Ահա', 'այն', '՝', 'նախանշանը', '։', 'Եթե', 'նա', 'դիվահար', 'լիներ', ',', 'ապա', 'նրանից', 'գարշահոտ', 'կփչեր', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'Որովհետև', 'նա', 'առողջ', 'է', '–', 'գոռաց', 'Տերյեն', ',', '–', 'նա', 'առողջ', 'է', ',', 'այդ', 'պատճառով', 'էլ', 'հոտ', 'չունի', '։', 'Հոտ', 'ունեն', 'միայն', 'հիվանդ', 'երեխաները', ',', 'դա', 'բոլորին', 'է', 'հայտնի', '։', 'Օրինակ', ',', 'եթե', 'երեխան', 'ջրծաղիկ', 'ունի', ',', 'նրանից', 'ձիու', 'թրիքի', 'հոտ', 'է', 'գալիս', ',', 'իսկ', 'եթե', 'քութեշ', ',', 'ապա', 'հին', 'խնձորի', ',', 'իսկ', 'թոքախտավոր', 'երեխայից', 'սոխի', 'հոտ', 'է', 'գափս', '։', 'Սա', 'առողջ', 'է', '.', 'ահա', 'և', 'բոլորը', '։', 'Այդ', 'դեպքում', 'ինչու', '՛', 'պետք', 'է']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['նրանից', 'գարշահոտ', 'գա', '։', 'Միթե', '՛', 'քո', 'Աեփական', 'երեխաներից', 'գարշահոտ', 'է', 'գալիս', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90\n",
            "100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'Ահա', ',', '֊', 'ասաց', 'բավարարված', 'Տերյեն', 'ու', 'ձեռքերը', 'կրկին', 'ծալեց', 'թիկունքում', '։', '–', 'Կնշանակի', '՝', 'սատանայի', 'հետ', 'կապված', 'խոսքը', 'մենք', 'ետ', 'ենք', 'վերցնում', '։', 'Լավ', '։', 'Իսկ', 'հիմա', 'բարի', 'եղիր', 'ինձ', 'բացատրել', 'ինչ', '՞', 'հոտ', 'է', 'գալիս', 'կրծքի', 'նորածիններից', ',', 'եթե', 'նրանցից', 'գալիս', 'է', 'այնպիսի', 'հոտ', ',', 'որպիսին', ',', 'քո', 'կարծիքով', ',', 'պետք', 'է', 'գա', '։', 'Դե', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'Ինչ', '՞', 'է', 'նշանակում', '«', 'լավ', '»', ',', '–', 'ողջ', 'ուժով', 'գոռաց', 'նրա', 'վրա', 'Տերյեն', '։', '–', 'Աիթե', '՞', 'քիչ', 'են', 'այնպիսի', 'բաները', ',', 'որոնք', 'լավ', 'հոտ', 'ունեն', '։', 'Փնջովնարդոսը', 'լավ', 'հոտ', 'ունի', '։', 'Ապուրի', 'միսը', 'լավ', 'հոտ', 'ունի', '։', 'Արաբական', 'այգիները', 'լավ', 'հոտ', 'ունեն', '։', 'Ես', 'ցանկանում', 'եմ', 'իմանալ', 'ինչ', '՞', 'հոտ', 'ունեն', 'նորածինները', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'Ինչպես', 'կարւսմեեը', '՞', ',', '–', 'հւսրցրեց', 'նա', '՝', 'ձգտելով', 'կրկին', 'վերադառնալ', 'խիստ', 'խոսելւսոճին', '։', '–', 'Կարւսմել', '։', 'Ինչ', '՞', 'ես', 'հւսսկւսնում', 'կարամելից', '։', 'Դոնե', 'մի', 'անգամ', 'կերել', '՞', 'ես', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['մինչդեռ', 'սեփական', 'բանականությունից', 'օգտվելու', 'համար', 'մարդուն', 'անհրաժեշտ', 'է', 'ինքնավստահություն', 'ու', 'հանգիստ', '։', 'Սակայն', 'նա', 'ամենավճռական', 'ձևով', 'պայքարում', 'էր', 'հասարակ', 'ժողովրդի', 'սնահավատության', 'դեմ', '։', 'Կախարդանքն', 'ու', 'խաղաթղթով', 'գուշակությունը', ',', 'հմայիլների', 'կրումը', ',', 'չար', 'աչքից', 'ազատվելը', ',', 'ոգիների', 'կախարդանքները', ',', 'լիալոանի', 'պահին', 'աճպարարությունները', '...', 'Ինչով', '՜', 'ասես', 'չէին', 'զբաղվում', 'այդ', 'մարդիկ', '։', 'Նրան', 'խորապես', 'հուսահատեցնում', 'էր', ',', 'որ', 'նմանատիպ', 'հեթանոսական', 'ավանդույթները', 'քրիստոնեական', 'կրոնի', 'առավել', 'քան', 'հազարամյա', 'գոյությունից', 'հետո', 'դեռևս', 'արմատախիլ', 'չէին', 'արվել', '։', 'Միաժամանակ', ',', 'այսպես', 'կոչված', ',', 'դիվահարության', 'ու', 'սատանայի', 'հետ', 'կապերի', 'դեպքերի', 'մեծ', 'մասը', 'էլ', 'ավելի', 'մոտիկից', 'ուսումնասիրման', 'ժամանակ', 'ներկայանում', 'էին', 'որպես', 'սնոտիապաշտական', 'ներկայացումներ', '։', 'ճիշտ', 'է', ',', 'մերժել', 'բուն', 'սատանայի', 'գոյությունը', ',', 'կասկածել', 'նրա', 'իշխանության', 'մեջ', 'այդքան', 'հեռու', 'հայր', 'Տերյեն', 'չէր', 'գնա', '.', 'նմանատիպ', 'խնդիրների', 'լուծումը', ',', 'որոնք', 'առնչվում', 'էին', 'աստվածաբանության', 'հիմքերին', ',', 'համեստ', 'ու', 'հասարակ', 'վանականի', 'գործը', 'չէր', ',', 'դրա', 'համար', 'գոյություն', 'ունեն', 'այլ', 'ատյաններ', '։', 'Մյուս', 'կողմից', '՝', 'օրվա', 'պես', 'պարզ', 'էր', ',', 'որ', ',', 'եթե', 'նման', 'կարճամիտ', 'անձնավորությունը', ',', 'ինչպիսին', 'այդ', 'ծծմայրն', 'էր', ',', 'պնդում', 'է', ',', 'որ', 'ինքն', 'ինչ', '-', 'որ', 'դիվայնություն', 'է', 'հայտնաբերել', ',', 'նշանակում', 'է', '՝', 'սատանան', 'ոչ', 'մի', 'դեպքում', 'չէր', 'կարող', 'կապված', 'լինել', 'այդ', 'գործի', 'հետ', '։', 'Հատկապես', 'այն', 'պատճառով', ',', 'որ', 'Նրան', 'թվում', 'է', ',', 'թե', 'իբր', 'ինքը', 'դա', 'հայտնաբերել', 'է', '։', 'չէ', '՞', 'որ', 'դա', 'ճշմարիտ', 'ապացույցն', 'է', 'նրա', ',', 'որ', 'ոչ', 'մի', 'դիվայնություն', 'էլ', 'իրականում', 'չկար', ',', 'սատանան', 'այն', 'աստիճան', 'հիմար', 'չէ', ',', 'որ', 'թույլ', 'տա', 'ծծմայր', 'ժ', '՝', 'ան', '–', 'Նա', 'Ռյուսիին', 'իրեն', 'բացահայտել', '։', 'Եվ', 'այն', 'էլ', 'հոտառությամբ', '։', 'Զգացմունքներից', 'ամենապարզունակի', ',', 'ամենանվաստի', 'օգնությամբ', '։', 'Կարծես', 'թե', 'դժոխքից', 'ծծմբի', 'հոտ', 'է', 'գալիս', ',', 'իսկ', 'դրախտից', 'խունկի', 'ու', 'զմուռսի', '։', 'Դա', ',', 'իրոք', ',', 'ամենամութ']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['֊', 'Ախ', '՜', ',', 'և', 'այս', 'դժբախտ', 'փոքրիկ', 'մանկիկը', '։', 'Այս', 'անմեղ', 'արարածը', '։', 'Պառկած', 'է', 'իր', 'զամբյուղում', 'ու', 'քաղցր', 'քնել', 'է', '՝', 'անտեղյակ', 'այն', 'ստոր', 'կասկածանքներին', ',', 'որոնք', 'առաջ', 'են', 'քաշվել', 'նրա', 'դեմ', '։', 'Իսկ', 'այդ', 'անպատկառ', 'անձը', 'համարձակվում', 'է', 'պնդել', ',', 'որ', 'դու', ',', 'իբր', ',', 'հոտ', 'չունես', ',', 'ինչպի', '–', 'սին', 'պետք', 'է', 'ունենան', 'մարդկային', 'մանուկները', '։', 'Եվ', 'խնչ', 'ասենք', 'մենք', 'դրա', 'վերաբերյալ', '։', '<UNK>ու', '՜', '-', 'ղու', '՜', '-', 'ղու', '՜', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Եվ', 'նա', 'զգուշորեն', 'օրորում', 'էր', 'ծնկների', 'վրա', 'դրված', 'զամբյուղը', ',', 'մատով', 'շոյում', 'նորածնի', 'գլուխն', 'ու', 'մի', 'քանի', 'աեգամ', 'կրկնում', 'ղու', '՜', '-', 'ղու', '՜', '-', 'ղու', '՜', ',', 'քանզի', 'կարծում', 'էր', ',', 'որ', 'այդ', 'բացականչությունը', 'հանգստացուցիչ', 'ու', 'բարերար', 'ազդեցություն', 'է', 'թողնում', 'փոքրիկների', 'վրա', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['֊', 'Կարամելի', 'հոտ', 'պետք', 'է', 'ունենաս', ',', 'այ', 'քեզ', 'հիմարություն', ',', 'ղու', '՜', '-', 'ղու', '՜', '-', 'ղու', '՜', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['խցկեց', 'շատ', 'խոր', ',', 'այնպես', 'որ', ',', 'Նորածնի', 'բարակ', 'շիկահեր', 'մազիկները', 'խուտուտ', 'տվեցին', 'նրա', 'ռունգերը', ',', 'հոտոտեց', 'երեխայի', 'գլուխը', 'հուսալով', 'ինչ', '-', 'որ', 'հոտ', 'ներշնչել', '։', 'Նա', 'այնքան', 'էլ', 'լավ', 'չէր', 'պատկերացնում', ',', 'թե', 'ինչպիսի', 'հոտ', 'պետք', 'է', 'ունենան', 'նորածինների', 'գլուխները', '։', 'Բնականաբար', ',', 'ոչ', 'կարամելի', 'հոտ', ',', 'այդ', 'մեկը', 'պարզից', 'պարզ', 'էր', '.', 'չէ', '՞', 'որ', 'կարամելն', 'այրված', 'շաքար', 'է', ',', 'և', 'ինչպես', 'կարող', 'է', 'նորածինը', ',', 'որը', 'մինչ', 'այժմ', 'միայն', 'կաթ', 'էր', 'խմում', ',', 'այրված', 'շաքարի', 'հոտ', 'ունենալ', '։', 'Նրանից', 'կարող', 'էր', 'կաթի', 'հոտ', 'գալ', 'ծծմոր', 'կաթի', '։', 'Բայց', 'նրանից', 'կաթի', 'հոտ', 'չէր', 'գալիս', '։', 'Նրանից', 'կարող', 'էր', 'մազերի', 'հոտ', 'գալ', ',', 'մաշկի', 'ու', 'մազերի', ',', 'և', ',', 'միգուցե', ',', 'մի', 'քիչ', 'մւսնկան', 'քրտնքի', 'հոտ', '։', 'Տերյեն', 'հոտոտեց', 'և', 'այնուհետև', 'համոզեց', 'իրեն', ',', 'որ', 'զգում', 'է', 'մաշկի', ',', 'մազերի', 'ու', ',', 'միգուցե', ',', 'մանկան', 'քրտնքի', 'թույլ', 'հոտը', '։', 'Բայց', 'նա', 'ոչինչ', 'չէր', 'զգում', '։', 'Որքան', 'էլ', 'ճգնում', 'էր', '։', '«', 'Հւսվանաբար', ',', 'մանուկները', 'հոտ', 'չունեն', '»', ',', '–', 'մտածում', 'էր', 'Նւս', '։', 'Հավանաբար', ',', 'դա', 'է', 'հարցը', '։', 'Հարցն', 'այն', 'է', ',', 'որ', 'նորւսծինը', ',', 'եթե', 'նրան', 'ւցահեն', 'մաքրության', 'մեջ', ',', 'ընդհանրապես', 'չի', 'կարող', 'հոտ', 'ունենալ', ',', 'ինչպես', 'չի', 'կարող', 'խոսել', ',', 'վազել', 'կամ', 'գրել', '։', 'Այս', 'հատկանիշները', 'գալիս', 'են', 'միայն', 'տարիքի', 'հետ', '։', 'ճշգրիտ', 'ւսսած', '՝', 'մարդը', 'միայն', 'սեռական', 'հասունացման', 'շրջւսնում', 'է', 'սկսում', 'սուր', 'հոտ', 'արձակել', '։', 'Այո', ',', 'հենց', 'ւսյդպես', 'էլ', 'կա', '։', 'Այդպես', ',', 'այլ', 'ոչ', 'ւսյլ', 'կերպ', '։', 'միթե', '՞', 'իր', 'ժամանակին', 'Հորացիոսը', 'չէր', 'գրում', '.', '«', 'Պատանուց', 'այծիկի', 'հոտ', 'է', 'գալիս', ',', 'իսկ', 'աղջիկը', 'բուրում', 'է', ',', 'ինչպես', 'ծաղիկը', 'սպիտակ', 'նարգիզի', '»', '։', 'Ով-ով', ',', 'բայց', 'հռոմեացիները', 'դրա', 'մասին', 'պատկերացում', 'ունեին', '։', 'Մարդկային', 'հոտը', 'մշտապես', 'մարմնի', 'հոտն', 'է', ',', 'հետևաբար', 'մեղքի', 'հոտը', '։', 'Այդ', 'դեպքում', 'ինչ', '՞', 'հոտ', 'պետք', 'է', 'ունենա', 'նորածինը', ',', 'որը', 'դեռևս', 'ոչ', 'երազով', ',', 'ոչ', 'հոգով', 'մեղավոր', 'չէ', 'մարմնական', 'մեղքի', 'մեջ', '։', 'Ինչ', '՞', 'հոտ', 'պետք', 'է', 'նւս', 'ունենա', '։', '<UNK>ու', '՜', '-', 'ղու', '՜', '-', 'ղու', '՜', '։', 'Ոչ', 'մի', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['ձեռքը', 'փոքրիկ', 'և', 'գեղեցիկ', ',', 'դուրս', 'էր', 'ցցվել', 'կափարիչի', 'տակից', 'ու', 'ցնցվում', 'էր', 'այտի', 'ուղղությամբ', '։', 'Տերյեն', 'գորովալից', 'ժպտաց', 'ու', 'հանկարծ', 'իրեն', 'շատ', 'հարմարավետ', 'զգաց', '։', 'Ինչ', '-', 'որ', 'մի', 'պահ', 'Նա', 'նույնիսկ', 'իրեն', 'թույլ', 'տվեց', 'մի', 'ֆանտաստիկ', 'միտք', ',', 'որ', 'կարծես', 'թե', 'ինքը', 'այդ', 'երեխայի', 'հայրն', 'էր', '։', 'Կարծես', 'թե', 'ինքը', 'դարձել', 'է', 'ոչ', 'թե', 'վանական', ',', 'այլ', 'նորմալ', 'քաղքենի', ',', 'միգուցե', 'ազնիվ', 'արհեստավոր', ',', 'իրեն', 'կին', 'է', 'գտել', 'մի', 'այնպիսի', 'տաքուկ', 'կին', ',', 'որից', 'բրդի', 'ու', 'կաթի', 'հոտ', 'է', 'գալիս', ',', 'և', 'նրանք', 'ծնել', 'են', 'որդի', ',', 'և', 'ահա', 'ինքը', 'նրան', 'օրորում', 'է', 'իր', 'սեփական', 'ծնկների', 'վրա', 'իր', 'սեփական', 'երեխային', '՝', 'ղու', '՜', '-', 'ղու', '՜', '-', 'ղու', '՜', 'այդ', 'միտքը', 'հաճույք', 'էր', 'պատճառում', '։', 'Նրանում', 'ինչ', '-', 'որ', 'սփոփիչներշնչանք', 'կար', '։', 'Հայրը', 'ծնկների', 'վրա', 'օրորում', 'է', 'իր', 'սեփական', 'որդուն', '՝', 'ղու', '՜', '-', 'ղու', '՜', '-', 'ղու', '՜', ',', 'պատկերը', 'հին', 'էր', '՝', 'ինչպես', 'աշխարհը', ',', 'և', 'հավերժ', 'նոր', 'ու', 'ճիշտ', 'այն', 'ժամանակից', ',', 'ինչ', 'աշխարհը', 'լուսավոր', 'է', ',', 'հենց', 'այդպես', '։', 'Տերյեի', 'սիրտը', 'ջերմացավ', ',', 'նա', 'հուզվեց', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Եվ', 'այդ', 'պահին', 'նորածինը', 'սկսեց', 'ժչալ', '։', 'Նա', 'կկոցեց', 'աչքերը', ',', 'լայն', 'բացեց', 'իր', 'կարմիր', 'բուկը', 'և', 'այնքան', 'զզվելի', 'ու', 'ականջ', 'ծակող', 'ձայնով', 'ծղրտաց', ',', 'որ', 'Տերյեի', 'արյունը', 'երակներում', 'սառեց', '։', 'Նա', 'առաջ', 'պարզած', 'ձեռքով', 'ցնցում', 'էր', 'զամբյուղն', 'ու', 'գոռում', '՝', 'ղու', '՜', '–', 'ղու', '՜', '–', 'ղու', '՜', ',', 'որպեսզի', 'երեխային', 'ստիպի', 'լռել', ',', 'բայց', 'վերջինս', 'ավելի', 'բարձր', 'էր', 'ոռնում', '.նրա', 'դեմքը', 'կապտել', 'էր', ',', 'ևնա', 'կարծես', 'պատրաստ', 'էր', 'ոռնոցից', 'պայթել', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140\n",
            "150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['ուժեղ', 'խելքն', 'ու', 'փորձը', ',', 'որպեսզի', 'ընտրություն', 'կատարի', 'երկու', 'տարբերակների', 'միջև', '։', 'Բայց', ',', 'այնուամենայնիվ', ',', 'նա', 'ընտրություն', 'կատարեց', 'հօգուտ', 'աճեցողական', 'տարբերակի', ',', 'ինչպես', 'ընտրություն', 'է', 'կատարում', 'սերմնահատիկը', ',', 'պետք', '՛', 'է', 'արդյոք', 'իրեն', 'ծիլեր', 'տալ', 'թե', 'ավելի', 'լավ', 'է', 'մնալ', 'չհասունացած', '։', 'Կամ', 'ինչպես', 'տիզը', 'ծառի', 'վրա', ',', 'որին', 'նույնպես', 'կյւսնքը', 'չի', 'ւսռաջարկում', 'որևէ', 'այլ', 'տարբերւսկ', ',', 'հւսրա', '–', 'տև', 'ձմեռներից', 'բացի', '։', 'Փոքրիկ', 'այլանդակ', 'տիզն', 'իր', 'ւսրճճա', '–', 'մոխրագույն', 'մարմինը', 'փաթաթում', 'է', 'գնդի', 'պես', ',', 'որպեսզի', 'դեպի', 'արտաքին', 'աշխարհը', 'դարձնի', 'յուր', 'նվազագույն', 'մակերեսը', ',', 'նա', 'իր', 'մաշկը', 'դարձնում', 'է', 'հւսրթ', 'ու', 'խիտ', ',', 'որպեսզի', 'ոչինչ', 'չարձակի', 'դուրս', 'ոչնվազագույն', 'ճառագայթում', ',', 'ոչ', 'թեթևագույն', 'գոլորշացում', '։', 'Տիզը', 'դիտավորյալ', 'իրեն', 'փոքր', 'ու', 'աննկատ', 'է', 'դարձնում', ',', 'որւցեսզի', 'ոչ', 'ոք', 'չնկատի', 'ու', 'չտրորի', '։', 'Միայն', 'տիզը', ',', 'կենտրոնանալով', 'ինքն', 'իր', 'մեջ', ',', 'Նստում', 'է', 'իր', 'ծառի', 'վրա', '՝', 'կույր', ',', 'խուլ', 'ու', 'համր', ',', 'ու', 'միայն', 'հոտոտում', 'է', ',', 'տարիներ', 'շւսրունակ', 'մի', 'քանի', 'միլիմետր', 'հեռավորությունից', 'հոտոտում', 'է', 'մոտակայքով', 'ւսնցնող', 'ողջերի', 'արյունը', ',', 'որոնց', 'նա', 'երբեք', 'չի', 'հասնի', '։', 'Տիզը', 'թերևս', 'կարող', 'էր', 'թույլ', 'տւսլ', 'իրեն', 'ընկնել', '։', 'Նա', 'թերևս', 'կւսրող', 'էր', 'թույլ', 'տալ', 'իրեն', 'ընկնել', 'անտառի', 'հողին', ',', 'իր', 'փոքրազույն', 'ոտիկներով', 'սողալ', '-', 'անցնել', 'մի', 'քանի', 'միլիմետր', 'այս', 'կւսմ', 'այն', 'կողմ', 'ու', 'թւսղվելչոր', 'տերևների', 'մեջ', 'մեռնելու', ',', 'և', 'ոչ', 'ոք', 'նրա', 'համար', 'չէր', 'ափսոսա', '։', 'Ասւո', '–', 'ծան', 'է', 'հւսյտնի', ',', 'որ', 'ոչ', 'ոք', '։', 'Բայց', 'տիզը', 'հւսմւսռ', 'է', ',', 'տոկուն', 'ու', 'գւսրշելի', ',', 'ծւցտվել', 'է', ',', 'աւցրում', 'է', 'ու', 'սւցասում', '։', 'Սպւսսում', 'է', ',', 'մինչև', 'որ', 'բւսրձրագույն', 'աստիճանի', 'անհավւսնւսկւսն', 'դիւց', '–', 'կածն', 'ուղիղ', 'իր', 'մոտ', '՝', 'ծառի', 'ւոակ', 'բերի', 'որևիցե', 'կենդւսնու', 'տեսքուէ', 'արյուն', '։', 'Եվ', 'միայն', 'այդ', 'ժւսմանւսկ', 'է', 'նա', 'հրւսժար', '–', 'վում', 'իր', 'զարտնիությունից', ',', 'պոկվում', 'է', 'տեղից', 'ու', 'կառչում', ',', 'ներպտուտակվում', ',', 'խրվում', 'օտար', 'մւսրմնի', 'մեջ', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160\n",
            "170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Գրենույը', 'Աստել', 'էր', 'ոտքերը', 'մեկնած', 'փայտերի', 'վրա', '.', 'թիկունքով', 'հենվելով', 'ցախանոցի', 'պատին', '՝', 'նա', 'փակել', 'էր', 'աչքերն', 'ու', 'չէր', 'շարժվում', '։', 'Նա', 'ոչինչ', 'չէր', 'տեսնում', ',', 'ոչինչ', 'չէր', 'լսում', 'ու', 'չէր', 'զգում', '։', 'Նա', 'ուղղակի', 'ներշնչում', 'էր', 'կւայ', '՞', '–', 'տի', 'հոտը', ',', 'որը', 'քուլայվում', 'էր', 'նրա', 'շուրջն', 'ու', 'կուտակվում', 'տանիքի', 'ներքևում', ',', 'ինչպես', 'թասակի', 'տակ', '։', 'Նա', 'խմում', 'էր']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "180\n",
            "190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Գետից', 'ոչ', 'հեռու', 'Աորտելյերի', 'փողոցի', 'վրա', ',', 'ապրում', 'էր', 'Նրա', 'ծանոթը', 'Գրիմալ', 'ազգանունով', 'կաշեգործը', ',', 'որին', 'աշխատանքի', 'համար', 'մշտապես', 'պետք', 'էին', 'լինում', 'տղաներ', 'ոչ', 'թե', 'որպես', 'աշակերտներ', 'կամ', 'ենթավարպետներ', ',', 'այլ', 'որպես', 'էժան', 'աշխատուժ', '։', 'չէ', '՞', 'որ', 'այղ', 'արհեստի', 'մեջ', 'հարկ', 'էր', 'լինում', 'կատարել', 'կյանքի', 'համար', 'այն', 'աստիճան', 'վտանգավոր', 'գործողություններ', 'մորթափառից', 'մաքրել', 'նեխող', 'գազանների', 'մորթիները', ',', 'միմյանց', 'խառնել', 'դաբաղման', 'թունավոր', 'ու', 'ներկանյութերի', 'լուծույթները', ',', 'թափել', 'կսկծոր', 'օգտագործված', 'քիմիական', 'նյութերը', ',', 'որ', 'կարգին', 'վարպետը', ',', 'սովորաբար', 'խնայելով', 'իր', 'ուսուցառած', 'օգնականներին', ',', 'վարձում', 'էր', 'գործազուրկ', 'ու', 'անտուն', 'խառնամբոխին', 'կամ', 'խնամազուրկ', 'երեխաներին', ',', 'որոնց', 'ճակատագրով', 'դժբախտության', 'դեպքում', 'ոչ', 'ոք', 'չի', 'հետաքրքրվի', '։', 'Հասկանալի', 'է', ',', 'որ', 'տիկին', 'Գայարը', 'գիտեր', ',', 'որ', 'Գրիմալի', 'դաբաղանոցում', 'Գրենույը', 'մարդկային', 'չափանիշներով', 'կենդանի', 'մնալու', 'շանս', 'չուներ', '։', 'Բայց', 'Նա', 'այնպիսի', 'կին', 'չէր', ',', 'որ', 'մտահոգվեր', 'նման', 'հարցերի', 'շուրջ', '։', 'Չէ', '\"', 'որ', 'նա', 'կատարել', 'էր', 'իր', 'պարտքը', '։', 'Խնամակալությունն', 'ավարտվել', 'է', '։', 'Ինչ', 'էլ', 'որ', 'տեղի', 'ունենար', 'խնամառուի', 'հետ', 'ապագայում', ',', 'դա', 'նրան', 'չէր', 'վերաբերում', '։', 'Ողջ', 'կմնա', '՝', 'լավ', 'է', ',', 'կմեռնի', '՝', 'Նույնպես', 'լավ', 'է', ',', 'կարևորն', 'այն', 'է', ',', 'որ', 'ամեն', 'ինչ', 'լինի', 'օրենքով', '։', 'Այդ', 'իսկ', 'պատ', '–', 'ճառովնա', 'պարոն', 'Գրիմալին', 'խնդրեց', 'գրությամբ', 'հաստատել', 'երեխայի', 'փոխանցումը', ',', 'իր', 'հերթին', 'ստացական', 'տվեց', 'տասնհինգ', 'ֆրանկ', 'միջնորդադրամ', 'ստանալու', 'վերաբերյալ', 'և', 'ուղղվեց', 'տուն', '՝', 'Շարոն', 'փողոց', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n",
            "210\n",
            "220\n",
            "230\n",
            "240\n",
            "250\n",
            "260\n",
            "270\n",
            "280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Այն', 'հանգամւսնքը', ',', 'որ', 'այս', 'հոյւսկապության', 'սկզբում', 'կանգնած', 'էր', 'սպանությունը', ',', 'նա', ',', 'եթե', 'ընդհւսնրաւցես', 'գիտակցում', 'էր', 'դւս', ',', 'ընդունում', 'էր', 'խոր', 'անտւսրբերու', '–', 'թյամբ', '։', 'Աարե', 'փորոցի', 'աղջկւս', 'արտւսքինը', '՝', 'նրւս', 'դեմքը', ',', 'Նրա', 'մարմինը', ',', 'Գրենույն', 'արդեն', 'չէր', 'կւսրողւսնում', 'վերհիշել', '։', 'չէ', '՞', 'որ', 'ւցւսհւցւսնել', 'էր', 'լավագույնը', ',', 'ինչը', 'նա', 'խլեց', 'ու', 'սեփւսկանացրեց', '.', 'նրա', 'բուրմունքի', 'էությունը', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "290\n",
            "300\n",
            "310\n",
            "320\n",
            "330\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['ալքիմիկոս', 'է', ',', 'ասում', 'են', 'մարդիկ', ',', 'լավ', 'է', ',', 'թող', 'այդպես', 'էլ', 'մտածեն', '։', 'Այն', 'մասին', ',', 'որ', 'իր', 'արվեստն', 'արհեստ', 'է', ',', 'ինչպես', 'և', 'ցանկացած', 'ուրիշը', ',', 'գիտեր', 'միայն', 'ինքը', ',', 'և', 'դրանում', 'էր', 'նրա', 'հպարտությունը', '։', 'Նա', 'չէր', 'էլ', 'ցանկանում', 'գյուտարար', 'լինել', '։', 'Գյուտարարությունը', 'բավական', 'կասկածելի', 'է', ',', 'գտնում', 'էր', 'Ռալդինին', ',', 'քանի', 'որ', 'այն', 'մշտապես', 'նշանակում', 'է', 'կանոնների', 'խախտում', '։', 'Նա', 'ամենևին', 'էլ', 'չէր', 'պատրաստվում', 'կոմս', 'Վերամոնի', 'համար', 'նոր', 'օծանելիք', 'հնա', '–', 'րել', '։', 'Համենայնդեպս', ',', 'Շենյեն', 'հարկադրված', 'չի', 'լինի', 'իրեն', 'հւսմոզել', 'Պելիսյեից', '«', 'Ամուրն', 'ու', 'Պսիքեն', '»', 'ձեռք', 'բերել', '։', 'Նա', 'արդեն', 'ձեռք', 'էր', 'բերել', 'այդ', 'օծանելիքները', '։', 'Ահա', 'դրանք', ',', 'պաւոուհւսնի', 'մոտի', 'գրասեղանի', 'վրա', '՝', 'հղկած', 'խցանով', 'փոքրիկ', 'ապակե', 'սրվակների', 'մեջ', '։', 'Նա', 'դրւսնք', 'գնել', 'էր', 'մի', 'քանի', 'օր', 'ւսռաջ', '։', 'Բնականաբար', ',', 'անձամբ', 'չէր', 'գնել', '։', 'չէ', '՞', 'որ', 'նւս', 'չէր', 'կարող', 'օծանելիքի', 'համար', 'անձամբ', 'մտնել', 'Պելիսյեի', 'մոտ', '։', 'Նա', 'գործել', 'էր', 'միջնորդի', 'միջոցով', ',', 'իսկ', 'վերջինս', 'էլ', 'իր', 'հերթին', 'մեկ', 'այլ', 'միջնորդի', 'օգնությւսմբ', '։', 'Զգուշությունը', 'երբեք', 'չի', 'խանգւսրի', '։', 'Քանզի', 'Ռալդինին', 'պատրաստվում', 'էր', 'այդ', 'օծանելիքն', 'օգտւսգործել', 'ոչ', 'միայն', 'իսպանակւսն', 'կաշին', 'բուրումնավետ', 'դարձնելու', 'հւսմար', '.', 'դրա', 'համար', 'մեկ', 'սրվակը', 'չէր', 'բւսվականւսցնի', '։', 'Նա', 'էլ', 'ավելի', 'վատ', 'մտադրություն', 'ուներ', ',', 'ւցւստճենել', 'այն', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ախ', '՜', ',', 'որքան', 'կատ', 'է', ',', 'որ', 'ազնիվ', 'մարդը', 'ստիււ|ւ|ած', 'է', 'հնւսրամտություն', 'գործածել', '։', 'Որքան', 'ծանր', 'է', 'զոհւսբե–', 'րել', 'այն', 'ամենւսթանկարժեքը', ',', 'որ', 'ունես', '՝', 'նման', 'խղճուկ', 'ձևով', 'վարկւսբեկելով', 'սեփական', 'ւցւստիվը', '։', 'Ռւսյց', 'ինչ', '՞']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['«', 'խոնջանքով', '»', ',', 'մշկային', 'գերհագեցած', 'բուրմունքով', '։', 'Բոլորին', 'հանկարծ', 'տիրում', 'էր', 'մուշկի', 'հոտով', 'բուրելու', 'գազա', '–', 'նւսյին', 'ցանկությունը', ',', 'և', 'Բալդինիին', 'ոչինչ', 'չէր', 'մնում', ',', 'քան', 'իր', 'հազրեվարդը', 'վերամշակել', 'գլուխը', 'լվանալու', 'համար', 'ջրի', 'ունարդոսը', 'կարել', 'բույրաբարձիկի', 'մեջ', '։', 'Դրա', 'փոխարեն', ',', 'երբ', 'հաջորդ', 'տարի', 'նա', 'պատվիրեց', 'համապատասխան', 'քանակությամբ', 'մուշկ', ',', 'մշկահոտ', 'ցիբետին', 'ու', 'կող', '–', 'բենու', 'շիթ', ',', 'Պելիսյեի', 'խելքին', 'փչեց', 'հորինել', '«', 'Անտառային', 'ծաղիկ', '»', 'անվանումով', 'օծանելիք', ',', 'և', 'այն', 'անմիջապես', 'հաջողություն', 'նվաճեց', '։', 'Գիշերային', 'երկարատև', 'փորձերի', 'գնով', 'կամ', 'խելագար', 'գումարներով', 'լրտեսներին', 'կա', '–', 'շառելով', 'Բալդինին', 'ի', 'վերջո', 'պարզեց', ',', 'թե', 'ինչից', 'է', 'բաղկացած', '«', 'Անտառային', 'ծաղիկները', '»', ',', 'իսկ', 'Պելիսյեն', 'արդեն', 'կրկին', 'աչքի', 'ընկավ', 'այս', 'անգամ', '«', 'Թուրքական', 'գիշերներով', '»', 'կամ', '«', 'էիսաբոնյան', 'բուրմունքով', '»', ',', '«', 'Թագավորակւսն', 'պալատի', 'ծաղկեփնջով', '»', 'կւսմ', ',', 'սատանան', 'գիտի', ',', 'թե', 'էլ', 'ինչով', '։', 'Համենայնդեպս', ',', 'այդ', 'մարդն', 'իր', 'անսանձելի', 'նորարարական', 'կրքով', 'վտանգ', 'էր', 'ներկայացնում', 'ողջ', 'արհեստի', 'համար', '։', 'Ինչ', 'լավ', 'կլիներ', ',', 'եթե', 'դաժան', 'ժամանակների', 'արտադրամասային', 'իրավունքը', 'նորից', 'ետ', 'գւսր', '։', 'Նմւսն', 'լկտի', 'դուրսպրծուկի', ',', 'նման', 'գռփողի', 'հանդեպ', ',', 'որը', 'հւսրս', '–', 'ւոանում', 'էր', 'հոտերի', 'արժեզրկման', 'հաշվին', ',', 'կարելի', 'էր', 'կիրառել', 'ամենահրեշային', 'միջոցները', '։', 'Նրւսնից', 'խլել', 'ւսրւոո', '–', 'նագիրը', ',', 'արգելելօծանագործային', 'գործի', 'մեջ', 'խցկվել', '...', 'և', 'ընդհանրապես', ',', 'խարդախը', 'նւսխևառաջ', 'թող', 'ինչ', '-', 'որ', 'բան', 'սովորի', '։', 'չէ', '՞', 'որ', 'նա', '՝', 'այդ', 'Պելիսյեն', ',', 'ուսուցւսռւսծ', 'օծանագործ', 'ու', 'ձեռնոցագործ', 'չէր', '։', 'Նրա', 'հայրն', 'ընդամենը', 'քացախ', 'քամող', 'էր', ',', 'և', 'Պելիսյեն', 'ևս', 'քւսցւսխ', 'քամող', 'էր', ',', 'ոչ', 'այլ', 'ինչ', '։', 'Եվ', 'միայն', 'այն', 'պատճւսռով', ',', 'որ', 'նւս', '՝', 'որպես', 'քացախ', 'քւս', '–', 'մող', ',', 'սպիրտային', 'ւսրտադրության', 'մեջ', 'մուտքի', 'իրւսվունք', 'ուներ', ',', 'նրան', 'հաջողվեց', 'ներթափանցել', 'իրւսկւսն', 'օծւսնա', '–', 'գործների', 'շրջան', ',', 'և', 'այժմ', 'նւս', 'անօրինւսկւսնություններ', 'է']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "350\n",
            "360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Եվ', 'կամ', 'վերցնենք', 'խանգարվածությունը', 'արագության', 'վրա', '։', 'Ինչու', '՞', 'անհրաժեշտ', 'եղավ', 'այդքան', 'շատ', 'նոր', 'ճանապարհներ', 'անցկացնել', '։', 'Ինչի', '՞', 'համար', 'են', 'այդ', 'նոր', 'կամուրջները', '։', 'Ինչի', '՞', 'համար', '։', 'Որպեսզի', 'մեկ', 'շաբաթում', 'էիոն', '՞', 'հասնեն', '։', 'Իսկ', 'ինչ', '՞', 'օգուտ', 'կա', 'դրանից', '։', 'Ում', '՞', 'համար', 'է', 'դա', 'օգտավետ', '։', 'Ում', '՞', 'է', 'պետք', 'գլուխը', 'կոտրելով', 'սլանալ', 'Ատլան', '–', 'տյան', 'օվկիանոսով', '։', 'Մեկ', 'ամիս', 'անց', 'Ամերիկայում', 'հայտնվելու', 'համար', '՞', '։', 'Բայց', 'չէ', '՞', 'որ', 'մարդիկ', 'հազարամյակներ', 'շարունակ', 'հրաշալիորեն', 'բավարարվում', 'էին', 'առանց', 'այդ', 'աշխարհամասի', '։', 'Ինչ', '՞', 'է', 'կորցրել', 'նախնադարյան', 'անտառում', 'հնդկացիների', 'կամ', 'սևամորթների', 'մոտ', 'քաղաքակիրթ', 'մարդը', '։', 'Անգամ', 'Հյուսիս', 'Նրանք', 'հասան', '՝', 'էւսւցլան', '–', 'դիա', ',', 'որտեղ', 'հավերժական', 'սառույց', 'է', ',', 'և', 'որտեղ', 'ւսպրում', 'են', 'վայրի', 'մւսրդիկ', ',', 'ովքեր', 'հում', 'ձուկ', 'են', 'խժռում', '։', 'Դա', 'դեռ', 'քիչ', 'էր', ',', 'ցանկւսցան', 'ևս', 'մի', 'աշխարհամաս', 'հայտնաբերել', ',', 'ասում', 'են', ',', 'ինչ', '-', 'որ', 'տեղ', 'հարավային', 'ծովերում', '։', 'Իսկ', 'մեր', 'ինչին', '՞', 'է', 'պետք', 'այդ', 'խելագարությունը', '։', 'Միայն', 'այն', 'պատճառով', ',', 'որ', 'ուրիշներն', 'էլ', 'են', 'այդպես', 'անում', '՝', 'իսպանացիները', ',', 'անիծյալ', 'անգլիացիները', ',', 'անպատկառ', 'հոլւսնդացիները', ',', 'որոնց', 'հետ', 'հետագայում', 'ստիպված', 'էինք', 'մարտ', 'մղել', ',', 'ինչն', 'ընդհանրաւցես', 'մեզ', 'չէինք', 'կարող', 'թույլ', 'տւսլ', '։', 'Երեք', 'հարյուր', 'հազար', 'լիկր', 'կանխիկ', 'գումար', 'ահւս', 'թե', 'որքան', 'արժի', 'մեկ', 'ռազմանավը', ',', 'իսկ', 'հետո', 'նա', 'մեկ', 'թնդւսնոթային', 'կրակոցից', 'հինգ', 'րոպեի', 'ընթացքում', 'խորտակվում', 'է', ',', 'և', 'մնւսք', 'բւս', '–', 'րով', 'հավերժ', ',', 'հարկատուների', 'փողեր', '։', 'Այժմ', 'ֆինւսնսների', 'պարոն', 'Նւսխարարը', 'պւսհանջում', 'է', 'իրեն', 'փոխւսնցել', 'բոլոր', 'եկամուտների', 'տւսսներորդ', 'մւսսը', ',', 'և', 'դա', 'կործւսնարար', 'է', ',']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "370\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Քանզի', 'եթե', 'արդեն', 'թույլատրելի', 'է', 'Աստծու', 'եկեղեցու', 'հեղինակությունն', 'ամենաանամոթ', 'ու', 'անպատկառ', 'ձևով', 'կասկածի', 'տակ', 'դնելը', ',', 'եթե', 'ոչ', 'պակաս', 'աստվածատուր', 'միապետության', 'ու', 'թագավորների', 'սրբազան', 'անձերի', 'մասին', 'է', 'խոսվում', 'ուղղակի', 'որպես', 'կառավարման', 'այլ', 'ձևերի', 'ընդհանուր', 'կատալոգում', 'տեղ', 'գտած', 'հնարավոր', 'տարբերակների', ',', 'նրանց', 'կարելի', 'է', 'ընտրել', 'սեփական', 'ճաշակով', '։', 'Ի', 'վերջո', ',', 'հասել', 'ենք', 'նրան', ',', 'որ', 'անգամ', 'անձամբ', 'Աստծուն', '՝', 'ամենազոր', 'Տիրոջը', ',', 'համարում', 'են', 'ավելորդություն', 'ու', 'բացարձակ', 'լրջությամբ', 'պնդում', ',', 'որ', 'երկրի', 'վրա', 'կարգ', 'ու', 'կանոնը', ',', 'բարոյականությունն', 'ու', 'երջանկությունը', 'կարող', 'են', 'լինել', 'առանց', 'նրա', 'ուղղակի', 'բուն', 'մարդկանց', 'բնածին', 'բարոյականության', 'ու', 'բանականության', 'շնորհիվ', '...', '<UNK>', '՜', 'Աստված', ',', 'Աստված', ',', 'այդ', 'դեպքում', 'համենայնդեպս', 'պետք', 'չէ', 'զարմանալ', ',', 'եթե', 'ամեն', 'ինչ', 'ընթանում', 'է', 'գլխիվայր', ',', 'և', 'բարքերը', 'վերջնականապես', 'այլասերվել', 'են', ',', 'ու', 'մարդկությունը', 'իր', 'վրա', 'բերեց', 'պատիժը', 'նրա', ',', 'ում', 'մերժում', 'է', '։', 'Դա', 'վատ', 'կվերջանա', '։', '1681', 'թվին', 'մեծ', 'գիսաստղը', ',', 'որի', 'վրանրանք', 'զվարճանում', 'էին', ',', 'որընրանք', 'համարում', 'են', 'ընդամենը', 'աստղերի', 'կույտ', ',', 'Տիրոջ', 'նախազգու–', 'շացնողնախանշանն', 'էր', ',', 'քանզի', 'այն', ',', 'այժմ', 'արդեն', 'մենք', 'դա', 'գիտենք', ',', 'կանխագուշակեց', 'սանձարձակության', 'հոգևոր']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "380\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Օծանելիքը', 'գարշելիության', 'աստիճան', 'լավն', 'էր', '։', 'Ցավոք', 'սրտի', ',', 'այդ', 'ողորմելի', 'Պելիսյեն', 'լավ', 'գիտեր', 'իր', 'գործը', '։', 'Վարւցետ', 'էր', ',', 'Աստված', 'վկա', ',', 'թող', 'որ', 'նա', 'հազար', 'անգամ', 'ոնչ', 'փ', 'չէր', 'սովորել', '։', 'Ռալդինին', 'կցւսնկանար', ',', 'որպեսզի', 'դւս', 'լիներ', 'իր', 'օծանելիքը', '«', 'Ամուրն', 'ու', 'Պսիքեն', '»', '։', 'Նրանում', 'չկար', 'գռեհկության', 'ստվեր', 'անգամ', '։', 'Ռացարձւսկապես', 'դասական', 'հոտ', 'էր', 'ավարտուն', 'ուներդւսշնակ', '։', 'Եվ', 'միևնույն', 'ժւս', '–', 'մանակ', 'սքանչելիորեն', 'նոր', '։', 'Թարմ', 'էր', ',', 'բայց', 'ոչ', 'ձանձրալի', '։', 'Ծաղկային', 'էր', ',', 'բայց', 'ոչ', 'քաղցրւսվուն', '։', 'Ուներ', 'խորություն', '՝', 'հրաշալի', ',', 'գրավիչ', ',', 'շքեղ', ',', 'մուգ', 'շւսգւսնակագույն', 'խորություն', ',', 'և', 'ընդ', 'որում', '՝', 'նրւսնում', 'չկար', 'ոչ', 'գերծանրւսբեռնվւս', '–', 'ծություն', ',', 'ոչ', 'վերամբարձություն', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'Հրաշալի', 'է', ',', 'հրաշալի..', '-', 'մրթմրթաց', 'նւս', '՝', 'ւսգւււ', '–', 'հաբւսր', 'հոտուոելով', '։', '–', 'Նրւսնում', 'ուրւսխություն', 'կւս', ',', 'նա', 'չքնաղ', 'է', '՝', 'ինչպես', 'մեղեդին', ',', 'նւս', 'ուղղւսկի', 'ստեղծոււՏ', 'է', 'լւսվ', 'ւորւսմւսդրություն', '...', 'Ինչ', '՜', 'ւսնհեթեթություն', 'է', '՝', 'լւսվ']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['<UNK>իծաղեի', '՜', 'է', 'նման', 'ճարտասանությունը', '.', '«', 'Մեղեդի', 'է', '։', 'Ուրախություն', 'է', '։', 'Չքնաղ', 'է', '։', 'Բարձրացնում', 'է', 'տրամադրությունը', '»', '։', 'Հիմարություն', '։', 'Մանկական', 'հիմարություն', '.', 'Րոպեական', 'տպավորություն', '։', 'Հին', 'սխալ', '։', 'Խառնվածքի', 'հարց', '։', 'Ամենայն', 'հավանականությամբ', 'իտալական', 'ժառանգականություն', '։', 'Երբեք', 'մի', 'դատիր', 'առաջին', 'տպավորությամբ', '։', 'չէ', '՞', 'որ', 'դա', 'ոսկե', 'կանոն', 'է', ',', 'Բւսլդինի', ',', 'այ', 'դու', 'ծեր', 'ոչխարի', 'գլուխ', '։', 'Երբ', 'հոտ', 'ես', 'քաշում', '՝', 'հոտ', 'քաշիր', ',', 'իսկ', 'դատիր', 'հետո', '։', '«', 'Ամուրն', 'ու', 'Պսիքեն', '»', '–', 'ը', 'շարքային', 'օծանելիք', 'չէ', '։', 'Բավական', 'հաջող', 'արտադրանք', 'է', '։', 'ճարպկորեն', 'թխված', 'անշնորհք', 'ապրանք', '։', 'Եթե', 'չասենք', 'կեղծիք', '։', 'Իսկ', 'կեղծիքից', 'բացի', ',', 'ուրիշ', 'էլ', 'ինչ', '՞', 'կարելի', 'է', 'սպասել', 'Պելիսյեի', 'նման', 'մարդուց', '։', 'Բնականաբար', ',', 'այնպիսի', 'տիպը', ',', 'որպիսին', 'Պելիսյեն', 'է', ',', 'հասարակ', 'օծանելիք', 'չի']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Բայց', 'դու', ',', 'Բալդինի', ',', 'թույլչես', 'տա', 'քեզհիմարւսցնեն', '։', 'Դու', 'միայն', 'առաջին', 'րոպեին', 'փոքր', '-', 'ինչ', 'կորցրիր', 'գլուխդ', 'կեղծ', 'տպավորությամբ', '։', 'Բայց', 'միթե', '՞', 'հայտնի', 'է', ',', 'թե', 'ինչ', 'տեղի', 'կունենա', 'այդ', 'հոտի', 'հետ', 'մեկ', 'ժամ', 'անց', ',', 'երբ', 'եթերային', 'փոխւս', '–', 'րինումները', 'գոլորշանան', ',', 'ու', 'բացահայտվի', 'նրա', 'միջուկը', '։', 'Կամ', 'այսօր', 'երեկոյան', ',', 'երբ', 'բուրմունք', 'կւսրձակեն', 'միւսյն', 'այս', 'ծանր', ',', 'մութ', 'բաղադրիչները', ',', 'որոնք', 'այժմ', 'թաքնվում']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400\n",
            "410\n",
            "420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'Քեզ', 'ինչ', '՞', 'է', 'պետք', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "430\n",
            "440\n",
            "450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'հնչում', '՞', 'է', 'բանը', ',', '–', 'հարցրեց', 'նա', '։', '–', 'Դու', 'էլի', '՞', 'ինչ', '-', 'որ', 'բան', 'պետք', 'է', 'ինձ', 'փոխանցես', '։', 'Դե', '՞', '։', 'Խոսիր', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'Դուք', 'ուզում', 'եք', 'ւսյծի', 'մորթիները', 'բուրումնեա', '՛', 'դարձնել', ',', 'վարւցետ', 'Ռալդինի', '։', 'Այս', 'մորթիները', ',', 'որոնք', 'ես', 'եմ', 'ձեզ', 'բերել', ',', 'դուք', 'դրանց', '՞', 'եք', 'ցանկանում', 'բուրմունք', 'հւս', '–', 'ղորդել', ',', '–', 'շշնջաց', 'Գրենույը', 'կւսրծես', 'ի', 'գիտություն', 'չընդունելով', 'Ռալդինիի', 'պատասխանը', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "460\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'Այդպես', ',', '–', 'ասւսց', 'Ռալդինին', ',', 'որը', 'բւսցարձւսկապես', 'ցնցված', 'էր', 'խոսակցության', '՝', 'դեւցի', 'ճշգրիտի', 'ոլորտ', 'նմւսն', 'շրջադարձով', '–', 'էլ', '՞', 'ինչ', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'Ռուրավետ', 'բալասանի', 'յուղը', '՞', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "470\n",
            "480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['բուրավետ', 'բալասանից', ',', 'վարդի', 'յուղից', 'ու', 'մեխակից', ',', 'ինչ֊', 'ւցես', 'նաև', 'բերգամոտից', 'ու', 'հազրեվարդի', 'լուծամզված', '–', 'քից', 'և', 'ւսյլն', '։', 'Դա', 'պարզելու', 'համար', 'պետք', 'է', ',', 'ինչպես', 'ասում', 'են', ',', 'ունենալ', 'բավակւսնին', 'նուրբ', 'հոտառություն', ',', 'ու', 'լիովին', 'հնարավոր', 'է', ',', 'որ', 'Աստված', 'քեզ', 'բավականին', 'նուրբ', 'հոտառություն', 'է', 'տվել', ',', 'ինչպես', 'և', 'շատ', 'ուրիշ', 'մարդկանց', '՝', 'հատկապես', 'քո', 'տարիքում', '։', 'Սակայն', 'օծանագործի', 'համար', '–', 'և', 'այստեղ', 'նա', 'վեր', 'պարզեց', 'մատն', 'ու', 'դուրս', 'ցցեց', 'կուրծքը', ',', '-', 'սակայն', 'օծանագործի', 'համար', 'քիչ', 'է', 'ուղղակի', 'Նուրբ', 'հոտւսռություն', 'ունենալը', '։', 'Նրան', 'անհրաժեշտ', 'է', 'տասնամյակների', 'ընթացքում', 'վարժեցված', ',', 'անկաշառ', 'ւսշխատող', 'հոտառական', 'օրգան', ',', 'որը', 'թույլ', 'կտա', 'վստահորեն', 'կռահել', 'նույնիսկ', 'ամենաբարդ', 'հոտերը', ',', 'դրանց', 'բաղադրությունն', 'ու', 'համաչափությունները', ',', 'ինչւղես', 'նաև', 'ստեղծել', 'նոր', 'բուրմունքների', 'անհայտ', 'խառնուրդներ', '։', 'Նման', 'քիթը', ',', '-', 'և', 'Նա', 'մատով', 'թխկթխկացրեց', 'իրենը', ',', '–', 'այդքան', 'հեշտ', 'չի', 'տրվում', ',', 'երիտասարդ', '։', 'Նման', 'քիթը', 'վաստւսկում', 'են', 'ջւսնւս', '–', 'սիրությւսմբ', 'ու', 'համբերությամբ', '։', 'թե', '՞', 'դու', 'կկւսրողանայիր', 'ուղղակի', 'այնւցես', '՝', 'ձեռքի', 'հետ', 'անվանել', '«', 'Ամուրն', 'ու', 'Պսի', '–', 'քեն', '»', '–', 'ի', 'ճշգրիտ', 'բանաձևը', '։', 'Դե', '՞', '։', 'Կկարոաղնյյի', '՞', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['֊', 'Աիգուցե', 'դու', 'ինձ', 'գոնե', 'մոտավոարպես', '՞', 'կասես', ',', '-', 'ասաց', 'Ռալդինին', 'ու', 'թեթևակի', 'թեքվեց', 'առաջ', ',', 'որպեսզի', 'ավելի', 'լավ', 'ուսումնասիրի', 'դուսն', 'մեջ', 'թաքնված', 'դոդոշին', '։', '–', 'Դոնե', 'մոտավորապես', ',', 'ընդհանուր', 'տեսքով', '։', 'Դե', '՞', '։', 'Խոսիր', ',', 'չէ', '\"', 'որ', 'դու', 'Փւսրիզի', 'լւսվւսգույն', 'քիթն', 'ես', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['֊', 'Դե', ',', 'տեսնում', '՞', 'ես', ',', '–', 'փնթփնթաց', 'Ռալդինին', '՝', 'միւսժա', '–', 'մանւսկ', 'բավւսրարված', 'ու', 'հիւսսթւսփվւսծ', '։', '–', 'Չես', 'կւսրող', '։', 'Իհարկե', 'ոչ', '։', 'Ոնց', 'կարող', 'ես', 'իմւսնւսլ', '։', 'Դու', 'Նրանցից', 'ես', ',', 'ով', 'ճւսշ', 'ուտեփս', 'որոշում', 'է', '՝', 'ւսրդյոք', 'աւցուրի', 'մեջ', 'մաղւսդւս֊', 'նոս', '՞', 'է', ',', 'թե', '\"', 'կերբելուկ', '։', 'Դե', 'ինչ', ',', 'դւս', 'էլ', 'ւսրդեն', 'քիչ', 'չէ', '։', 'Ռայց']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['֊', 'Բանաձև', ',', 'բանաձև', ',', '–', 'խռպոտ', 'ձայնով', 'խոսեց', 'Գրե–', 'նույը', ',', 'և', 'Նրա', 'կերպարը', 'դռան', 'շրջանակի', 'մեջ', 'առավել', 'հստակ', 'ուրվագծվեց', '։', '–', 'Ինձ', 'ոչ', 'մի', 'բանաձև', 'պետք', 'չէ', '։', 'Դեղատոմսն', 'իմ', 'քթի', 'մեջ', 'է', '։', '<UNK>առնեմ', '՞', 'դրանք', 'ձեզ', 'համար', ',', 'մետր', ',', 'խառնեմ', '՞', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'Այսինքն', '՝', 'ինչպես', '՞', ',', '–', 'բացականչեց', 'Բալդինին', 'ավելի', 'բարձր', ',', 'քան', 'պատշաճ', 'էր', 'նրան', ',', 'և', 'մոմը', 'մոտեցրեց', 'թզուկի', 'դեմքին', '։', '–', 'Այսինքն', '՝', 'ինչպես', '՞', 'խառնել', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['֊', 'Դու', 'կարծում', 'ես', ',', 'որ', 'ես', 'քեզ', 'թույլ', 'կտամ', 'տնօրինել', 'իմ', 'արհեստւսնոցը', '։', 'Բնւսհյութերը', ',', 'որոնք', 'մի', 'ողջ', 'ունեցւխսծք', 'ար<UNK>են', '՞', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'Պան', '։', '–', 'Ռալդինին', 'կտրուկ', 'դուրս', 'փչեց', 'իր', 'մեջ', 'եղած', 'շունչը', '։', 'Այնուհետև', 'թոքերը', 'լցրեց', 'օդով', ',', 'երկար', 'նայեց', 'սարդանման', 'Գրենույին', 'և', 'մտորեց', '։', '«', 'Ըստ', 'էության', '՝', 'միթե', 'միևնույն', 'չէ', ',', '–', 'մտածեց', 'նաայսպես', 'թե', 'այնպես', 'վաղն', 'ամեն', 'ինչ', 'ավարտվելու', 'է', '։', 'Ես', ',', 'իհարկե', ',', 'գիտեմ', ',', 'որ', 'նա', 'չի', 'կարող', 'անել', 'այն', ',', 'ինչը', 'խոստանում', 'է', ',', 'դա', 'բացառվում', 'է', ',', 'այլապես', 'նւս', 'ավելի', 'մեծ', 'համբավ', 'կունենար', ',', 'քան', 'մեծն', 'Ֆրանժիպանին', '։', 'Ռայց', 'ինչու', '՞', 'սեւիւսկան', 'աչքերով', 'չհամոզվեմ', 'նրանում', ',', 'ինչը', 'գիտեմ', '։', 'Աիգուցե', 'հւսնկարծ', 'մի', 'գեղեցիկ', 'օր', 'Աեսինայում', 'իմ', 'գլուխը', 'մի', 'միտք', 'գա', ',', 'ծեր', 'մարդկանց', 'մոտ', 'երբեմն', 'լինում', 'են', 'տարօրինակություններ', 'ու', 'խենթ', 'մտքեր', ',', 'որ', 'ես', 'չճանաչեցի', 'մի', 'հանճարի', ',', 'հրաշամա', '–', 'նուկի', ',', 'արարածի', ',', 'ով', 'Աստծու', 'ողորմությամբ', 'շռայլորեն', 'օժտված', 'էր', '...', 'Դա', 'ամբողջապես', 'բացառվում', 'է', '։', 'Ելնելով', 'այն', 'ամենից', ',', 'ինչ', 'ինձ', 'հուշում', 'է', 'բանականությունս', ',', 'դա', 'բացառվում', 'է', '...', 'Ռայց', 'չէ', '՞', 'որ', 'լինում', 'են', 'հրաշքներ', '։', 'Անկասկած', '։', 'Եվ', 'ահա', ',', 'երբ', 'Աեսինայում', 'մոտենա', 'իմ', 'մեռնելու', 'ժամը', ',', 'մահվան', 'մահճում', 'ինձ', 'կայցելի', 'մի', 'միտք', ',', 'այն', 'երեկո', 'Փարիզում', 'քեզներկայացավ', 'հրաշքը', ',', 'իսկ', 'դու', 'փակեցիր', 'աչքերդ', '...', 'Դա', ',', 'բնականաբար', ',', 'այնքան', 'էլ', 'հաճելի', 'չէր', 'լինի', ',', 'Ռալդինի', '։', 'Ավելի', 'լավ', 'է', 'այս', 'հիմարը', 'սեղանի', 'վրա', 'մի', 'երկու', 'կաթիլ', 'վարդի', 'յուղ', 'ու', 'մուշկի', 'թուրմ', 'թափի', ',', 'դու', 'ևս', 'դրանք', 'կթափեիր', ',', 'եթե', 'քեզ', 'դեռևս', 'իրոք', 'շարունակեր', 'հե–', 'տաքրքրել', 'Պելիսյեի', 'օծանելիքը', '։', 'Եվ', 'ինչ', '՞', 'Նշանակություն', 'ունի', 'մի', 'քանի', 'կաթիլը', ',', 'այո', ',', 'թանկարժեք', ',', 'բավականին', ',', 'բավականին', 'թանկարժեք', ',', 'եթե', 'համեմատես', 'դա', 'գիտելիքների', 'հուսալիության', 'ու', 'հանգիստ', 'ծերության', 'հետ', '»', '։', '–', 'Էսիր', ',', '–', 'ասաց', 'նա', 'միտումնավոր', 'խիստ', 'տոնով', '–', 'Ասիր', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ես', '...', 'Ի', 'դեպ', ',', 'ինչպես', '՞', 'է', 'քո', 'անունը', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "510\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['հնարավորություն', 'կստանաս', 'հենց', 'այժմ', ',', 'անմիջապես', 'գործով', 'ապացուցել', 'քո', 'հիմնավորումը', '։', 'Դրանով', 'իսկ', 'դու', 'հնարավորություն', 'կստանաս', 'խայտառակ', 'ձախողման', 'միջոցով', 'սովորել', 'համեստության', '՝', 'որպես', 'առաքինության', '։', 'Քո', 'պատանի', 'տարիքում', 'դա', 'դեռևս', 'ներելի', 'է', 'և', 'դժվար', 'թե', 'անուղղելի', 'աստիճանի', 'հասած', 'լինի', ',', 'սակայն', 'այդ', 'դասը', 'Նախադրյալ', 'է', 'քո', 'հետագա', 'հաջողության', 'համար', '՝', 'որպես', 'արտադրամասի', 'անդամի', ',', 'որպես', 'մարդու', 'ու', 'բարի', 'քրիստոնյայի', '։', 'Ես', 'պատրաստ', 'եմ', 'իմ', 'հաշվին', 'քեզ', 'տալ', 'այդ', 'դասը', ',', 'քանզի', 'որոշակի', 'հանգամանքների', 'բերումով', 'այսօր', 'տրամադրված', 'եմ', 'շռայլություն', 'ցուցաբերել', ',', 'և', ',', 'ով', 'գիտի', ',', 'ինչ', '-', 'որ', 'ժամանակ', 'այս', 'տեսարանի', 'մասին', 'հիշողությունը', ',', 'հնարավոր', 'է', ',', 'ուրախություն', 'պատճառի', 'ինձ', '։', 'Ռայց', 'չկարծես', ',', 'թե', 'քեզ', 'կհաջողվի', 'ինձ', 'խաբել', '։', 'Ջուզեպպե', 'Ռալդինիի', 'քիթը', 'հին', 'է', ',', 'բայց', 'հոտառությունը', 'սուր', ',', 'բավականաչափ', 'սուր', ',', 'որպեսզի', 'անմիջապես', 'հայտնաբերի', 'նվազագույն', 'տարբերությունը', 'քո', 'լուծույթի', 'ու', 'այս', 'արտադրանքի', 'միջև', '։', '–', '–', 'Եվնա', 'գրպանից', 'դուրս', 'հանեց', '«', 'Ամուրն', 'և', 'Պսիքեն', '»', '–', 'ով', 'Ներծծված', 'թաշկինակն', 'ու', 'թւսփ', 'տվեց', 'Գրենույի', 'քթի', 'առաջ', '։', '–', 'Հապա', 'մի', 'այստեղ', 'արի', ',', 'դու', ',', 'Փւսրիզի', 'լավագույն', 'քիթ', '։', 'Հապա', 'այստեղ', 'արի', '՝', 'սեղանի', 'մոտ', ',', 'ու', 'ցույց', 'տուր', ',', 'թե', 'ինչի', 'ես', 'ունակ', '։', 'Ռայց', 'տես', ',', 'այստեղ', 'ոչինչ', 'չկոտրես', 'ու', 'շուռ', 'չտաս', '։', 'Չհւս', '–', 'մարձակվես', 'ոչ', 'մի', 'բանի', 'ձեռք', 'տալ', '։', 'Նախևւսռաջ', 'ես', 'շւստ', 'լույս', 'կվառեմ', '։', 'Մենք', ',', 'ի', 'պատիվ', 'այս', 'փոքրիկ', 'փորձարարության', ',', 'հրավառություն', 'կկազմակերպենք', ',', 'այդդւեսէէէէէ', '՞', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'Ձեզ', 'համար', 'որքան', '՞', 'պատրաստեմ', ',', 'մետր', ',', '–', 'հարցրեց', 'Գրենույը', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'որքան', '՞', ',', 'ինչը', '՞', ',', '–', 'հարցրեց', 'Ռալդինին', ',', 'ով', 'դեռ', 'չէր', 'ավարտել', 'իր', 'խոսքը', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'Այս', 'օծանելիքից', '՝', 'որքան', '՞', ',', '–', 'խռպոտ', 'հարցրեց', 'Գրե–', 'նույը', '։', '–', 'որքան', '՞', 'է', 'ձեզ', 'պետք', 'ղրանից', '։', 'Կուզեք', '՞', 'մինչև', 'եզրը', 'լցնեմ', 'այ', 'այն', 'մեծ', 'ամանը', '։', '–', 'Եվ', 'նա', 'մատնացույց', 'արեց', 'երեք', 'լիտրից', 'ոչ', 'պակաս', 'տարողությամբ', 'խառնամանը', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['֊', 'Ոչ', ',', 'պետք', 'չի', ',', '–', 'սարսափած', 'բացականչեց', 'Ռալդինին', '.', 'նրա', 'այդ', 'գոռոցի', 'մեջ', 'կար', 'վախ', '՝', 'որչափ', 'խոր', 'արմատացած', ',', 'նույնչափ', 'էլ', 'տարերային', 'վախ', 'շռայլության', 'հանդեպ', ',', 'վախ', 'իր', 'սեփականության', 'համար', '։', 'Ռայց', ',', 'կարծես', 'ամաչելով', 'այդ', 'ինքն', 'ա', 'մերկացնող', 'գոռոցից', ',', 'նա', 'անմիջապես', 'էլ', 'մռնչաց', ',', '–', 'չհամարձակվես', 'ինձ', 'ընդհատել', '։', '–', 'Այնուհետև', 'մի', 'քիչ', 'հանգստացավ', 'և', 'շարունակեց', 'թեթևակի', 'հեգնական', 'ձայնով', '–', 'Աեր', 'ինչին', '՞', 'է', 'պետք', 'երեք', 'լիտր', 'օծանելիքը', ',', 'որը', 'երկուսս', 'էլ', 'չենք', 'գնահատում', '։', 'Ըստ', 'էության', 'սրվակի']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "530\n",
            "540\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['քո', 'պարզունակ', 'բթամտությունը', 'ցույց', 'են', 'տալիս', ',', 'որ', 'դու', 'ոչինչ', 'չես', 'հասկանում', ',', 'դու', 'բարբարոս', 'ես', 'ու', 'անտաշ', ',', 'դրա', 'հետ', 'էլ', 'գոնջոտ', ',', 'լկտի', ',', 'փսլնքոտ', '։', 'Դու', 'ի', 'վիճակի', 'չես', 'լիմոնադ', 'խառնել', ',', 'քեզ', 'չի', 'կարելի', 'սովորական', 'մատուտակի', 'ջրի', 'վաճառք', 'վստահել', ',', 'իսկ', 'դու', 'խցկվում', 'ես', 'օծանագործի', 'գործի', 'մեջ', '։', 'Գոհ', 'եղիր', ',', 'ուրախացիր', 'ու', 'շնորհակալ', 'եղիր', ',', 'որ', 'քո', 'տերը', 'քեզ', 'դեռ', 'մոտ', 'է', 'թողնում', 'դաբաղման', 'լուծույթին', '։', 'Եվ', 'չհամարձակվես', ',', 'լսում', '՞', 'ես', ',', 'երբեք', 'չհամարձակվես', 'օծանագործի', 'դռան', 'շեմն', 'անցնել', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "550\n",
            "560\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'Դուք', 'չեք', 'ցանկանում', 'փորձանմուշ', 'վերցնել', ',', '–', 'կրկին', 'կարկաչող', 'ձայնով', 'ասաց', 'Գրենույը', ',', '֊', 'միթե', '՞', 'չեք', 'ուզում', ',', 'վարւցետ', '։', 'միթե', '՞', 'չեք', 'փորձի', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "570\n",
            "580\n",
            "590\n",
            "600\n",
            "610\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['դեպքում', 'ժամանակ', 'առ', 'ժամանակ', 'նա', 'սխալներ', 'էր', 'գործում', ',', 'որոնք', 'այնպես', 'էին', 'հաշվարկված', ',', 'որ', 'Ռալդինին', 'դրանք', 'նկատի', ',', 'մոռանում', 'էր', 'ինչ', '-', 'որ', 'նյութ', 'զտիչի', 'միջով', 'անցկացնել', ',', 'ճիշտ', 'չէր', 'տեղադրում', 'կշեռքը', ',', 'բանաձևի', 'մեջ', 'հավելագրում', 'էր', 'ամպարի', 'անհեթեթ', 'բարձր', 'տոկոս', 'և', 'առիթ', 'էր', 'ստեղծում', ',', 'որ', 'իրեն', 'ցույց', 'տան', 'իր', 'սխալները', ',', 'որպեսզի', 'հետո', 'ինքը', 'մանրակրկիտ', 'ուղղի', '։', 'Այդ', 'կերպ', 'նրան', 'հաջողվում', 'էր', 'Ռալդինիին', 'ներշնչել', 'այն', 'պատրանքը', ',', 'որ', 'վերջիվերջո', 'ամեն', 'ինչ', 'ընթանում', 'է', 'կանոնավոր', 'և', 'պատշաճ', 'հունով', '։', 'չէ', '՞', 'որ', 'նա', 'չէր', 'ուզում', 'վախեցնել', 'ծերուկին', '։', 'չէ', '՞', 'որ', 'իրոք', 'ուզում', 'էր', 'նրանից', 'սովորել', '։', 'Ոչ', 'թե', 'օծանելիքի', 'բաղադրությունը', ',', 'ոչ', 'թե', 'այս', 'կամ', 'այն', 'բուրմունքի', 'կառուցվածքը', ',', 'ամենևին', 'ոչ', '։', 'Այդ', 'բնագավառում', 'աշխարհում', 'չկար', 'մեկը', ',', 'ով', 'կարող', 'էր', 'նրան', 'ինչ', '-', 'որ', 'բան', 'սովորեցնել', '։', 'Ռալդինիի', 'կրպակում', 'առկա', 'բաղադրամասերը', 'ամենևին', 'էլ', 'բավարար', 'չէին', 'իսկական', 'մեծ', 'օծանելիքի', 'մասին', 'նրա', 'պատկերացումներն', 'իրականացնելու', 'համար', '։', 'Այն', 'անուշահոտերը', ',', 'որ', 'կարողանում', 'էր', 'ստեղծել', 'Ռալդինիի', 'մոտ', ',', 'մանկական', 'զվարճանք', 'էր', 'այն', 'անու–', 'շաբույրերի', 'համեմատ', ',', 'որոնք', 'կրում', 'էր', 'իր', 'մեջ', 'ու', 'պատրաստվում', 'էր', 'մի', 'հրաշալի', 'օր', 'դրանք', 'իրականացնել', '։', 'Ռայց', 'դրա', 'համար', ',', 'նա', 'գիտեր', ',', 'պահանջվում', 'էին', 'երկու']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "620\n",
            "630\n",
            "640\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['փոշու', 'տեսքով', 'առանց', 'Նվազագույն', 'հաջողության', '։', 'Նա', 'թորեց', 'արույրը', ',', 'ճենապակին', ',', 'կաշին', ',', 'ցորենն', 'ու', 'մանրախիճը', '։', 'Ուղղակի', 'հողը', '։', 'ԼԼրյունը', ',', 'և', 'ծառը', ',', 'և', 'թարմ', 'ձուկը', '։', 'Իր', 'սեփական', 'մազերը', '։', 'Ի', 'վերջո', ',', 'նա', 'թորեց', 'նույնիսկ', 'ջուրը', 'Սենայի', 'ջուրը', ',', 'որովհետև', 'նրան', 'թվում', 'էր', ',', 'որ', 'պետք', 'է', 'պահպանել', 'նրա', 'ինքնատիպ', 'հոտը', '։', 'Նա', 'մտածում', 'էր', ',', 'որ', 'թորման', 'կաթսայի', 'օգնությամբ', 'ինքը', 'կարող', 'էր', 'այղ', 'նյութերից', 'դուրս', 'բերել', 'նրանց', 'առանձնահատուկ', 'բուրմունքը', ',', 'ինչպես', 'այն', 'դուրս', 'էր', 'բերում', 'ուրցից', ',', 'նարդոսի', 'ու', 'չամանի', 'սերմերից', '։', 'չէ', '՞', 'որ', 'չգիտեր', ',', 'որ', 'թորումը', 'ոչ']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "650\n",
            "660\n",
            "670\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['սիֆիլիսային', 'ծաղկաչեչով', 'ու', 'թարախային', 'կարմրուկով', 'ւո', 'տէՁմԽ', 'ս1էւրոօ', '։', 'Ինչու', '՞', 'ոչ', 'երկու', 'տարի', 'անց', '։', 'Ինչու', '՞', 'ոչ', 'մեկ', 'տարի', 'անց', '։', 'Այդ', 'ընթացքում', 'նրան', 'կարելի', 'էր', 'ամբողջապես', 'քամել', 'ինչպես', 'արծաթի', 'հանքը', ',', 'ինչպես', 'ոսկե', 'ավանակին', '։', 'Եվ', 'թող', 'մի', 'տարուց', 'իր', 'համար', 'հանգիստ', 'մեռներ', '։', 'Ռայց', 'ոչ', '։', 'Նա', 'հիմա', 'է', 'մահանում', ',', 'անիծված', 'լինի', 'նա', ',', 'կմեռնի', 'քառասունութ', 'ժամվա', 'մեջ', '։', 'Ինչ', '-', 'որ', 'մի', 'կարճ', 'պահ', 'Բալդինին', 'մտածեց', 'այն', 'մասին', ',', 'որ', 'ուխտագնացության', 'մեկնի', 'գետից', 'այն', 'կողմ', '՝', 'Նոտր', '-', 'Գամ', '՝', 'մոմ', 'վառելու', ',', 'ու', 'Գրենույի', 'առողջության', 'համար', 'աղերսի', 'Սուրբ', 'Աստվածամորը', '։', 'Ռայց', 'հետո', 'նա', 'հրաժարվեց', 'այդ', 'մտքից', ',', 'քանի', 'որ', 'ժամանակը', 'սուղ', 'էր', '։', 'Նա', 'վազեց', 'գրչի', 'ու', 'թղթի', 'ետևից', 'և', 'կնոջը', 'վռնդեց', 'հիվանդի', 'սենյակից', '։', 'Նա', 'ասաց', ',', 'որ', 'անձամբ', 'կհերթապահի', '։', 'Այնուհետև', 'նստեց', 'մահճակալի', 'կողքի', 'աթոռի', 'վրա', '՝', 'գրառումների', 'համար', 'թղթերը', 'ծնկներին', 'դրած', ',', 'թանաքի', 'մեջ', 'թաթախված', 'գրիչը', 'ձեռքին', ',', 'և', 'փորձեց', 'Գրենույին', 'ղրդել', 'օծանագործային', 'խոստովանանքի', '։', 'Աստծու', 'սիրուն', ',', 'նա', 'չպետք', 'է', 'հենց', 'այնպես', 'իր', 'հետ', 'գերեզման', 'տանի', 'այն', 'գանձերը', ',', 'որոնք', 'կրում', 'է', 'իր', 'մեջ', '։', 'Այժմ', '՝', 'վերջին', 'ժամերին', ',', 'Նա', 'պարտավոր', 'է', 'հուսալի', 'ձեռքերի', 'փոխանցել', 'իր', 'կտակը', ',', 'որպեսզի', 'ժառանգներին', 'չզրկի', 'բոլոր', 'ժամանակների', 'լավագույն', 'բուրմունքներից', '։', 'Ինքը', '՝', 'Ռալդինին', ',', 'հոաալիորեն', 'կտնօրինի', 'այդ', 'կտակը', 'այն', 'բոլոր', 'ամենավեհ', 'անուշահոտությունների', 'բանաձևերի', 'կանոններին', ',', 'որոնք', 'երբևիցե', 'գոյություն', 'են', 'ունեցել', 'աշխարհի', 'վրա', ',', 'նա', 'կհասնի', 'դրանց', 'ճանաչմանը', '։', 'Նա', 'Գրենույի', 'անվանն', 'անմահ', 'փառք', 'կհաղոր', '–', 'ղի', ',', 'երդվում', 'է', 'բոլոր', 'սրբերով', ',', 'որ', 'այդ', 'բուրմունքներից', 'լավագույնը', 'կդնի', 'անձամբ', 'թագավորի', 'ոտքերի', 'առաջ', 'ագարե', 'սրվակով', 'ու', 'ոսկե', 'քանդակադրոշմով', 'և', 'փորագրված', 'ընծայումով', '.', '«', 'Ժան', '-', 'Ռատիստ', 'Գրենույից', '՝', 'Փարիզի', 'օծանագործից', '»', '։', 'Այդպես', 'էր', 'խոսում', 'կամ', ',', 'ավելի', 'շուտ', ',']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'Ասացեք', ',', 'մետր', ',', 'կան', '՞', 'արդյոք', 'այլ', 'միջոցներ', ',', 'քամումից', 'ու', 'թորումից', 'բացի', ',', 'ինչ', '-', 'որ', 'մարմնից', 'բուրմունք', 'ստանալու', 'համար', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "680\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['-', 'որոնք', '՞', 'ենկրկին', 'հնչեց', 'հարցը', ',', 'և', 'այս', 'անգամ', 'Բալ', '–', 'դինին', 'նկատեց', 'Գրենույի', 'շուրթերի', 'շարժումը', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['֊', 'որոնք', '՞', 'են', ',', '–', 'հարցրեց', 'Նա', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['֊', 'Որտեղ', '՞', ',', '–', 'հարցրեց', 'Գրենույը', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "690\n",
            "700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Գրենույը', ',', 'որը', 'ոչ', 'մի', 'պատիվ', 'չուներ', ',', 'չէր', 'հավատում', 'սրբերին', 'և', 'առւսվել', 'ևս', '՝', 'իր', 'մոր', 'դժբախտ', 'հոգուն', ',', 'երդվեց', '։', 'Նա', 'կարող', 'էր', 'երդվել', 'ամեն', 'ինչով', '։', 'Նա', 'կընդուներ', 'Ռալդի', '–', 'նիի', 'բոլոր', 'ւցայմանները', ',', 'քանի', 'որ', 'նրան', 'ւսնհրաժեշտ', 'էր', 'ենթավարպետի', 'կարգավիճակը', 'հաստատող', 'թուղթը', ',', 'որը', 'հնարավորություն', 'էր', 'տալիս', 'նրան', 'առանց', 'աչքի', 'ընկնելու', 'ապրել', ',', 'առանց', 'խոչընդոտների', 'ճանապւսրհորդել', 'և', 'գտնել', 'աշխատանք', '։', 'Մնացածի', 'հւսնդեպ', 'նա', 'անտարբեր', 'էր', '։', 'Եվ', 'իՂւչ', 'պայմաններ', 'էին', 'դրանք', 'որ', '։', 'Չվերադառնալ', 'Փարիզ', '։', 'Իսկ', 'նրա', 'ինչիՆ', 'էր', 'պետք', 'Փւսրիզը', '։', 'Նա', 'անգիր', 'գիտեր', 'մինչև', 'վերջին', 'գարշահոտ', 'անկյունը', ',', 'այն', 'ամենուր', 'կրում', 'էր', 'իր', 'հետ', ',', 'արդեն', 'մի', 'քանի', 'տարի', 'շարունակ', 'տիրում', 'էր', 'Փարիզին', '։', 'Չպատրաստել', 'բւսլդինյան', 'մոդայիկ', 'օծանեիքներ', '՞', '։', 'Չփոխւսնցել', 'բանաձձեր', '՞', '։', 'Կարծես', 'թե', 'նա', 'չի', 'կարող', 'հայտնագործել', 'հազարավոր', 'ուրիշները', ',', 'նույնչափ', 'լավը', ',', 'ավելի', 'լավը', ',', 'հարկավոր', 'է', 'միայն', 'ցանկանալ', '։', 'չէ', '՞', 'որ', 'նա', 'չէր', 'պատրաստվում', 'մրցակցել', 'Ռալդինիի', 'կամ', 'բուրժուա', 'օծանագործներից', 'ցանկւսցածի', 'հետ', '։', 'Նւս', 'չէր', 'էլ', 'մտւսծում', 'իր', 'արվեստով', 'մեծ', 'գումարներ', 'վաստակելու', 'մւսսին', ',', 'նա', 'չէր', 'ուզում', 'նույնիսկ', 'վաստակել', 'դրանով', 'ապրելու', 'գումւսր', ',', 'եթե', 'հՆարաւխր', 'լիներ', 'այլ', 'կերւց', 'ապրել', '։', 'Նա', 'ցւսնկանում', 'էր', 'իրենից', 'դուրս', 'բերել', 'իր', 'ներքին', 'եսը', ',', 'ոչ', 'այլ', 'ինչ', ',', 'քան', 'իր', 'ներքին', 'եսը', ',', 'որը', 'համւսրում', 'էր', 'ավելի']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "710\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['հրաշալի', 'տաղանդը', 'և', 'Ներկայացրել', 'նրա', 'ընդունակությունները', 'որպես', 'իմ', 'սեփականը', '։', 'Ամենաշատը', 'այն', ',', 'որ', 'թեթևակի', 'շեղվել', 'եմ', 'ավանդակւսն', 'արհեստավորական', 'առաքինության', 'ուղուց', '։', 'Ամենաշատը', 'նրանում', ',', 'որ', 'այսօր', 'անում', 'եմ', 'այն', ',', 'ինչը', 'դեռ', 'երեկ', 'անիծում', 'էի', '։', 'Միթե', '՛', 'դւս', 'հանցագործություն', 'է', '։', 'Ուրիշները', 'խաբում', 'են', 'ողջ', 'կյանքում', '։', 'Իսկ', 'ես', 'ընդամենը', 'մի', 'քանի', 'տարի', 'մի', 'քիչ', 'խւսրդախու', '–', 'թյուն', 'արեցի', '։', 'Եվ', 'այն', 'էլ', 'այն', 'պատճառով', ',', 'որ', 'նման', 'անսովոր', 'հնարավորություն', 'ընձեռվեց', '։', 'Միգուցե', 'հնարավորություն', 'էլ', 'չի', 'ընձեռվել', ',', 'միգուցե', 'անձամբ', 'Տերն', 'է', 'իմ', 'տուն', 'ուղարկել', 'այդ', 'կախարդին', ',', 'որպեսզի', 'ինձ', 'պարգևատրի', 'նվաստացումների', 'համար', ',', 'որոնք', 'կրել', 'եմ', 'Պելիսյեից', 'ու', 'նրա', 'հանցակից', 'ընկերներից', '։', 'Միգուցե', 'Աստծու', 'պւստիժը', 'սպասում', 'է', 'ամենևին', 'էլ', 'ոչ', 'ինձ', ',', 'այլ', 'Պելիսյեին', '։', 'Դա', 'շատ', 'ու', 'շատ', 'հնարավոր', 'է', '։', 'Իսկ', 'էլ', '՛', 'ինչով', 'Տերը', 'կկարողւսնար', 'պատժել', 'Պելիսյեին', ',', 'եթե', 'ոչ', 'իմ', 'վերելքով', '։', 'Հետևաբար', ',', 'իմ', 'երջանկությունը', 'Աստծու', 'ձեռքի', 'գործն', 'էր', ',', 'և', 'ես', 'ոչ', 'միայն', 'իրավունք', 'ունեի', ',', 'այլև', 'պարտավոր', 'էի', 'այն', 'ընդունել', 'որպես', 'այդպիսին', '՝', 'առանց', 'ամոթի', 'ու', 'զղջումի', '...', '»', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "720\n",
            "730\n",
            "740\n",
            "750\n",
            "760\n",
            "770\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Եվ', 'այղ', 'ժամանակ', 'հանկարծ', ',', 'հենց', 'դրանում', 'էր', 'վարժության', 'իմաստը', ',', 'կուտակված', 'ատելությունը', 'գինարբու', '–', 'քային', 'հզորությամբ', 'մղում', 'էր', 'դուրս', '։', 'Ինչպես', 'ամպրոպը', 'այն', 'հավաքվում', 'էր', 'այղ', 'հոտերի', 'վերևում', ',', 'որոնք', 'համարձակվել', 'էին', 'անպատվել', 'իր', 'պայծառափայլ', 'քիթը', '։', 'Ինչպես', 'կարկուտը', 'ցորենի', 'դաշտի', 'վրա', '՝', 'նա', 'հարձակվում', 'էր', 'այդ', 'գարշելիության', 'վրա', ',', 'ինչպես', 'մրրիկը', 'այն', 'վեր', 'էր', 'ածում', 'դիմափոշու', 'և', 'խեղդում', 'հորդառատ', 'ջրի', 'կողմից', 'մաքրված', 'ահռելի', 'թորած', 'ջրի', 'մեջ', '։', 'Այդ', 'աստիճան', 'արդար', 'էր', 'նրա', 'ցասումը', '։', 'Այդ', 'աստիճան', 'արդար', 'էրնրա', 'կրեժը', '։', '<UNK>', '՜', '։', 'Ինչպիսի', '՜', 'վեհ', 'ակնթարթ', '։', 'Գրենույը', 'այդ', 'փոքրիկ', 'մարդը', ',', 'գրգռվածությունից', 'դողում', 'էր', ',', 'նրա', 'մարմինը', 'ջղաձգորեն', 'սեղմվում', 'էր', 'քաղցրավուն', 'հաճույքի', 'մեջ', 'ու', 'գալարվում', 'այնպես', ',', 'որ', 'ինչ', '-', 'որ', 'մի', 'պահ', 'նա', 'բախվում', 'էր', 'հանքարանի', 'բովանցքի', 'առաստաղին', ',', 'հետո', 'դանդաղ', 'թուլանում', 'էր', 'ու', 'մնում', 'պառկած', ',', 'դատարկված', 'ու', 'խորապես', 'բավարարված', '։', 'Բոլոր', 'գարշեփ', 'հոտերի', 'ժայթքման', 'այս', 'գործողությունը', 'իրոք', 'չափից', 'դուրս', 'հաճելի', 'էր', ',', 'չափից', 'դուրս', '...', 'Նրա', 'երևակայական', 'համաշխարհային', 'թատրոնի', 'սցենարում', 'այդ', 'համարը', 'կարծես', 'ամենասիրվածն', 'էր', ',']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "780\n",
            "790\n",
            "800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Եվ', 'երբ', 'մեր', 'թանկագին', 'Ժան', '-', 'Բատիստը', ',', 'որն', 'ի', 'վերջո', 'վերադարձել', 'էր', 'իր', 'մոտ', ',', 'պառկեց', 'ծիրանագույն', 'սրահում', '՝', 'իր', 'հարմարավետ', 'բազմոցի', 'վրա', ',', 'եթե', 'կուզեք', ',', 'ի', 'վերջո', 'հանեց', 'ճտքակոշիկները', ',', 'ծափ', 'տվեց', 'ու', 'իր', 'մոտ', 'կանչեց', 'ծառա', '–', 'ներին', ',', 'որոնք', 'անտեսանելի', 'էին', ',', 'անշոշափելի', ',', 'անլսելի', 'ու', 'հոտառությամբ', 'անորսալի', ',', 'այսինքն', '՝', 'ամբողջապես', 'երևակայական', 'ծառասերին', ',', 'ևնրանց', 'ուղարկեց', 'պահեստանոց', ',', 'որպեսզի', 'հոտերի', 'մեծ', 'գրադարանից', 'իրեն', 'բերեն', 'այս', 'կամ', 'այն', 'հատորը', ',', 'ու', 'հրամայեց', 'նրանց', 'իջնել', 'նկուղ', ',', 'որպեսզի', 'իրեն', 'խմիչք', 'բերեն', '։', 'Երևակայական', 'ծառաները', 'շտապում', 'էին', 'կատարել', 'կարգադրությունը', ',', 'և', 'Գրենույի', 'ստամոքսը', 'սեղմվում', 'էր', 'տանջալի', 'սպասման', 'ջղաձգությունից', '։', 'Նա', 'անսպասելիորեն', 'ունենում', 'էր', 'վաճառասեղանի', 'առջև', 'կանգնած', 'հարբեցողի', 'զգացում', ',', 'որին', 'տիրում', 'էր', 'սարսափը', ',', 'որ', 'ինչ', '-', 'որ', 'անհայտ', 'պատճառներով', 'Նրան', 'կհրաժարվեն', 'մատուցել', 'պատվիրած', 'օղին', '։', 'Իսկ', 'միգուցե', 'նկուղներն', 'ու', 'պահեստները', 'միանգամից', 'դատարկվել', '՞', 'էին', '։', 'Միգուցե', 'տակառների', 'գինին', 'ցնդել', '՞', 'էր', '։', 'Ինչու', '՞', 'իրեն', 'ստիպեցին', 'սպասել', '։', 'Ինչու', '՞', 'չեն', 'գալիս', '։', 'Գեղանյութընրան', 'հիմա', 'էր', 'պահանջվում', ',', 'անմիջապես', ',', 'նա', 'ծարավից', 'մահանում', 'է', ',', 'նա', 'կմեռնի', 'տեղում', ',', 'եթե', 'այն', 'չստանա', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "810\n",
            "820\n",
            "830\n",
            "840\n",
            "850\n",
            "860\n",
            "870\n",
            "880\n",
            "890\n",
            "900\n",
            "910\n",
            "920\n",
            "930\n",
            "940\n",
            "950\n",
            "960\n",
            "970\n",
            "980\n",
            "990\n",
            "1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['դրա', 'հետ', 'ոչինչ', 'չէր', 'կարող', 'անել', '։', 'Չափից', 'դուրս', 'անսպասելի', 'էր', 'բուրմունքի', 'այդ', 'հարձակումը', '։', 'Մ՝ի', 'ակնթարթ', 'մի', 'ներշնչումի', 'ակնթարթ', ',', 'որը', 'հավերժություն', 'տևեց', ',', 'նրան', 'թվաց', ',', 'որ', 'ժամանակը', 'կրկնակի', 'արագացել', 'է', 'կամ', ',', 'հակառակը', ',', 'անհետացել', ',', 'քանի', 'որ', 'դադարեց', 'հասկանալ', 'արդյոք', 'հիման', 'հիմա', '՞', 'է', ',', 'այստեղը', 'այստեղ', ',', 'ու', 'արդյոք', 'այժմը', 'անցյյա', '՞', 'չէ', ',', 'իսկ', 'այստեղը', 'այնտեղ', ',', 'այսինքն', '՝', '1753', 'թվականի', 'սեպտեմբերին', 'Փարիզում', '՝', 'Աարե', 'փողո', '–', 'ցում', ',', 'բուրմունքը', ',', 'որը', 'շիթով', 'բխում', 'էր', 'այգուց', ',', 'շիկահեր', 'աղջկա', '՞', 'բուրմունքն', 'էր', ',', 'որին', 'այն', 'ժամանակ', 'սպանեց', '։', 'Լկն', ',', 'որ', 'նա', 'այդ', 'բուրմունքը', 'կրկին', 'գտավ', 'աշխարհում', ',', 'նրա', 'աչքերը', 'լցրին', 'երանելի', 'երջանկության', 'արտասուքներով', ',', 'իսկ', 'այն', ',', 'որ', 'դա', 'կարող', 'էր', 'իրական', 'չլինել', ',', 'մահվան', 'աստիճան', 'սարսափեցրեց', 'նրան', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Լէխ', '՜', '։', 'Նա', 'ուզում', 'էր', 'տիրւսնալ', 'ւսյդ', 'բուրմունքին', '։', 'Տիրանւսլ', 'ոչ', 'այնքան', 'խենթորեն', ',', 'ինչպես', 'այն', 'ժւսմւսնակ', 'Մարե', 'փողոցի', 'վրւս', '։', 'Նա', 'ուղղակի', 'խմեց', 'այն', 'ւսղջկւս', 'հոտը', ',', 'լցրեց', 'իր', 'մեջ', 'ու', 'դրանով', 'էլ', 'կործանեց', '։', 'Ոչ', ',', 'պւստից', 'ւսյն', 'կողմ', 'գտնվող', 'աղջկւս', 'բուրմունքը', 'ցանկանում', 'էր', 'իրաւցես', 'յուրացնել', ',', 'հանել', 'նրւս', 'վրւսյից', ',', 'ինչւցես', 'մաշկը', ',', 'և', 'դւսրձնել', 'իր', 'սեւիակւս', '–', 'նությունը', '։', 'Նա', 'չգիտեր', ',', 'թե', 'դւս', 'ինչւցես', 'պետք', 'է', 'տեղի', 'ունե', '–', 'նւս', '։', 'Բայց', 'առջեում', 'ուներ', 'երկու', 'տարի', ',', 'որւցեսզի', 'սու|որեր', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1020\n",
            "1030\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['կսողան', 'վեր', '։', 'Եվ', 'պետք', 'է', 'մտածել', 'ար<UNK>ե', '՞', 'արդյոք', 'ապրանքը', 'վաճառել', 'այդ', 'խաբեբաներին', ',', 'թե', '՞', ',', 'ինչպես', 'անում', 'են', 'մնացած', 'մանր', 'արդյունաբերողները', ',', 'շրթներկի', 'բեռը', 'նավով', 'ուղարկել', 'Ջենովա', 'կամ', ',', 'օրինակ', ',', 'մասնակցել', 'Բոկերի', 'աշնանային', 'տոնավաճառին', ',', 'վտանգավոր', 'ձեռնարկում', 'է', ',', 'իհարկե', ',', 'բայց', 'հաջողության', 'դեւցքում', '՝', 'վերին', 'աստիճանի', 'եկամտաբեր', '։', 'Տիկինը', 'մանրակրկիտ', 'ձևով', 'ծանրութեթև', 'էր', 'անում', 'ւսյդ', 'տարբեր', 'հնւսրավորությունները', ',', 'համադրում', 'էր', 'դրանք', ',', 'իսկ', 'երբեմն', 'զուգակցում', 'մեկը', 'մյուսի', 'հետ', 'կամ', 'օգտագործում', 'դրանք', 'բոլորը', ',', 'իր', 'գանձերի', 'մի', 'մասը', 'վաճառում', 'էր', ',', 'մյուս', 'մասը', 'թաքցնում', ',', 'իսկ', 'երրորդով', 'ռիսկային', 'առուծախ', 'էր', 'անում', '։', 'Եվ', 'եթե', 'տեղեկություններ', 'հավաքելիս', 'նրա', 'մոտ', 'տպավորություն', 'էր', 'ստեղծվում', ',', 'որ', 'շուկան', 'գերհագեցած', 'է', 'շրթներկերով', ',', 'ու', 'տեսանելի', 'ժամանակահատվածում', 'իր', 'ապրանքի', 'պահանջարկը', 'չի', 'մեծանա', ',', 'նա', 'իր', 'ծածանվող', 'գլխաշորով', 'շտապում', 'էր', 'տուն', 'և', 'Դրյուոյին', 'հրամայում', 'արտւսդրանքը', 'վերւսմշակել', 'մւսքուր', 'բնահյութի', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1040\n",
            "1050\n",
            "1060\n",
            "1070\n",
            "1080\n",
            "1090\n",
            "1100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Գրենույը', 'սարսափեց', '։', '«', 'Իսկ', 'եթե', ',', '–', 'մտածեց', 'Նա', ',', '–', 'իսկ', 'եթե', 'այդ', 'բուրմունքը', ',', 'որին', 'տիրում', 'եմ', ',', 'վերջանա', '՞', '։', 'չէ', '՞', 'որ', 'դա', 'այնպես', 'չէ', ',', 'ինչպես', 'հիշողություններում', ',', 'որտեղ', 'բոլոր', 'հոտերն', 'անանցողիկ', 'են', '։', 'Իրականում', 'հոտը', ',', 'շփվելով', 'աշխարհի', 'հետ', ',', 'մաշվում', 'է', '։', 'Այն', 'եթերային', 'է', '։', 'Եվ', 'երբ', 'մաշվի', ',', 'այլևս', 'չի', 'լինի', 'ակունքը', ',', 'որտեղից', 'կերցրել', 'եմ', 'այն', '։', 'Եվ', 'ես', 'կմնամ', 'մերկ', ',', 'ինչպես', 'նախկինում', ',', 'և', 'ստիպված', 'կլինեմ', 'կրկին', 'ինձ', 'օգնել', 'փոխարինողնյութերով', '։', 'Ոչ', ',', 'կլինի', 'ավելի', 'վատ', ',', 'քան', 'Նախկինում', '։', 'չէ', '՞', 'որ', 'արդեն', 'ճանաչում', 'ու', 'տիրում', 'եմ', 'նրան', '՝', 'իմ', 'սեփական', 'արքայական', 'բուրմունքին', ',', 'և']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['չեմ', 'կարողնրան', 'մոռանալ', ',', 'քանի', 'որ', 'երբեք', 'չեմ', 'մոռանում', 'հոտերը', '։', 'Եվ', 'նշանակում', 'է', ',', 'որ', 'ողջ', 'կյանքումս', 'պետք', 'է', 'ւոառապեմ', 'նրա', 'մասին', 'հիշողությամբ', ',', 'ինչպես', 'արղեն', 'հիմա', 'եմ', 'տառապում', '՝', 'կանխավայելման', 'պահին', '...', 'Սյդ', 'դեպքում', 'ինչու', '՞', 'եմ', 'ընդհանրապես', 'ցանկանում', 'տիրել', 'դրան', ',', 'իմ', 'ինչին', '՞', 'է', 'պետք', '...', '»', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['ԼԼյդ', 'միտքը', 'չափազանց', 'տհաճ', 'էր', '։', 'Գրենույն', 'անչափ', 'վախեցավ', ',', 'որ', 'տիրելով', 'բուրմունքին', ',', 'որին', 'դեռչէր', 'տիրացել', ',', 'անխուսափելիորեն', 'այն', 'կրկին', 'կկորցնի', '։', 'որքան', '՞', 'երկար', 'կկարողանա', 'այն', 'պահել', '։', 'Սի', 'քանի', 'օր', '՞', '։', 'Սի', 'քանի', 'շաբաթ', '՞', '։', 'Սիգուցե', 'ողջ', 'ամիս', ',', 'եթե', 'շատ', 'խնայողաբար', 'օծվի', '։', 'Իսկ', 'հետո', '՞', '։', 'Նա', 'արդեն', 'տեսնում', 'էր', ',', 'թե', 'ինչպես', 'է', 'սրվակի', 'միջից', 'թափ', 'տալիս', 'վերջին', 'կաթիլը', ',', 'սրվակը', 'ողողում', 'գինու', 'սւղիրտով', ',', 'որւցեսզի', 'նույնիսկ', 'նվազագույն', 'մնացորդ', 'անգամ', 'չմնւս', ',', 'ու', 'տեսնում', ',', 'հոտառությամբ', 'զգում', 'է', ',', 'թե', 'ինչպես', 'է', 'իր', 'սիրելի', 'բուրմունքը', 'մեկընդմիշտ', 'ու', 'անվերադարձ', 'ցնդում', '։', 'Դա', 'կլինի', 'դանդաղ', 'մահ', ',', 'նա', 'կխեղդվի', ',', 'աստիճանաբար', ',', 'տւսնջալի', 'ցավերով', 'իրեն', 'կգոլորշացնի', 'դուրս', '՝', 'դեպի', 'գարշելի', 'աշխարհ', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Կան', 'հոտեր', ',', 'որոնք', 'պահպանվում', 'են', 'տասնամյակներ', 'շարունակ', '։', 'Մուշկով', 'շփված', 'սնդուկը', ',', 'դարչինի', 'յուղով', 'ներծծված', 'կաշվի', 'կտորը', ',', 'համպարի', 'գնդիկը', ',', 'մայրու', 'ծառից', 'զարդատուփը', 'հոտառական', 'իմաստով', 'գրեթե', 'հավերժ', 'են', 'ապրում', '։', 'Իսկ', 'մյուսները', '՝', 'սինե', '–', 'մեքի', 'յուղը', ',', 'բերգամոտը', ',', 'նարգիզի', 'լուծամզվածքն', 'ու', 'բրաբիոնը', 'և', 'ծաղկային', 'բույրերից', 'շատերը', ',', 'արդեն', 'մի', 'քանի', 'ժամ', 'անց', ',', 'եթե', 'դրանք', 'մաքուր', 'տեսքով', 'բացօթյա', 'դրվեն', ',', 'կորցնում', 'են', 'համն', 'ու', 'հոտը', '։', 'Օծանագործը', 'պայքարում', 'է', 'այդ', 'ճակատագրական', 'հանգամանքի', 'դեմ', '՝', 'չափից', 'դուրս', 'ցնդող', 'բուրմունքը', 'կաւցելով', 'կւսյունների', 'հետ', ',', 'նրանց', 'վրա', 'դնելով', 'կապանքներ', ',', 'որոնք', 'սանձում', 'են', 'նրանց', 'ազաւոության', 'ձգտումը', ',', 'իսկ', 'արվեստն', 'այն', 'է', ',', 'որ', 'կապանքները', 'չդրվեն', 'կոշտորեն', ',', 'այլ', 'ւսզատություն', 'տրամադրեն', 'կապված', 'հոտին', ',', 'բայց', ',', 'այսուհանդերձ', ',', 'նրան', 'պահելով', 'բավականաչափ', 'մոտ', ',', 'որպեսզի', 'չկարողանա', 'փախչել', '։', 'Այս', 'ճարւցիկ', 'հնարքը', 'Գրենույին', 'երկու', 'անգամ', 'հրաշալի', 'հաջողվեց', 'կատարել', 'բրաբիոնի', 'յուղի', 'հետ', ',', 'որի', 'վաղանցիկ', 'բուրմունքը', 'շղթայեց', 'չափւսզւսնց', 'փոքր', 'քանակությամբ', 'մուշկի', ',', 'վանիլի', ',', 'խունկի', 'ունոճու', 'հետ', 'և', 'հատկապես', 'դրւսնով', 'բացւսհայտեց', 'նրւս', 'հմւսյ', '–', 'քը', '։', 'չի', '՞', 'կարելի', 'արդյոք', 'նմւսնատիպ', 'մի', 'գործողություն', 'կատարել', 'աղջկա', 'բուրմունքի', 'հետ', '։', 'միթե', '՞', 'ւսնւցայմանո', '–', 'րեն', 'պետք', 'է', 'վատնել', 'բուրմունքներից', 'ամենասւսրսւս', '–', 'փեցնորը', ',', 'ամենաթանկարժեքն', 'ու', 'ւսմենւսփխրունը', '՝', 'այն', 'օգտագործելով', 'մաքուր', 'տեսքով', '։', 'Որքքսնն', '՜', 'ւսնհեթեթ', 'է', '։', 'Ինչպիսի', '՜', 'ւսպաշնորհություն', '։', 'միթե', '՞', 'ալմաստը', 'թողնում', 'են', 'չհղկված', '։', 'միթե', '՞', 'ոսկին', 'պարւսնոցին', 'են', 'կրում', 'բնակտորներով', '։', 'միթե', '՞', 'ինքը', 'Գրենույը', ',', 'ընդւսմենը', 'հոտերի', 'պրիմիտիվ', 'գող', 'է', '՝', 'նման', 'Դրյուոյին', 'ու', 'մնւսցած']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['ծաղիկներ', 'կակղեցնողներին', ',', 'թորողներին', 'ու', 'քամողներին', '։', 'միթե', '՞', 'ինքը', 'չէ', 'աշխարհի', 'մեծագույն', 'օծանագործը', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1130\n",
            "1140\n",
            "1150\n",
            "1160\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['երկրորդ', 'խորհրդականի', 'համար', ',', 'որը', ',', 'նրա', 'կարծիքով', ',', 'քաղաքացիների', 'համար', 'պարտավոր', 'է', 'զսպվածության', ',', 'քաջարիության', 'ու', 'անկոտրումության', 'օրինակ', 'դառնալ', '։', 'Դրանից', 'զատ', ',', 'նա', 'մեկն', 'էր', ',', 'որի', 'վզին', 'ոչ', 'մեկը', 'չէր', 'համարձակվի', 'փաթաթել', 'իր', 'որոշումները', 'ոչ', 'խուճապով', 'բռնկված', 'ամբոխը', ',', 'ոչ', 'առավել', 'ևս', 'մեն', '-', 'միակ', 'անւսնուն', 'տականք', '-', 'հանցագործը', '։', 'Եվ', 'այդ', 'ողջ', 'սարսափելի', 'ժամանակաընթացքում', 'Նա', 'քաղաքում', 'քչերից', 'մեկն', 'էր', ',', 'ով', 'չտրվեց', 'սարսավի', 'տենդին', 'ու', 'պահպանեց', 'սթափ', 'մտածողությունը', '։', 'Բայց', 'այդ', 'ամենը', 'տարօրինակ', 'ձևով', 'այժմ', 'փոխվել', 'էր', '։', 'Այն', 'ժամանւսկ', ',', 'երբ', 'մարդիկ', 'փողոցներում', '(', 'կարծես', 'թե', 'նրանք', 'արդեն', 'կախաղւսն', 'էին', 'բարձրացրել', 'մարդասպանին', ')', 'տոնում', 'էին', 'նրա', 'չւսրագործություննե', '–', 'րի', 'ավարտն', 'ու', 'գրեթե', 'մոռացել', 'էին', 'այն', 'չարւսբաստիկ', 'ժամանակը', ',', 'Անտուան', 'Ռիշիի', 'սրտի', 'մեջ', ',', 'ինչպես', 'մի', 'անխուսափելի', 'դժոխք', ',', 'վախ', 'էր', 'մտել', '։', 'Սկզբում', 'չէր', 'ցանկւս', '–', 'նում', 'ընդունել', ',', 'որ', 'հատկապես', 'վախն', 'էր', 'ստիպում', 'իրեն', 'հետաձգել', 'վաղուց', 'հասունացած', 'ուղևորությունները', ',', 'ավելի', 'հազվադեպ', 'քաղաք', 'դուրս', 'գալը', ',', 'այցերի', 'ու', 'խորհրդակցությունների', 'կրճատումը', ',', 'միայն', 'այն', 'ւցատճւսռով', 'որպեսզի', 'շուտ', 'տուն', 'վերադառնա', '։', 'Նա', 'երկւսր', 'ժւսմանւսկ', 'ւսրդւսրանում', 'էր', 'ինքն', 'իր', 'ւսռջև', 'զբւսղվւսծությամբ', 'ուգեր', '–', 'հոգնւսծությամբ', ',', 'բայց', ',', 'վերջիվերջո', 'խոււտու|անեց', ',', 'որ', 'որոշակիորեն', 'մտւսհոգված', 'է', ',', 'ինչւցես', 'մտահոգված', 'կլիներ', 'նրա', 'տեղում', 'գտնվող', 'յուրաքւսնչյուր', 'հւսյր', ',', 'որն', 'ուներ', 'հարսնւսցու', '-', 'դուստր', '.', 'չէ', '՞', 'որ', 'նմւսն', 'մւուսհոգությռւնը', 'սովո', '–', 'րւսկան', 'երևույթ', 'է', '...', 'Աիթե', '՞', 'ողջ', 'ւսշխւււրհով', 'մեկ', 'ւսրդեն', 'չի', 'տարածվել', 'Նրւս', 'գեղեցկության', 'մւսսին', 'փւսռքը', '։', 'Աիթե', 'չեն', 'ձգվում', 'բոլոր', 'ւցարւսնոցները', ',', 'երբ', 'կիրակի', 'օրերին', 'նրւս', 'հետ', 'եկեղեցի', 'է', 'մտնում', '։', 'Աիթե', 'Խորհրդի', 'որոշ', 'պարոններ', 'իրենց', 'կւսմ', 'իրենց', 'որդիների', 'անունից', 'արդեն', 'չեն', 'ւսկնար', '–', 'կել', 'հնարավոր', 'հավւսկնությունների', 'մւսսին', '...']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['նրբաճաշակ', 'գեղեցկությամբ', '։', 'Երբևիցե', 'նա', 'չէր', 'էլ', 'կարծել', ',', 'որ', 'Գրասում', 'Նման', 'քանակությամբ', 'չգնահատված', 'գեղեցկություն', 'կար', '։', 'Մարդասպանը', 'բացել', 'էր', 'նրա', 'աչքերը', '։', 'Մարդասպանն', 'աչքի', 'էր', 'ընկնում', 'գերազանց', 'ճաշակով', '։', 'Եվ', 'գործում', 'էր', 'համակարգված', 'ձևով', '։', 'Բավական', 'չէ', ',', 'որ', 'նրա', 'բոլոր', 'սպանություններն', 'իրականացված', 'էին', 'միատեսակ', 'ճշտակատարությամբ', ',', 'զոհերի', 'բուն', 'ընտրությունն', 'իսկ', 'մատնում', 'էր', 'գրեթե', 'մաթեմատիկական', 'հաշվարկը', '։', 'ճիշտ', 'է', ',', 'Ռիշին', 'չգիտեր', ',', 'թե', ',', 'անկեղծ', 'ասած', ',', 'ինչ', 'էր', 'մարդասպանն', 'ուզում', 'իր', 'զոհերից', ',', 'քանզի', 'չէ', '՞', 'որ', 'նա', 'չէր', 'գողացել', 'նրանց', 'գլխավոր', 'հարստությունը', '՝', 'պատանեկության', 'գեղեցկությունն', 'ու', 'հմայքը', '...', 'թե', '՞', 'գողացել', 'էր', '։', 'Համենայնդեպս', ',', 'որքան', 'էլ', 'դա', 'անհեթեթ', 'է', 'հնչում', ',', 'թվում', 'էր', ',', 'թե', 'սպանությունների', 'նպատակը', 'ոչ', 'թե', 'կործա', '–', 'նելն', 'էր', ',', 'այլ', 'խնամքով', 'հավաքածու', 'կազմելը', '։', 'Եթե', ',', 'օրինակ', ',', 'տրամաբանում', 'էր', 'Ռիշին', ',', 'բոլոր', 'զոհերին', 'ւցատկե', '–', 'րացնենք', 'ոչ', 'թե', 'որպես', 'առանձին', 'անհատներ', ',', 'այլ', 'որւցես', 'ինչ', '-', 'որ', 'բարձրագույն', 'սկզբունքի', 'մաս', ',', 'և', 'իդեալիստորեն', 'պատկերացնենք', 'նրանց', 'այդչւսփ', 'տւսրբեր', 'հատկանիշները', 'մեկ', 'միասնության', 'մեջ', 'միաձուլված', ',', 'աւցւս', 'պւստկե', '–', 'ԸԸ', ',', 'ՈՐԸ', 'բաղկացւսծ', 'էնմանատիպ', 'բազմերւսնգությունից', ',', 'ընդհանուր', 'առմամբ', ',', 'կլիներ', 'գեղեցկության', 'պատկեր', ',', 'ու', 'կախարդանքը', ',', 'որը', 'դուրս', 'էր', 'գալիս', 'նրւսնից', ',', 'կունենար', 'ոչ', 'թե', 'մարդկային', ',', 'այլ', 'աստվւսծային', 'իշխանություն', '։', '(', 'Ինչպես', 'տեսնում', 'ենք', ',', 'Ռիշին', 'լուսավորյւսլ', 'ու', 'տրւսմաբանող', 'մարդ', 'էր', ',', 'որը', 'չէր', 'վախենում', 'նույնիսկ', 'նման', 'սրբւսպիղծ', 'հետևություններ', 'անելուց', ',', 'և', 'չնւսյւսծ', 'նւս', 'մւոածում', 'էր', 'ոչ', 'թե', 'հոտւսռական', ',', 'այլ', 'օպտիկւսկան', 'կատեգորիւսներով', ',', 'այնուամենայնիվ', ',', 'շատ', 'մոտ', 'էր', 'ճշմւսրտությւււնը', ')', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Գալով', 'նման', 'սարսափեցնող', 'եզրահանգման', '՝', 'Ռիշին', ',', 'իր', 'անկողնու', 'վրա', 'գիշերային', 'վերնաշապիկով', 'նստած', ',', 'ապշում', 'էր', 'Աեփական', 'հանգստության', 'վրա', '։', 'Նա', 'այլևս', 'դողից', 'չէր', 'ցնցվում', '։', 'Անորոշ', 'վախը', ',', 'որը', 'մի', 'քանի', 'շաբաթ', 'շարունակ', 'կեղեքում', 'էրնրան', ',', 'անհետացել', 'էր', '՝', 'տեղը', 'զիջելով', 'կոնկրետ', 'վտանգի', 'գիտակցմանը', '։', 'Մնյրդասպանի', 'մտադրությունն', 'ակնհայտորեն', 'ուղղված', 'էր', 'էաուրայի', 'վրա', 'ամենասկզբից', '։', 'Իսկ', 'մնացած', 'բոլոր', 'սպանությունները', 'շրջապատն', 'էին', 'այդ', 'վերջինի', '՝', 'ավարտական', 'սպանության', '։', 'ճիշտ', 'է', ',', 'անհասկանալի', 'էր', 'մնում', ',', 'թե', 'ինչպիսի', 'նյութական', 'նպատակներ', 'են', 'հետապնդում', 'այդ', 'սպանությունները', 'և', ',', 'ընդհանրապես', ',', 'ունեն', '՞', 'դրանք', 'որևէ', 'նպատակ', '։', 'Ռայց', 'հիմնականը', ',', 'հատկապես', '՝', 'մարդասպանի', 'դասակարգված', 'գործելաոճն', 'ու', 'նրա', 'ձգտումը', 'դեպի', 'կատարյալը', ',', 'Ռիշին', 'ճիշտ', 'կռահեց', '։', 'Եվ', 'որքան', 'երկար', 'էր', 'դրա', 'վրա', 'խորհում', ',', 'այնքան', 'ավելի', 'էր', 'դուր', 'գալիս', 'նրան', 'թե*', 'մեկը', ',', 'թև', 'մյուսը', ',', 'և', 'այնքան', 'մեծ', 'հարգանք', 'էր', 'տածում', 'մարդասպանի', 'հանդեպ', ',', 'նման', 'հարգանքը', ',', 'ինչպես', 'հարթ', 'հայելու', 'մեջ', ',', 'արտացոլում', 'էրնրա', 'վերաբերմունքն', 'իր', 'հանդեպ', ',', 'չէ', '՞', 'որ', 'ոչ', 'այլ', 'ոք', ',', 'այլ', 'հենց', 'ինքը', '՝', 'Ռիշին', ',', 'իր', 'նուրբ', ',', 'վերլուծական', 'խելքով', 'ներթափանցեց', 'հակառակորդի', 'մտահղացման', 'մեջ', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1180\n",
            "1190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Դստեր', 'հետ', 'Սնտուան', 'Ռիշիի', 'մեկնումը', 'մարդկանց', 'վրա', 'թողեց', 'տարօրինակ', 'խոր', 'տպավորություն', '։', 'Նրանց', 'թվում', 'էր', ',', 'թե', 'իրենք', 'ներկա', 'են', 'գտնվում', 'զոհւսբերությւսն', 'մի', 'ինչ', '-', 'որ', 'հնադարյան', 'ծիսակատարությւսն', '։', 'Չորսբոլորը', 'խոսում', 'էին', 'միայն', 'այն', 'մասին', ',', 'որ', 'Ռիշին', 'մեկնում', 'է', 'Գրենոբլ', ',', 'այսինքն', '՝', 'մի', 'քաղաք', ',', 'որտեղ', 'վերջին', 'ժամանակներս', 'գործում', 'է', 'աղջիկներին', 'սպանող', 'հրեշը', '։', 'Մարդիկ', 'չգիտեին', 'էլ', ',', 'թե', 'ինչ', 'մտածեն', 'դրա', 'վերաբերյալ', '։', 'Ինչով', '՞', 'բացատրել', 'Ռիշիի', 'արարքը', 'դատապարտե|ի', 'թեթևատութթ', '՞', ',', 'թե', '՞', 'հիացմունքի', 'արժանի', 'խիզախությամբ', '։', 'Մարտահարեր', '՞', 'էր', 'դա', ',', 'թե', '՞', 'աստվածների', 'ողորմածությունը', 'շարժելու', 'փորձ', '։', 'Ռայց', 'Նրանց', 'տանջում', 'էր', 'աղոտ', 'կանխազգացումը', ',', 'որ', 'շիկահեր', 'ծամերով']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ռիշին', 'հասկանում', 'էր', ',', 'որ', 'նման', 'շտապողականությունը', 'որոշակիորեն', 'բարձրացնում', 'է', 'բարոնի', 'ընտանիքի', 'հետ', 'իր', 'ընտանիքի', 'միացման', 'վճարի', 'չափը', '։', 'Նա', 'շատ', 'ավելի', 'քիչ', 'կվճարեր', ',', 'եթե', 'սպասելու', 'ժամանակ', 'ունենար', '։', 'Այդ', 'դեպքում', 'բարոնը', 'ստիպված', 'կլիներ', ',', 'ինչպես', 'աղքատը', ',', 'հարուստ', 'վաճառականի', 'համաձայնությունը', 'խնդրել', 'այդ', 'գործարքի', 'համար', ',', 'չէ', '՞', 'որ', 'էաուրայի', 'գեղեցկության', 'մասին', 'փառքը', 'պիտի', 'աճի', ',', 'ինչպես', 'որ', 'Ռիշիի', 'հարստությունը', 'և', 'ինչպես', 'Ռույոնների', 'ֆինանսական', 'սնանկացումը', '։', 'Ռայց', 'լավ', ',', 'թող', 'լինի', 'այդպես', '։', 'չէ', '՞', 'որ', 'Նրա', 'հակառակորդը', 'ոչ', 'թե', 'բարոնն', 'էր', ',', 'այլ', 'անհայտ', 'մարդասպանը', '։', 'Ահա', 'թե', 'ում', 'գործին', 'պետք', 'է', 'խանգարեբ', 'Ամուսնացած', 'կինը', ',', 'ով', 'կորցրել', 'է', 'կուսությունը', 'և', 'միգուցե', 'հղի', 'է', ',', 'արդեն', 'չի', 'կարոդ', 'ներգրավվել', 'նրա', 'բացառիկ', 'հավաքածուի', 'մեջ', '։', 'Այդնախշապատկերի', 'վերջին', 'վանդակը', 'կմնա']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1200\n",
            "1210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Գրենույը', 'պահակախմբին', 'հարցրեց', ',', 'թե', 'որ', 'ճանապարհով', 'է', 'գնացել', 'երկրորդ', 'խորհրդականը', '։', 'Պահակներից', 'մեկը', 'ցույց', 'տվեց', 'դեպի', 'հյուսիս', '։', 'Իսկ', 'գուցե', 'Կաբ', '–', 'րիի', '՞', 'ուղղությամբ', '։', 'Կամ', 'գուցե', 'ուղղվել', 'է', 'դեպի', 'հարավ', '՞', 'Օրիբոյի', 'կամ', 'էա', '-', 'Նապուլի', 'ուղղությամբ', '։', 'Իհարկե', 'ոչ', ',', 'ասաց', 'պահակը', ',', 'նա', 'Աեփական', 'աչքերով', 'է', 'տեսել', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1220\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Երկու', 'ժամ', 'հետո', ',', 'երբ', 'արդեն', 'շատ', 'էր', 'մթնել', ',', 'նրանք', 'մոտեցան', '։', 'Իրենց', 'ծպտվածությունը', 'պահպանելու', 'համար', 'երեքն', 'էլ', 'փոխել', 'էին', 'հագուստները', '։', 'Երկու', 'կանայք', 'էլ', 'մուգ', 'գույնի', 'շրջազգեստներով', 'ու', 'շղարշներով', 'էին', ',', 'Ռիշին', '՝', 'սև', 'բաճկոնով', '։', 'Նա', 'իրեն', 'ներկայացնում', 'էր', 'որպես', 'Կաստեղանայից', 'եկած', 'ազնվական', ',', 'վաղը', 'ցանկանում', 'էր', 'ծովանցով', 'հասնել', 'էերինյան', 'կղզիներ', ',', 'թող', 'տերը', 'լուսաբացին', 'մոտ', 'նախաճաշ', 'պատրաստի', '։', 'Կան', '՞', 'արդյոք', 'տանն', 'այլ', 'կենվորներ', '։', 'Ոչ', ',', 'ասաց', 'տերը', ',', 'միայն', 'Նիցցայից', 'մի', 'կաշեգործի', 'ենթավարպետ', ',', 'ով', 'ախոռում', 'է', 'գիշերում', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1230\n",
            "1240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Նա', 'մի', 'կողմ', 'դրեց', 'մահակն', 'ու', 'ողջ', 'ջանասիրությամբ', 'անցավ', 'գործի', '։', 'Սկզբում', 'բացեց', 'իր', 'հետ', 'բերած', 'քաթւսնը', 'և', 'այն', 'մաքուր', 'կողմով', 'փռեց', 'սեղանի', 'ու', 'աթոռների', 'կրա', '՝', 'հետևելով', ',', 'որպեսզի', 'ճարպոտ', 'կողմին', 'չդիպչի', '–', 'Մղջ', '–', 'կա', 'շքեղ', 'բուրմունքը', ',', 'որը', 'հանկարծ', 'տաք', 'ու', 'խիտ', 'ալիքով', 'հորդեց', 'նրանից', ',', 'այս', 'անգամ', 'Գրենային', 'չհուզեց', '։', 'չէ', '՞', 'որ', 'դա', 'նրան', 'ծանոթ', 'էր', ',', 'իսկ', 'արբածության', 'աստիճան', 'վայելքն', 'ավելի', 'ուշ', 'կստանա', 'այն', 'բանից', 'հետո', ',', 'երբ', 'իրոք', 'կտիրի', 'նրան', '։', 'Այժմ', 'այն', 'որքան', 'հնարավոր', 'է', 'շատ', 'պետք', 'է', 'հավաքել', ',', 'որքան', 'հնարավոր', 'է', 'քիչ', 'արտահոսք', 'տալ', ',', 'այժմ', 'նրանից', 'պահանջվում', 'էր', 'կենտրոնացվածու', '–', 'թյուն', 'ու', 'արագաշարժություն', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['և', 'որ', 'ճակատագիրն', 'իրեն', 'տանում', 'էր', 'խճճված', ',', 'բայց', 'վերջին', 'հաշվով', 'ճիշտ', 'ուղով', ',', 'այլապես', 'միթե', '՞', 'ինքը', 'կարող', 'էր', 'հայտնվել', 'այստեղ', 'այս', 'մութ', 'սենյակում', '՝', 'իր', 'ձգտումների', 'նպատակակետի', 'մոտ', '։', 'Ինքը', ',', 'եթե', 'լավ', 'խորհրդածենք', ',', 'հիրավի', 'օրհնյալ', 'անհատ', 'է', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1260\n",
            "1270\n",
            "1280\n",
            "1290\n",
            "1300\n",
            "1310\n",
            "1320\n",
            "1330\n",
            "1340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Իսկ', 'հետո', '՞', '։', 'Ինչ', '՞', 'կանի', 'դրանից', 'հեւոո', '։', 'Չգիտեր', '։', 'Միգուցե', 'կվերադւսռնա', 'սովորակւսն', 'կյանքին', ',', 'միգուցե', 'կամուսնանա', ',', 'միգուցե', 'որդի', 'կսաղմնավորի', ',', 'միգուցե', 'ոչինչ', 'չի', 'անի', ',', 'միգուցե', 'կմեռնի', '։', 'Նա', 'բացւսրձւսկւսպես', 'անտւսրբեր', 'էր', 'դրա', 'հւսնդեպ', '։', 'Դրա', 'մասին', 'մւուսծելը', 'նրւսն', 'նույնչափ', 'անիմաստ', 'էր', 'թվում', ',', 'ինչւցես', 'ւևռածելը', 'ւսյն', 'մասին', ',', 'թե', 'ինչ', 'ւսնի', 'մահւսնալուց', 'հետո', ',', 'բնւսկանւսբւսր', ',', 'ոչինչ', '։', 'Ոչինչ', ',', 'ինչի', 'մասին', 'նա', 'կարող', 'էր', 'իմանալ', 'ւսրդեն', 'հիմւս', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['հանցագործի', 'նկատմամբ', 'պահանջվում', 'էր', 'բացառիկ', 'վերաբերմունք', '։', 'չէ', '՞', 'որ', 'չի', 'կարելի', 'նրան', '՝', 'ինչպես', 'հասարակ', 'ավազակին', ',', 'շղթայակապ', 'քարշ', 'տալ', 'հրապարակ', 'ու', 'գավազաններով', 'խփել', '։', 'Դրանում', 'ոչ', 'մի', 'սենսացիոն', 'բան', 'չէր', 'լինի', '։', 'Բոլորովին', 'այլ', 'բան', 'է', 'նրան', 'հանել', 'շքեղ', 'կառքի', 'փափուկ', 'նստատեղից', 'ու', 'մոտեցնել', 'խաչին', '.', 'դրանում', 'անհամեմատ', 'ավելի', 'շատ', 'ահագնացող', 'դաժանություն', 'կար', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Եվ', 'Պապոնը', 'դա', 'գիտեր', '։', 'Նրա', 'բռունցքները', ',', 'որոնք', 'սեղմել', 'էին', 'երկաթյա', 'ձողը', ',', 'դողացին', '։', 'Նրա', 'ուժեղ', 'ձեռքերը', 'հանկարծ', 'դարձան', 'այնքան', 'թույլ', ',', 'ծնկներն', 'այնքան', 'փափուկ', ',', 'սիրտն', 'այնքան', 'երկչոտ', ',', 'ինչպես', 'երեխայինը', '։', 'Նա', 'չէր', 'կարողանա', 'բարձրացնել', 'այդ', 'ձողը', ',', 'կյանքում', 'երբեքնրա', 'մոտ', 'ուժ', 'չէր', 'գտնվի', 'բարձրացնել', 'այն', 'ընդդեմ', 'փոքրիկ', 'անմեղ', 'մարդու', ',', 'ախ', '՜', ',', 'նա', 'վախենում', 'էր', 'այն', 'պահից', ',', 'երբ', 'նրան', 'կբերեն', 'այստեղ', 'վերև', '.', 'Նա', 'արտասվեց', ',', 'ստիպված', 'եղավ', 'հենվել', 'իր', 'մահաբեր', 'ձողի', 'վրա', ',', 'որպեսզի', 'ծնկների', 'թուլությունից', 'վայր', 'չընկնի', 'հսկայամարմին', ',', 'ուժեղ', 'Պապոնը', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1370\n",
            "1380\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Ռայց', 'դրանից', 'ոչինչ', 'չստացվեց', '։', 'Դրանից', 'ոչինչ', 'չէր', 'էլ', 'կարող', 'ստացվել', '։', 'չէ', '՞', 'որ', 'դիմակավորված', 'էր', 'աշխարհի', 'լւսվւսգույն', 'օծանելիքով', ',', 'իսկ', 'այդ', 'դիմակի', 'տակ', 'դեմք', 'չկար', ',', 'ոչինչ', 'չկար', ',', 'բացի', 'հոտի', 'համատարած', 'բացակայությունից', '։', 'Եվ', 'այդ', 'պահին', 'նա', 'անսպասելիորեն', 'վատ', 'զգաց', ',', 'որովհետև', 'տեսավ', ',', 'թե', 'ինչպես', 'են', 'կրկին', 'մառախուղները', 'վեր', 'բարձրանում', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1390\n",
            "1400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['֊', 'Այժմ', 'ամեն', 'ինչ', 'լավ', 'կլինի', '։', 'Քաղաքային', 'խորհուրդը', 'չեղյալ', 'համարեց', 'դատավճիռը', '։', 'Բոլոր', 'վկաները', 'հրաժարվեցին', 'ցուցմունքներից', '։', 'Դու', 'ազատ', 'ես', '։', 'Դու', 'կարող', 'ես', 'անել', 'ինչ', 'ուզում', 'ես', '։', 'Ռայց', 'ես', 'ուզում', 'եմ', ',', 'որ', 'դու', 'մնաս', 'ինձ', 'մոտ', '։', 'Ես', 'կորցրել', 'եմ', 'դստերս', ',', 'ես', 'ուզում', 'եմ', 'քեզ', 'որ', '–', 'դեգրել', '։', 'Դու', 'նման', 'ես', 'նրան', '։', 'Դունույնչափ', 'գեղեցիկ', 'ես', ',', 'ինչպես', 'նա', ',', 'քո', 'մազերը', ',', 'քո', 'շուրթերը', ',', 'քո', 'ձեռքը', '...', 'Ես', 'ողջ', 'ժամանակ', 'բռնել', 'էի', 'քո', 'ձեռքից', ',', 'դու', 'այնպիսի', 'ձեռք', 'ունես', ',', 'ինչպիսին', 'նրանն', 'էր', '։', 'Իսկ', 'երբ', 'նայում', 'եմ', 'քո', 'աչքերին', ',', 'թվում', 'է', ',', 'որնա', 'է', 'ինձ', 'նայում', '։', 'Դու', 'նրա', 'եղբայրն', 'ես', ',', 'և', 'ես', 'ուզում', 'եմ', ',', 'որ', 'դառնաս', 'իմ', 'որդին', ',', 'իմ', 'ուրախությունը', ',', 'իմ', 'հպարտությունը', ',', 'իմ', 'ժառանգորդը', '։', 'Քո', 'ծնողները', 'դեռ', 'ողղ', '՞', 'են', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['֊', 'Նշանակում', 'է', '՝', 'դու', 'համաձայն', 'ես', 'դառնալ', 'իմ', 'որդին', ',', '-', 'մի', 'շեչով', 'ասաց', 'նա', 'ու', 'վեր', 'թռավնստարանի', 'վրայից', ',', 'որպեսզի', 'նստի', 'մահճակալի', 'եզրին', 'և', 'Դրենույի', 'մյուս', 'ձեռքը', 'սեղմի', '։', '–', 'Համաձձան', '՞', 'ես', '։', 'Համաձձան', '՞', 'ես', '։', 'Դու', 'ցանկանու', '՞', 'ես', ',', 'որ', 'ես', 'քո', 'հայրը', 'դառնամ', '։', 'Ոչինչ', 'մի', 'ասա', '։', 'Մի', 'խոսիր', '։', 'Դու', 'դեռ', 'շատ', 'թույլ', 'ես', ',', 'որպեսզի', 'խոսես', '։', 'Միայն', 'գլխով', 'արա', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1410\n",
            "1420\n",
            "1430\n",
            "1440\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/language.py:1014: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
            "Words: ['Եվ', 'մեկ', 'այլ', 'անգամ', ',', 'երբ', 'արդեն', 'Բուրգունդիայում', 'էր', ',', 'նրա', 'մտքով', 'անցավ', '.', '«', 'Երբ', 'ես', 'կանգնած', 'էի', 'այգու', 'քարե', 'պատից', 'այս', 'կողմ', ',', 'որտեղ', 'խաղում', 'էր', 'շիկահեր', 'աղջիկը', ',', 'և', 'ինձ', 'էր', 'հասնում', 'նրա', 'բուրմունքը', '...', 'կամ', 'ավելի', 'շուտ', 'բուրմունքի', 'խոստումը', ',', 'քանի', 'որ', 'Նրա', 'ավելի', 'ուշ', 'բուրմունքը', 'դեռ', 'ընդհանրապես', 'գոյություն', 'չուներ', ',', '֊', 'միգուցե', 'այն', ',', 'ինչը', 'զգացի', 'այն', 'ժամանակ', ',', 'նման', 'էր', 'ւսյն', 'բանին', ',', 'ինչը', 'մարդիկ', 'զգում', 'էին', 'հրաւցարակում', ',', 'երբ', 'ես', 'նրանց', 'հեղեղեցի', 'իմ', 'օծանելիքով', '...', '–', 'Բայց', 'նա', 'անմիջապես', 'դեն', 'նետեց', 'այդ', 'միտքը', '։', '–', 'Ոչ', ',', 'դա', 'մի', 'այլ', 'բան', 'էր', '։', 'չէ', '՞', 'որ', 'գիտեի', ',', 'որ', 'ուզում', 'եմ', 'բուրմունքին', 'տիրանալ', ',', 'այլ', 'ոչ', 'աղջկան', '։', 'Իսկ', 'այդ', 'մարդիկ', 'մտածում', 'էին', ',', 'որ', 'իրենք', 'հրապուրված', 'են', 'ինձնով', ',', 'մինչդեռ', 'այն', ',', 'ինչով', 'նրանք', 'իրոք', 'հրապուրված', 'էին', ',', 'նրանց', 'համար', 'մնաց', 'գաղտնիք', '»', '։']\n",
            "Entities: []\n",
            "  doc = self._ensure_doc(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1450\n",
            "1460\n",
            "1470\n",
            "1480\n",
            "1490\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/743a1a57a37c42d8b585/?dl=1\n",
        "!mv index.html?dl=1 Parfum_Armenian_uncorrected.txt\n"
      ],
      "metadata": {
        "id": "D-Qu1FP8mbO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dealing with Armenian OCR output with line breaks (is it correct?)\n",
        "\n",
        "FName = 'Parfum_Armenian_uncorrected.txt'\n",
        "FNameOut = 'Parfum_Armenian.txt'\n",
        "\n",
        "FIn = open(FName, 'r')\n",
        "FOut = open(FNameOut, 'w')\n",
        "\n",
        "for SLine in FIn:\n",
        "    SLine = SLine.strip()\n",
        "    if SLine == '': \n",
        "        FOut.write('\\n\\n')\n",
        "        continue\n",
        "    if SLine[-1] == '-':\n",
        "        SLine2write = SLine[:-1]\n",
        "        FOut.write(SLine2write)\n",
        "        continue\n",
        "\n",
        "    FOut.write(SLine + ' ')\n",
        "FOut.flush()\n",
        "\n"
      ],
      "metadata": {
        "id": "tstyqM_cph11"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parseFile('Parfum_Armenian.txt', 'Parfum_Armenian.vert.txt', nlp_hy)"
      ],
      "metadata": {
        "id": "Gf-OW1n_rbue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wc Parfum_Armenian.vert.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpr4kEppuIj_",
        "outputId": "350811bb-5a79-47cb-b826-472238040237"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  83828  251460 2055080 Parfum_Armenian.vert.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DWiki = {}\n",
        "with open(\"hywiki-20221101-pages-articles-v03.vert\", 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.rstrip()\n",
        "        try:\n",
        "            DWiki[line] +=1\n",
        "        except:\n",
        "            DWiki[line] = 1\n"
      ],
      "metadata": {
        "id": "5TJS8qstj_5I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DText = {}\n",
        "with open(\"Parfum_Armenian.vert.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.rstrip()\n",
        "        try:\n",
        "            DText[line] +=1\n",
        "        except:\n",
        "            DText[line] = 1\n"
      ],
      "metadata": {
        "id": "exBjjf9rkMxJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### checking if there is a frequency difference for an entry"
      ],
      "metadata": {
        "id": "lNz9tWIKuy0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DFreqDiff = {} # dictionary of frequency differences\n",
        "lenWiki = 2735468\n",
        "lenText = 83829\n",
        "c = 0\n",
        "for key, val in sorted(DText.items(), key=lambda item: item[1], reverse=True):\n",
        "    c+=1\n",
        "    valText = val + 1\n",
        "    relText = valText / lenText\n",
        "    try:\n",
        "        valWiki = DWiki[key] + 1\n",
        "    except:\n",
        "        valWiki = 1\n",
        "    relWiki = valWiki / lenWiki\n",
        "\n",
        "    diffValue = relText / relWiki\n",
        "    DFreqDiff[key] = diffValue\n"
      ],
      "metadata": {
        "id": "ewfn2ngSu6sK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fOut = open('Parfum_Armenian-freq-diff.txt', 'w')\n",
        "for key, val in sorted(DFreqDiff.items(), key=lambda item: item[1], reverse=True):\n",
        "    try:\n",
        "        frqText = DText[key] + 1\n",
        "    except:\n",
        "        frqText = 1\n",
        "\n",
        "    try:\n",
        "        frqWiki = DWiki[key] + 1\n",
        "    except:\n",
        "        frqWiki = 1\n",
        "    fOut.write(f'{key}\\t{val}\\t{frqText}\\t{frqWiki}\\n')\n",
        "fOut.flush()"
      ],
      "metadata": {
        "id": "v3TNjk49xxPC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat texts-vert/* >text-vert-all.vert.txt"
      ],
      "metadata": {
        "id": "rBODLGvH0_Xe"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wc text-vert-all.vert.txt"
      ],
      "metadata": {
        "id": "vGRyIth63pks",
        "outputId": "cabc138a-fa01-4f87-aef8-e1a37c64c97f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 112723  338169 3062358 text-vert-all.vert.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DText2 = {}\n",
        "with open(\"text-vert-all.vert.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.rstrip()\n",
        "        try:\n",
        "            DText2[line] +=1\n",
        "        except:\n",
        "            DText2[line] = 1"
      ],
      "metadata": {
        "id": "mxkmuk223eEr"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DFreqDiff2 = {} # dictionary of frequency differences\n",
        "lenWiki = 2735468\n",
        "lenText = 112723\n",
        "c = 0\n",
        "for key, val in sorted(DText2.items(), key=lambda item: item[1], reverse=True):\n",
        "    c+=1\n",
        "    valText = val + 1\n",
        "    relText = valText / lenText\n",
        "    try:\n",
        "        valWiki = DWiki[key] + 1\n",
        "    except:\n",
        "        valWiki = 1\n",
        "    relWiki = valWiki / lenWiki\n",
        "\n",
        "    diffValue = relText / relWiki\n",
        "    DFreqDiff2[key] = diffValue\n"
      ],
      "metadata": {
        "id": "4XnwSDmF3j-D"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fOut = open('text-vert-all-freq-diff.txt', 'w')\n",
        "for key, val in sorted(DFreqDiff2.items(), key=lambda item: item[1], reverse=True):\n",
        "    try:\n",
        "        frqText = DText2[key] + 1\n",
        "    except:\n",
        "        frqText = 1\n",
        "\n",
        "    try:\n",
        "        frqWiki = DWiki[key] + 1\n",
        "    except:\n",
        "        frqWiki = 1\n",
        "    fOut.write(f'{key}\\t{val}\\t{frqText}\\t{frqWiki}\\n')\n",
        "fOut.flush()"
      ],
      "metadata": {
        "id": "6gKv0TaC39NL"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}
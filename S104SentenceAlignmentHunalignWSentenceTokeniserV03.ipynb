{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/corpustools/blob/main/S104SentenceAlignmentHunalignWSentenceTokeniserV03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments with alignment tools"
      ],
      "metadata": {
        "id": "hhDdKnVbT6Aa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "resources / corpora:\n",
        "https://heibox.uni-heidelberg.de/d/d65daff8341e467c82b1/\n",
        "\n",
        "also: parallel corpora \n",
        "https://heibox.uni-heidelberg.de/d/f1d52a90e86c4c688029/\n",
        "\n",
        "\n",
        "Instructions are here:\n",
        "https://github.com/danielvarga/hunalign\n"
      ],
      "metadata": {
        "id": "iM-SQNgcjh6t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wTTt2yCTo7U"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/danielvarga/hunalign.git\n",
        "%cd hunalign/src/hunalign\n",
        "!pwd\n",
        "!make\n",
        "%cd /content/hunalign/\n",
        "!pwd\n",
        "!src/hunalign/hunalign data/hu-en.stem.dic examples/demo.hu.stem examples/demo.en.stem -hand=examples/demo.manual.ladder -text > /tmp/align.txt\n",
        "!head --lines=5 /tmp/align.txt\n",
        "%cd /content/\n",
        "!pwd\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# detecting sentence boundaries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n"
      ],
      "metadata": {
        "id": "k0QrGqwEiDBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, os, sys\n",
        "def sentTokenizer(FIn, FOut):\n",
        "    for SPara in FIn:\n",
        "        SPara = SPara.strip()\n",
        "        if SPara == '': FOut.write('\\n')\n",
        "        all_sent = sent_tokenize(SPara)\n",
        "        for SSent in all_sent:\n",
        "            FOut.write(SSent + '\\n')\n",
        "    FOut.flush()\n",
        "\n",
        "def sentTokenizer_hy(FIn, FOut):\n",
        "    for SPara in FIn:\n",
        "        SPara = SPara.strip()\n",
        "        if SPara == '': \n",
        "            FOut.write('\\n')\n",
        "            continue\n",
        "        all_sent = sent_tokenize_hy(SPara)\n",
        "        for SSent in all_sent:\n",
        "            FOut.write(SSent + '\\n')\n",
        "    FOut.flush()\n",
        "\n",
        "\n",
        "def sent_tokenize_hy(SPara):\n",
        "    all_sent_hy = re.split('[:;]', SPara)\n",
        "    if isinstance(all_sent_hy, list):\n",
        "        return all_sent_hy\n",
        "    else:\n",
        "        return []\n",
        "    \n",
        "def sent_tokenize_en(SPara):\n",
        "    all_sent_hy = re.split('[\\.\\!\\?]', SPara)\n",
        "    if isinstance(all_sent_hy, list):\n",
        "        return all_sent_hy\n",
        "    else:\n",
        "        return []\n",
        "\n"
      ],
      "metadata": {
        "id": "55c2xspMiGTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing automatically uploaded files"
      ],
      "metadata": {
        "id": "3V9Imvq9_Ond"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optional : try with automatically downloaded files\n",
        "\n",
        "# downloading files - en de\n",
        "!wget https://heibox.uni-heidelberg.de/f/dc489828a2d642bfba82/?dl=1\n",
        "!mv index.html?dl=1 ab-t01-en.txt\n",
        "# English file\n",
        "!wget https://heibox.uni-heidelberg.de/f/e7e542d75d7644d1857d/?dl=1\n",
        "!mv index.html?dl=1 ab-t01-de.txt\n",
        "\n",
        "\n",
        "FIn = open('ab-t01-en.txt', 'r')\n",
        "FOut = open('ab-t01s-en.txt', 'w')\n",
        "# FIn = open('ab-t01-de.txt', 'r')\n",
        "# FOut = open('ab-t01s-de.txt', 'w')\n",
        "sentTokenizer(FIn, FOut)\n",
        "\n",
        "FIn = open('ab-t01-de.txt', 'r')\n",
        "FOut = open('ab-t01s-de.txt', 'w')\n",
        "sentTokenizer(FIn, FOut)\n",
        "\n",
        "# !./hunalign/src/hunalign/hunalign -text -realign -autodict=de2enAutodict.txt ./hunalign/data/null.dic ab-t01-en.txt ab-t01-de.txt > ab-t01-en2de.tsv\n",
        "!./hunalign/src/hunalign/hunalign -text -realign -autodict=de2enAutodict.txt ./hunalign/data/null.dic ab-t01s-en.txt ab-t01s-de.txt > ab-t01s-en2de.tsv"
      ],
      "metadata": {
        "id": "OYDn_STYYa1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload two files from Moodle:\n",
        "- vaccination-safety-t101-de.txt\n",
        "- vaccination-safety-t101-hy.txt"
      ],
      "metadata": {
        "id": "4mP0qWlq9BY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optional \n",
        "!head --lines=20 vaccination-safety-t101-de.txt"
      ],
      "metadata": {
        "id": "HWcpViE4ecG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optional \n",
        "!head --lines=20 vaccination-safety-t101-hy.txt"
      ],
      "metadata": {
        "id": "65s_UZhweilW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optional\n",
        "FIn1 = open('vaccination-safety-t101-hy.txt')\n",
        "FIn2 = open('vaccination-safety-t101-de.txt')\n",
        "FOUt1 = open('vaccination-safety-t101sent-hy.txt', 'w')\n",
        "FOut2 = open('vaccination-safety-t101sent-de.txt', 'w')\n",
        "sentTokenizer_hy(FIn1, FOUt1)\n",
        "sentTokenizer(FIn2, FOut2)\n",
        "# you can edit dictionaries and try realigning tests if results are not good, like in this example:\n",
        "!./hunalign/src/hunalign/hunalign -text -realign -autodict=hy2deAutodict2.txt ./hunalign/data/null.dic /content/vaccination-safety-t101sent-hy.txt /content/vaccination-safety-t101sent-de.txt > vaccination-safety-t101-hy-de.txt\n",
        "%cd /content/\n"
      ],
      "metadata": {
        "id": "i35YQ9WTZilz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S01LemmatizationEnHyV01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJ979MZOrgADO+Zjaxf+DX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/corpustools/blob/main/S01LemmatizationEnHyV01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1dRDzwnNTst"
      },
      "source": [
        "# Morphological analysis for English and Armenian\n",
        "\n",
        "We will create a workflow for analysing English and Armenian texts\n",
        "\n",
        "For English we will use the TreeTagger \n",
        "\n",
        "For Armenian we will use the git repository with Armenian morphological analyser: \n",
        "https://github.com/timarkh/uniparser-grammar-eastern-armenian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nX7LqgRytTb"
      },
      "source": [
        "# importing python libraries\n",
        "import os, re, sys"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HjwCMIqsztj"
      },
      "source": [
        "## English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp8FKqOANJ6A"
      },
      "source": [
        "# installing TreeTagger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUlWsn6UmgZb"
      },
      "source": [
        "%%bash\n",
        "mkdir treetagger\n",
        "cd treetagger\n",
        "# Download the tagger package for your system (PC-Linux, Mac OS-X, ARM64, ARMHF, ARM-Android, PPC64le-Linux).\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/tree-tagger-linux-3.2.4.tar.gz\n",
        "tar -xzvf tree-tagger-linux-3.2.4.tar.gz\n",
        "# Download the tagging scripts into the same directory.\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/tagger-scripts.tar.gz\n",
        "gunzip tagger-scripts.tar.gz\n",
        "# Download the installation script install-tagger.sh.\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/install-tagger.sh\n",
        "# Download the parameter files for the languages you want to process.\n",
        "# list of all files (parameter files) https://cis.lmu.de/~schmid/tools/TreeTagger/#parfiles\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/english.par.gz\n",
        "sh install-tagger.sh\n",
        "cd ..\n",
        "sudo pip install treetaggerwrapper\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VR2d5n_mo8R"
      },
      "source": [
        "%%bash\n",
        "wget https://heibox.uni-heidelberg.de/f/95a3875771c040db959a/?dl=1\n",
        "mv index.html?dl=1 humanrights02.txt\n",
        "\n",
        "wget https://heibox.uni-heidelberg.de/f/cdf240db84ca4718b718/?dl=1\n",
        "mv index.html?dl=1 en1984.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMxbGb2-m58C"
      },
      "source": [
        "!head --lines=20 humanrights02.txt\n",
        "!wc humanrights02.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HYUJAY5rAte",
        "outputId": "2236c7f5-9aae-454f-ff28-7bfb5db72810",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!./treetagger/cmd/tree-tagger-english en1984.txt >en1984_vert.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\treading parameters ...\n",
            "\ttagging ...\n",
            "121000\t finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PNvmwwVrwyn"
      },
      "source": [
        "!head --lines=20 en1984_vert.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7stVR-G1rFat"
      },
      "source": [
        "!./treetagger/cmd/tree-tagger-english humanrights02.txt >humanrights02_vert.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf0ZClfLrZd3"
      },
      "source": [
        "!head --lines=20 humanrights02_vert.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC5cBgvas4vQ"
      },
      "source": [
        "## Armenian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW7HUG3TQzjU",
        "outputId": "8494c5a9-d984-4ce4-e327-18ecd520bac2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# installing Armenian morphological analyser\n",
        "!git clone https://github.com/timarkh/uniparser-grammar-eastern-armenian"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'uniparser-grammar-eastern-armenian'...\n",
            "remote: Enumerating objects: 181, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 181 (delta 12), reused 40 (delta 12), pack-reused 141\u001b[K\n",
            "Receiving objects: 100% (181/181), 52.66 MiB | 13.50 MiB/s, done.\n",
            "Resolving deltas: 100% (78/78), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgrVLqTTStuN",
        "outputId": "a7ee5090-13d6-4cb1-d8d7-c23171d5f63f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Python classes\n",
        "!pip3 install uniparser-eastern-armenian"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting uniparser-eastern-armenian\n",
            "  Downloading uniparser_eastern_armenian-2.1.2-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 4.3 MB/s \n",
            "\u001b[?25hCollecting uniparser-morph>=2.2.0\n",
            "  Downloading uniparser_morph-2.6.4-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from uniparser-eastern-armenian) (5.9.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->uniparser-eastern-armenian) (3.8.1)\n",
            "Installing collected packages: uniparser-morph, uniparser-eastern-armenian\n",
            "Successfully installed uniparser-eastern-armenian-2.1.2 uniparser-morph-2.6.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bALlnO8gY4cF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4164faca-e19f-40f2-af8c-6eb0de16342d"
      },
      "source": [
        "# disambiguation\n",
        "!sudo apt-get install cg3"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libcg3-1\n",
            "The following NEW packages will be installed:\n",
            "  cg3 libcg3-1\n",
            "0 upgraded, 2 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 339 kB of archives.\n",
            "After this operation, 1,100 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libcg3-1 amd64 1.0.0~r12254-1ubuntu3 [229 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 cg3 amd64 1.0.0~r12254-1ubuntu3 [110 kB]\n",
            "Fetched 339 kB in 2s (210 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libcg3-1:amd64.\n",
            "(Reading database ... 155676 files and directories currently installed.)\n",
            "Preparing to unpack .../libcg3-1_1.0.0~r12254-1ubuntu3_amd64.deb ...\n",
            "Unpacking libcg3-1:amd64 (1.0.0~r12254-1ubuntu3) ...\n",
            "Selecting previously unselected package cg3.\n",
            "Preparing to unpack .../cg3_1.0.0~r12254-1ubuntu3_amd64.deb ...\n",
            "Unpacking cg3 (1.0.0~r12254-1ubuntu3) ...\n",
            "Setting up libcg3-1:amd64 (1.0.0~r12254-1ubuntu3) ...\n",
            "Setting up cg3 (1.0.0~r12254-1ubuntu3) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.5) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5-Y0JQES0oT"
      },
      "source": [
        "from uniparser_eastern_armenian import EasternArmenianAnalyzer\n",
        "a = EasternArmenianAnalyzer()\n",
        "analyses = a.analyze_words('Ձևաբանություն')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1gZH7s3XfMR",
        "outputId": "b66517f9-3ac0-4ec3-96e7-90191186fa38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for ana in analyses:\n",
        "    print(ana.wf, ana.lemma, ana.gramm, ana.gloss, ana.stem, ana.subwords, ana.wfGlossed, ana.otherData)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ձևաբանություն ձեւաբանություն N,inanim,sg,nom,nonposs morphology ձևաբանություն. [] ձևաբանություն [('trans_en', 'morphology')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nonexisting word\n",
        "analyses2 = a.analyze_words('Ձևաբայու')"
      ],
      "metadata": {
        "id": "NjugFV12tbrz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ana2 in analyses2:\n",
        "    if ana2.lemma:\n",
        "      print(ana2.wf, ana2.lemma, ana2.gramm, ana2.gloss, ana2.stem, ana2.subwords, ana2.wfGlossed, ana2.otherData)\n",
        "    else:\n",
        "      print(ana2.wf, ana2.wf, \"N\", \"x\", ana2.stem, ana2.subwords, ana2.wfGlossed, ana2.otherData)"
      ],
      "metadata": {
        "id": "nB0YgGgUtg78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# which fields we have in analysis:"
      ],
      "metadata": {
        "id": "HmBpgm-Bt1jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqmYRwfBlbNo"
      },
      "source": [
        "dir(ana)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ddvunxGTXQf"
      },
      "source": [
        "analyses = a.analyze_words([['և'], ['Ես', 'սիրում', 'եմ', 'քեզ', ':']],\n",
        "                           format='xml')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuOyUBXmTbep",
        "outputId": "6de2d309-5975-4912-d659-cef6a97ca133",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for ana in analyses:\n",
        "    print(str(ana))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<w><ana lex=\"եւ\" gr=\"CONJ\" parts=\"և\" gloss=\"and\" trans_en=\"and, too, either\"></ana>և</w>']\n",
            "['<w><ana lex=\"ես\" gr=\"PRON,S,hum,sg,nom\" parts=\"ես\" gloss=\"me\" trans_en=\"I\"></ana><ana lex=\"է\" gr=\"V,intr,prs,sg,2\" parts=\"ե-ս\" gloss=\"be-PRS.2SG\" trans_en=\"be\"></ana>Ես</w>', '<w><ana lex=\"սիրել\" gr=\"V,tr,cvb,ipfv\" parts=\"սիր-ում\" gloss=\"love-CVB.IPFV\" trans_en=\"love, have a passion/an affection for, like\"></ana>սիրում</w>', '<w><ana lex=\"է\" gr=\"V,intr,prs,sg,1\" parts=\"ե-մ\" gloss=\"be-PRS.1SG\" trans_en=\"be\"></ana>եմ</w>', '<w><ana lex=\"դու\" gr=\"PRON,S,hum,sg,dat\" parts=\"քեզ\" gloss=\"thou\" trans_en=\"you, thou\"></ana>քեզ</w>', '<w><ana lex=\"\" gr=\"\" parts=\"\" gloss=\"\"></ana>:</w>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWxToCMSTyL6"
      },
      "source": [
        "analyses = a.analyze_words(['Ձևաբանություն', [['և'], ['Ես', 'սիրում', 'եմ', 'քեզ', ':']]],\n",
        "                           format='json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxEQfFZvT1KM",
        "outputId": "d710d1fd-adbc-471f-bc40-f75968527b93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for ana in analyses:\n",
        "    print(str(ana))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'wf': 'Ձևաբանություն', 'lemma': 'ձեւաբանություն', 'gramm': ['N', 'inanim', 'sg', 'nom', 'nonposs'], 'wfGlossed': 'ձևաբանություն', 'gloss': 'morphology', 'trans_en': 'morphology'}]\n",
            "[[[{'wf': 'և', 'lemma': 'եւ', 'gramm': ['CONJ'], 'wfGlossed': 'և', 'gloss': 'and', 'trans_en': 'and, too, either'}]], [[{'wf': 'Ես', 'lemma': 'ես', 'gramm': ['PRON', 'S', 'hum', 'sg', 'nom'], 'wfGlossed': 'ես', 'gloss': 'me', 'trans_en': 'I'}, {'wf': 'Ես', 'lemma': 'է', 'gramm': ['V', 'intr', 'prs', 'sg', '2'], 'wfGlossed': 'ե-ս', 'gloss': 'be-PRS.2SG', 'trans_en': 'be'}], [{'wf': 'սիրում', 'lemma': 'սիրել', 'gramm': ['V', 'tr', 'cvb', 'ipfv'], 'wfGlossed': 'սիր-ում', 'gloss': 'love-CVB.IPFV', 'trans_en': 'love, have a passion/an affection for, like'}], [{'wf': 'եմ', 'lemma': 'է', 'gramm': ['V', 'intr', 'prs', 'sg', '1'], 'wfGlossed': 'ե-մ', 'gloss': 'be-PRS.1SG', 'trans_en': 'be'}], [{'wf': 'քեզ', 'lemma': 'դու', 'gramm': ['PRON', 'S', 'hum', 'sg', 'dat'], 'wfGlossed': 'քեզ', 'gloss': 'thou', 'trans_en': 'you, thou'}], [{'wf': ':', 'lemma': '', 'gramm': [], 'wfGlossed': '', 'gloss': ''}]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWNEo_RwUcSB"
      },
      "source": [
        "# analysis with disambiguation\n",
        "analyses = a.analyze_words(['Ես', 'սիրում', 'եմ', 'քեզ'], disambiguate=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRb9dCQoUdJc",
        "outputId": "93fddc2e-c477-4f38-d904-1729c81f534a"
      },
      "source": [
        "for ana in analyses:\n",
        "    if len(ana) > 1: tab = \"  \"\n",
        "    else: tab = \"\"\n",
        "    for wfo in ana:\n",
        "        print(tab, wfo.wf, wfo.lemma, wfo.gramm, wfo.gloss)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Ես ես PRON,S,hum,sg,nom me\n",
            "   Ես է V,intr,prs,sg,2 be-PRS.2SG\n",
            " սիրում սիրել V,tr,cvb,ipfv love-CVB.IPFV\n",
            " եմ է V,intr,prs,sg,1 be-PRS.1SG\n",
            " քեզ դու PRON,S,hum,sg,dat thou\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vzlcQSQlw55"
      },
      "source": [
        "print(type(wfo))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0P_zV02ZxQ8"
      },
      "source": [
        "dir(wfo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzINXeL7s9EL"
      },
      "source": [
        "# downloading and analysing texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJzA4Fg9tCaH"
      },
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/e0bfae444a5a4c76957b/?dl=1\n",
        "!mv index.html?dl=1 hy1984.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOXdvc_9uv0J"
      },
      "source": [
        "FInText = open('hy1984.txt','r')\n",
        "FOutText = open('hy1984_vert.txt','w')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdxZiI7ZyLGW"
      },
      "source": [
        "for SLine in FInText:\n",
        "    SLine = SLine.strip()\n",
        "    ListOfWords = re.split('[ ,\\.:;\\!\\(\\)\\\"\\[\\]՞՝«»\\-\\—՝։\\։]+', SLine) # tokenize: split on white spaces and punctuation\n",
        "    # if len(ListOfWords) > 0: FOutText.write(str(ListOfWords) + '\\n')\n",
        "    analyses = a.analyze_words(ListOfWords, disambiguate=False)\n",
        "    FOutText.write('<p>\\n')\n",
        "    for ana in analyses:\n",
        "        # for wfo in ana:\n",
        "        # how to type all variants + disambiguate ?\n",
        "        for wfo in ana:\n",
        "          # wfo = ana[0]\n",
        "          FOutText.write(wfo.wf + '\\t' + wfo.gramm + '\\t' + wfo.lemma + '\\t' + wfo.gloss + '\\n')\n",
        "          #    FOutText.write(wfo.wf + '\\t' + wfo.gramm + '\\t' + wfo.lemma + '\\t' + wfo.gloss + '\\n')\n",
        "    FOutText.write('</p>\\n')\n",
        "FOutText.flush()"
      ],
      "execution_count": 27,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/corpustools/blob/main/S101lemHYv202511RB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rule-based lemmatization for Armenian\n",
        "\n",
        "\n",
        "If downloading from the UN website, the link aboout pdf2txt conversion is https://chatgpt.com/share/68bbeed8-0d08-800e-8418-b0816c6393d4\n",
        "\n"
      ],
      "metadata": {
        "id": "j2SZAuuE1441"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing packages\n",
        "\n",
        "- gensim word2vec for similarity-based disambiguation\n",
        "- Eastern Armenian Analyser from https://github.com/timarkh/uniparser-grammar-eastern-armenian , which outputs translations into English\n",
        "\n",
        "\n",
        "Note:  please RESTART THE SESSION if requested (gensim needs another version of the default libraries).\n",
        "\n",
        "After this, just continue to run the next cell\n",
        "\n",
        "Note: running these cells can take some time (~ 1 or 2 min altogether).\n",
        "\n"
      ],
      "metadata": {
        "id": "PGF35NFNxe-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installing gensim\n",
        "!pip install gensim\n"
      ],
      "metadata": {
        "id": "VHnyNktq4_w4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# installing Armenian morphological analyser\n",
        "!git clone https://github.com/timarkh/uniparser-grammar-eastern-armenian\n",
        "# Python classes\n",
        "!pip3 install uniparser-eastern-armenian\n",
        "# disambiguation\n",
        "!sudo apt-get install cg3"
      ],
      "metadata": {
        "id": "jF1Sa8dm3WFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing python libraries\n",
        "import os, re, sys\n",
        "\n",
        "from gensim.models import Word2Vec # The word2vec model class\n",
        "import gensim.downloader as api # Allows us to download some free training data\n",
        "\n",
        "\n",
        "from uniparser_eastern_armenian import EasternArmenianAnalyzer\n",
        "a = EasternArmenianAnalyzer()\n",
        "analyses = a.analyze_words('Ձևաբանություն')\n",
        "for ana in analyses:\n",
        "    print(ana.wf, ana.lemma, ana.gramm, ana.gloss, ana.stem, ana.subwords, ana.wfGlossed, ana.otherData)"
      ],
      "metadata": {
        "id": "dRFuA6sN3Z62",
        "outputId": "b9d849c0-5d78-4f8c-ad93-4ee21b6cd0b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ձևաբանություն ձեւաբանություն N,inanim,sg,nom,nonposs morphology ձևաբանություն. [] ձևաբանություն [('trans_en', 'morphology')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading example file\n",
        "Example: Universal declaration of human rights"
      ],
      "metadata": {
        "id": "FkA0GLEu5htD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "wget https://heibox.uni-heidelberg.de/f/7a091a3c0372428a9aa2/?dl=1\n",
        "mv index.html?dl=1 udhr-all-languages.zip\n",
        "unzip udhr-all-languages.zip"
      ],
      "metadata": {
        "id": "kvSipdQ7yz6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "# Downloading UDHR\n",
        "# !wget https://unicode.org/udhr/assemblies/udhr_txt.zip\n",
        "# Alternatively, downloading from the UN website, converting pdf to txt\n",
        "# the link aboout pdf2txt conversion: https://chatgpt.com/share/68bbeed8-0d08-800e-8418-b0816c6393d4\n",
        "# !rm --recursive udhr-all-languages\n",
        "\n",
        "# delete lines which are not translations in some files (hy)\n",
        "# delete between lines $a and $b inclusive\n",
        "a=9\n",
        "b=21\n",
        "awk -v m=$a -v n=$b 'm <= NR && NR <= n {next} {print}' < /content/udhr-all-languages/udhr_hye.txt >/content/udhr-all-languages/udhr_hye_v02.txt\n",
        "\n",
        "a=1\n",
        "b=6\n",
        "awk -v m=$a -v n=$b 'm <= NR && NR <= n {next} {print}' < /content/udhr-all-languages/udhr_hye_v02.txt >/content/udhr-all-languages/udhr_hye_v03.txt\n",
        "\n",
        "\n",
        "\n",
        "# put paragraph tags\n",
        "# awk '{print \"<p>\\n\"$0 ; print \"</p>\"}' udhr/udhr_hye2.txt >udhr/udhr_hye_v03.txt\n",
        "\n",
        "\n",
        "cp /content/udhr-all-languages/udhr_hye_v03.txt /content/udhr_hye_v03.txt\n",
        "head --lines=10 /content/udhr_hye_v03.txt\n"
      ],
      "metadata": {
        "id": "gWOntGq42JvQ",
        "outputId": "1903a6cc-b564-427d-9e81-c0341804a06f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ՄԱՐԴՈՒ ԻՐԱՎՈՒՆՔՆԵՐԻ ՀԱՄԸՆԴՀԱՆՈՒՐ ՀՌՉԱԿԱԳԻՐ\n",
            "    ՄԻԱՎՈՐՎԱԾ ԱԶԳԵՐԻ ԿԱԶՄԱԿԵՐՊՈՒԹՅՈՒՆ\n",
            "    ՆԵՐԱԾԱԿԱՆ\n",
            "    Քանզի մարդկային ընտանիքի բոլոր անդամներին ներհատուկ արժանապատվությունը և հավասար ու անօտարելի իրավունքները աշխարհի ազատության, արդարության ու խաղաղության հիմքն են․\n",
            "    Քանզի մարդու իրավունքների նկատմամբ քամահրանքն ու արհամարհանքը հանգեցրել են մարդկության խիղճը խռոված բարբարոսական գործողությունների, և քանի որ այնպիսի աշխարհի ստեղծումը, ուր մարդիկ կվայելեն խոսքի ու համոզմունքների ազատություն և զերծ կլինեն վախից ու կարիքից հռչակվել է որպես մարդկանց բարձրագույն ձգտում․\n",
            "    Քանզի անհրաժեշտ է, որպեսզի մարդը, որպես մի վերջին միջոցի, չդիմի ապստամբության ընդդեմ բռնության ու ճնշման, օրենքի իշխանությամբ պաշտպանել մարդու իրավունքները․\n",
            "    Քանզի անհրաժեշտ է նպաստել ազգերի միջև բարեկամական հարաբերությունների զարգացմանը․\n",
            "    Քանզի Միավորված ազգերի ժողովուրդները կանոնադրության մեջ վերահավաստել են իրենց հավատը մարդու հիմնական իրավունքների, անձի արժանապատվության ու արժեքի, տղամարդու ու կնոջ հավասար իրավունքների նկատմամբ և որոշել են ավելի մեծ ազատության պայմաններում նպաստել սոցիալական առաջընթացին ու կյանքի պայմանների բարելավմանը․\n",
            "    Քանզի անդամ պետությունները պարտավորվել են Միավորված ազգերի հետ համագործակցությամբ հասնել մարդու իրավունքների ու հիմնական ազատությունների նկատմամբ համընդհանուր հարգանքի ու դրանց պահպանման․\n",
            "    Քանզի այս իրավունքների ու ազատությունների համընդհանուր ըմբռնումը կարևորագույն նշանակություն ունի այս պարտավորության լիարժեք իրագործման համար․\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional part: trying different output format with disambiguation, etc. (showing what the tool can do)\n",
        "\n",
        "This can be skipped. You can go directly to the \"File annotation\" section"
      ],
      "metadata": {
        "id": "b9_WoVsxg3Ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trying out:\n",
        "# nonexisting word\n",
        "analyses2 = a.analyze_words('Ձևաբայու')\n",
        "for ana2 in analyses2:\n",
        "    if ana2.lemma:\n",
        "      print(ana2.wf, ana2.lemma, ana2.gramm, ana2.gloss, ana2.stem, ana2.subwords, ana2.wfGlossed, ana2.otherData)\n",
        "    else:\n",
        "      print(ana2.wf, ana2.wf, \"N\", \"x\", ana2.stem, ana2.subwords, ana2.wfGlossed, ana2.otherData)"
      ],
      "metadata": {
        "id": "jMu1kHN03mxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyses = a.analyze_words([['և'], ['Ես', 'սիրում', 'եմ', 'քեզ', ':']],\n",
        "                           format='xml')\n",
        "for ana in analyses:\n",
        "    print(str(ana))"
      ],
      "metadata": {
        "id": "5b0wetvb3zvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyses = a.analyze_words(['Ձևաբանություն', [['և'], ['Ես', 'սիրում', 'եմ', 'քեզ', ':']]],\n",
        "                           format='json')\n",
        "for ana in analyses:\n",
        "    print(str(ana))"
      ],
      "metadata": {
        "id": "R735YSlN3_rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# analysis with disambiguation\n",
        "analyses = a.analyze_words(['Ես', 'սիրում', 'եմ', 'քեզ'], disambiguate=True)\n",
        "for ana in analyses:\n",
        "    if len(ana) > 1: tab = \"  \"\n",
        "    else: tab = \"\"\n",
        "    for wfo in ana:\n",
        "        print(tab, wfo.wf, wfo.lemma, wfo.gramm, wfo.gloss)"
      ],
      "metadata": {
        "id": "401nl-i84GMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Str = \"Սառը, վճիտ ապրիլյան օր էր, ու ժամացույցը խփում էր տասներեքը։ Չար քամուց թաքնվելու համար կզակը սեղմելով կրծքին՝ Ուինսթոն Սմիթն արագ ներս խցկվեց «Հաղթանակ» բնակելի տան ապակե շքադռնից՝ իր ետևից ներս թողնելով հատիկավոր փոշու մի ամբողջ փոթորիկ։\"\n",
        "\n",
        "StrDe = ' „Es war ein kalter, trostloser Apriltag, und die Uhr schlug dreizehn. Das Kinn an die Brust gedrückt, um sich vor dem bitteren Wind zu schützen, eilte Winston Smith durch die gläserne Veranda des Wohnhauses Victory und hinterließ einen körnigen Sturm Staub.\" '\n",
        "\n",
        "StrEn = ' \"It was a cold, dreary April day, and the clock struck thirteen. Tucking his chin to his chest to shield himself from the bitter wind, Winston Smith hurried through the glass porch of the Victory apartment building, leaving behind him a storm of granular dust.\" '"
      ],
      "metadata": {
        "id": "xByNh1fO4XKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Str = \"Սառը, վճիտ. ապրիլյան օր էր, ու ժամացույցը խփում էր տասներեքը։ Չար քամուց թաքնվելու համար կզակը սեղմելով կրծքին՝ Ուինսթոն Սմիթն արագ ներս խցկվեց «Հաղթանակ» բնակելի տան ապակե շքադռնից՝ իր ետևից ներս թողնելով հատիկավոր փոշու մի ամբողջ փոթորիկ։\""
      ],
      "metadata": {
        "id": "hySmqfbM4KBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Lst = re.split(r'([ ,\\.:;\\!\\(\\)\\\"\\[\\]՞՝«»\\-\\—՝։\\։]+)', Str)\n",
        "LstTok = []\n",
        "for el in Lst:\n",
        "    el = el.strip()\n",
        "    if el != '': LstTok.append(el)\n"
      ],
      "metadata": {
        "id": "LD8NIcoycI9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(LstTok)\n",
        "# does disambiguation work? not yet..."
      ],
      "metadata": {
        "id": "JK8FDH6A4nD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyses = a.analyze_words(LstTok, disambiguate=True)\n",
        "for ana in analyses:\n",
        "    if len(ana) > 1: tab = \"  \"\n",
        "    else: tab = \"\"\n",
        "    for wfo in ana:\n",
        "        print(tab, wfo.wf, wfo.lemma, wfo.gramm, wfo.gloss)"
      ],
      "metadata": {
        "id": "xI7ZHTveiMn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File annotation\n",
        "\n",
        "- downloading word2vec model for English for distance-based disambiguation\n",
        "- defining functions to process the file"
      ],
      "metadata": {
        "id": "aeazgS7ShLAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vahram's model for English\n",
        "!wget https://heibox.uni-heidelberg.de/f/38de0eb8b2ec41d284d7/?dl=1\n",
        "!mv index.html?dl=1 WIKI_EN.model\n"
      ],
      "metadata": {
        "id": "Uy8itpV9ka1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_WIKI_EN = Word2Vec.load(\"/content/WIKI_EN.model\")\n",
        "word_vectors_WIKI_EN = model_WIKI_EN.wv\n",
        "distance = word_vectors_WIKI_EN.similarity('obama', 'barak')\n",
        "distance2 = word_vectors_WIKI_EN.similarity('obama', 'zone')\n",
        "print('distance = %.4f' % distance)\n",
        "print('distance2 = %.4f' % distance2)"
      ],
      "metadata": {
        "id": "E8DhZznElVRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizeHy(Str2tokenise, rePattern = r'([ ,\\.:;\\!\\(\\)\\\"\\[\\]՞՝«»\\-\\—՝։\\։]+)'):\n",
        "    LTokens = []\n",
        "    Lst = re.split(rePattern, Str2tokenise)\n",
        "    # LstTok = []\n",
        "    for el in Lst:\n",
        "        el = el.strip()\n",
        "        if el != '': LTokens.append(el)\n",
        "    return LTokens\n",
        "\n",
        "# merging together different lines of code\n",
        "\n",
        "def disambiguate(LTok2disambiguate, Window = 4, show_variants = False):\n",
        "    SDisambig = ''\n",
        "\n",
        "    analysesN = a.analyze_words(LTok2disambiguate, disambiguate=False)\n",
        "\n",
        "    # preparing data structures\n",
        "    LLContext = [] # empty list of contexts, indices are the same as with the Text list\n",
        "    LLText = [] # test to disambiguate, ambiguous interpretations are double entries\n",
        "    for ana in analysesN:\n",
        "        # creating context window from glosses\n",
        "        # if len(ana) > 1: tab = \"~\"\n",
        "        # else: tab = \"!\"\n",
        "        LwfoContext = []\n",
        "        LwfoText = []\n",
        "        for wfo in ana: # preserve lemmas / word forms which are not found in dictionary\n",
        "            if wfo.gramm == '': wfo.gramm = 'N'\n",
        "            if wfo.lemma == '': wfo.lemma = wfo.wf\n",
        "            if wfo.gloss == '': wfo.gloss = '[unknown]'\n",
        "\n",
        "            SWfo = f'{wfo.wf}\\t{wfo.gramm}\\t{wfo.lemma}\\t{wfo.gloss}'\n",
        "            # print(SWfo)\n",
        "            # FNoD.write(SWfo + '\\n')\n",
        "\n",
        "            # find the first part of the gloss, which may be in the word vectors model\n",
        "            REPart = re.match('([A-Za-z]+)', wfo.gloss)\n",
        "            if REPart:\n",
        "                SGlossMin = REPart.group(1)\n",
        "                SGlossMin = SGlossMin.lower()\n",
        "                # print(SGlossMin)\n",
        "            else:\n",
        "                SGlossMin = '[NONE]'\n",
        "            LwfoContext.append(SGlossMin)\n",
        "            LwfoText.append(SWfo)\n",
        "        LLContext.append(LwfoContext)\n",
        "        LLText.append(LwfoText)\n",
        "\n",
        "    # for el in LLContext: print(el)\n",
        "    # for el in LLText: print(el)\n",
        "\n",
        "    # print(len(LLContext))\n",
        "    # print(len(LLText))\n",
        "\n",
        "    sys.stderr.write('.')\n",
        "\n",
        "\n",
        "    for i in range(len(LLText)):\n",
        "        if len(LLText[i]) > 1:\n",
        "            # print(LLText[i])\n",
        "            # print(LLContext[i])\n",
        "            # collect context window +- 3 words\n",
        "            iwStart = i-Window\n",
        "            if iwStart <0: iwStart=0\n",
        "            iwEnd = i+Window\n",
        "            if iwEnd > len(LLText): iwEnd = len(LLText)\n",
        "            # iwLen = iwEnd - iwStart\n",
        "            winContext = LLContext[iwStart:iwEnd]\n",
        "            # print(winContext)\n",
        "            LScores = []\n",
        "            LScCand = []\n",
        "\n",
        "            for candidate in LLContext[i]:\n",
        "                ScoreCand = 0\n",
        "                for LCtx in winContext:\n",
        "                    for Ctx in LCtx:\n",
        "                        try: distance = word_vectors_WIKI_EN.similarity(candidate, Ctx)\n",
        "                        except: distance = 0\n",
        "                        ScoreCand += distance\n",
        "                LScores.append((candidate,ScoreCand))\n",
        "                LScCand.append(ScoreCand)\n",
        "            LScores.sort(key=lambda a: a[1], reverse=True)\n",
        "            # print(LScores)\n",
        "\n",
        "            max_value = max(LScCand)\n",
        "            #  Return the max value of the list\n",
        "            max_index = LScCand.index(max_value)\n",
        "            StoWrite = LLText[i][max_index] + '\\n'\n",
        "            SDisambig += StoWrite\n",
        "\n",
        "            if show_variants == True:\n",
        "                for el in LLText[i]:\n",
        "                    StoWrite = '\\t~\\t' + el + '\\n'\n",
        "                    SDisambig += StoWrite\n",
        "                for el in LScores:\n",
        "                    StoWrite = '\\t~sc:\\t' + str(el) + '\\n'\n",
        "                    SDisambig += StoWrite\n",
        "        else:\n",
        "            StoWrite = LLText[i][0] + '\\n'\n",
        "            SDisambig += StoWrite\n",
        "\n",
        "    return SDisambig\n",
        "\n",
        "\n",
        "\n",
        "def lemmatizeHYfile(FInText, FOutText):\n",
        "    count = 0\n",
        "    for SLine in FInText:\n",
        "\n",
        "        count += 1\n",
        "        if count % 100 == 0: sys.stderr.write('\\n')\n",
        "        SLine = SLine.strip()\n",
        "        LTok = tokenizeHy(SLine)\n",
        "        # SDisambig = disambiguate(LTok, show_variants = True)\n",
        "        SDisambig = disambiguate(LTok, show_variants = False)\n",
        "        # FOutText.write('<p>\\n')\n",
        "        # FOutText.write('\\n')\n",
        "        FOutText.write(SDisambig)\n",
        "        # FOutText.write('</p>\\n')\n",
        "        FOutText.write('\\n')\n",
        "\n",
        "    FOutText.flush()\n",
        "    return\n",
        "\n"
      ],
      "metadata": {
        "id": "TpxYmxdTwGyH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### give names for the input and output file"
      ],
      "metadata": {
        "id": "Kb_7hsYchZZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run this to convert InText file into OutText file\n",
        "FInText = open('udhr_hye_v03.txt','r')\n",
        "FOutText = open('udhr_hye_v03.vert','w')\n",
        "\n",
        "lemmatizeHYfile(FInText, FOutText)"
      ],
      "metadata": {
        "id": "YMgnQ_crwuQw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## optional: if you do not need English translations in your output file\n",
        "\n",
        "- you can remove this with the command\n",
        "- the output is written in v04"
      ],
      "metadata": {
        "id": "au7KK2u48C-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!awk -F '\\t' '(NF==4){printf \"%s\\t%s\\t%s\\n\", $1, $2, $3}(NF!=4){printf \"%s\\n\", $0}' < udhr_hye_v03.vert >udhr_hye_v04.vert"
      ],
      "metadata": {
        "id": "lvfd0Fna6wkL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## optional: Creating a text file with lemmas\n",
        "\n",
        "all word forms are converted into lemmas (might be used for training word2vec models)"
      ],
      "metadata": {
        "id": "c9GN7kuzhfBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !awk -F '\\t' '(NF==4){printf \"%s \", $3}(NF!=4){printf \"\\n\"}' < /content/udhrTT/udhr_hye_vert.txt >/content/udhrTT/udhr_hye_lem.txt\n",
        "!awk -F '\\t' '(NF==4){printf \"%s \", $3}(NF!=4){printf \"\\n\"}' < udhr_hye_v03.vert >udhr_hye_v03_lem.txt"
      ],
      "metadata": {
        "id": "ScstreEf1RDn"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}